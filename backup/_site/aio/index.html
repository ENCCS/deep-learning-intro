















<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta http-equiv="last-modified" content="2023-05-17 15:41:55 +0200">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- meta "search-domain" used for google site search function google_search() -->
    <meta name="search-domain" value="">
    <link rel="stylesheet" type="text/css" href="../assets/css/bootstrap.css" />
    <link rel="stylesheet" type="text/css" href="../assets/css/bootstrap-theme.css" />
    <link rel="stylesheet" type="text/css" href="../assets/css/lesson.css" />
    <link rel="stylesheet" type="text/css" href="../assets/css/syntax.css" />
     <link rel="stylesheet" type="text/css" href="../assets/css/fonts.css" />
    
    <link rel="license" href="#license-info" />

    



    <!-- Favicons for everyone -->
    <link rel="apple-touch-icon-precomposed" sizes="57x57" href="../assets/favicons/incubator/apple-touch-icon-57x57.png" />
    <link rel="apple-touch-icon-precomposed" sizes="114x114" href="../assets/favicons/incubator/apple-touch-icon-114x114.png" />
    <link rel="apple-touch-icon-precomposed" sizes="72x72" href="../assets/favicons/incubator/apple-touch-icon-72x72.png" />
    <link rel="apple-touch-icon-precomposed" sizes="144x144" href="../assets/favicons/incubator/apple-touch-icon-144x144.png" />
    <link rel="apple-touch-icon-precomposed" sizes="60x60" href="../assets/favicons/incubator/apple-touch-icon-60x60.png" />
    <link rel="apple-touch-icon-precomposed" sizes="120x120" href="../assets/favicons/incubator/apple-touch-icon-120x120.png" />
    <link rel="apple-touch-icon-precomposed" sizes="76x76" href="../assets/favicons/incubator/apple-touch-icon-76x76.png" />
    <link rel="apple-touch-icon-precomposed" sizes="152x152" href="../assets/favicons/incubator/apple-touch-icon-152x152.png" />
    <link rel="icon" type="image/png" href="../assets/favicons/incubator/favicon-196x196.png" sizes="196x196" />
    <link rel="icon" type="image/png" href="../assets/favicons/incubator/favicon-96x96.png" sizes="96x96" />
    <link rel="icon" type="image/png" href="../assets/favicons/incubator/favicon-32x32.png" sizes="32x32" />
    <link rel="icon" type="image/png" href="../assets/favicons/incubator/favicon-16x16.png" sizes="16x16" />
    <link rel="icon" type="image/png" href="../assets/favicons/incubator/favicon-128.png" sizes="128x128" />
    <meta name="application-name" content="The Carpentries Incubator - Introduction to deep-learning"/>
    <meta name="msapplication-TileColor" content="#FFFFFF" />
    <meta name="msapplication-TileImage" content="../assets/favicons/incubator/mstile-144x144.png" />
    <meta name="msapplication-square70x70logo" content="../assets/favicons/incubator/mstile-70x70.png" />
    <meta name="msapplication-square150x150logo" content="../assets/favicons/incubator/mstile-150x150.png" />
    <meta name="msapplication-wide310x150logo" content="../assets/favicons/incubator/mstile-310x150.png" />
    <meta name="msapplication-square310x310logo" content="../assets/favicons/incubator/mstile-310x310.png" />


    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
	<script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
	<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
	<![endif]-->
  <title>
  Introduction to deep-learning
  </title>

  </head>
  <body>
    







<div class="panel panel-default life-cycle">
  <div id="life-cycle" class="panel-body beta">
    This lesson is being piloted (Beta version)
    
    <br> <strong><a href="/issues">If you teach this lesson, please tell the authors and provide feedback by opening an issue in the source repository</a></strong>
    
  </div>
</div>




    <div class="container">
      
















  
  










<nav class="navbar navbar-default">
  <div class="container-fluid">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>

      
      
      <a href="../index.html" class="pull-left">
        <img class="navbar-logo" src="../assets/img/incubator-logo-blue.svg" alt="The Carpentries Incubator logo" />
      </a>
      

      
      <a class="navbar-brand" href="../index.html">Home</a>

    </div>
    <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
      <ul class="nav navbar-nav">

	
        <li><a href="../CODE_OF_CONDUCT.html">Code of Conduct</a></li>

        
	
        <li><a href="../setup/">Setup</a></li>

        
        
        <li class="dropdown">
          <a href="../" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false">Episodes <span class="caret"></span></a>
          <ul class="dropdown-menu">
            
            
            <li><a href="../01-introduction/index.html">Introduction</a></li>
            
            
            <li><a href="../02-keras/index.html">Classification by a Neural Network using Keras</a></li>
            
            
            <li><a href="../03-monitor-the-model/index.html">Monitor the training process</a></li>
            
            
            <li><a href="../04-advanced-layer-types/index.html">Advanced layer types</a></li>
            
	    <li role="separator" class="divider"></li>
            <li><a href="../aio/index.html">All in one page (Beta)</a></li>
          </ul>
        </li>
        
	

	
	
        <li class="dropdown">
          <a href="../" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false">Extras <span class="caret"></span></a>
          <ul class="dropdown-menu">
            <li><a href="../reference.html">Reference</a></li>
            
            
            
              <li><a href="../about/index.html">About</a></li>
            
            
            
            
              <li><a href="../design/index.html">Lesson Design</a></li>
            
            
            
            
              <li><a href="../discuss/index.html">Discussion</a></li>
            
            
            
            
              <li><a href="../figures/index.html">Figures</a></li>
            
            
            
            
              <li><a href="../guide/index.html">Instructor Notes</a></li>
            
            
            
            
              <li><a href="../survey-templates/index.html">Workshop survey templates</a></li>
            
            
          </ul>
        </li>
	

	
        <li><a href="../LICENSE.html">License</a></li>
	
	<li><a href="/edit//aio.md" data-checker-ignore>Improve this page <span class="glyphicon glyphicon-pencil" aria-hidden="true"></span></a></li>
	
      </ul>
      <form class="navbar-form navbar-right" role="search" id="search" onsubmit="google_search(); return false;">
        <div class="form-group">
          <input type="text" id="google-search" placeholder="Search..." aria-label="Google site search">
        </div>
      </form>
    </div>
  </div>
</nav>

      


<div class="alert alert-info text-center" role="alert">
  This lesson is part of
  <a href="https://github.com/carpentries-incubator/proposals/#the-carpentries-incubator" data-checker-ignore>
    The Carpentries Incubator</a>, a place to share and use each other's
  Carpentries-style lessons. <strong>This lesson has not been reviewed by and is
  not endorsed by The Carpentries</strong>.
</div>




      














<h1 class="maintitle"><a href="../index.html">Introduction to deep-learning</a></h1>



<article>

<h1 id="introduction" class="maintitle">Introduction</h1>

<blockquote class="objectives">
  <h2>Overview</h2>

  <div class="row">
    <div class="col-md-3">
      <strong>Teaching:</strong> 40 min
      <br />
      <strong>Exercises:</strong> 15 min
    </div>
    <div class="col-md-9">
      <strong>Questions</strong>
      <ul>
	
	<li><p>What is Deep Learning?</p>
</li>
	
	<li><p>When does it make sense to use and not use Deep Learning?</p>
</li>
	
	<li><p>When is it successful?</p>
</li>
	
	<li><p>What are the tools involved?</p>
</li>
	
	<li><p>What is the workflow for Deep Learning?</p>
</li>
	
	<li><p>Why did we choose to use Keras in this lesson?</p>
</li>
	
	<li><p>How do neural networks learn?</p>
</li>
	
      </ul>
    </div>
  </div>

  <div class="row">
    <div class="col-md-3">
    </div>
    <div class="col-md-9">
      <strong>Objectives</strong>
      <ul>
	
	<li><p>Recall the sort of problems for which Deep Learning is a useful tool</p>
</li>
	
	<li><p>List some of the available tools for Deep Learning</p>
</li>
	
	<li><p>Recall the steps of a Deep Learning workflow</p>
</li>
	
	<li><p>Identify the inputs and outputs of a deep neural network.</p>
</li>
	
	<li><p>Explain the operations performed in a single neuron</p>
</li>
	
	<li><p>Test that you have correctly installed the Keras, Seaborn and Sklearn libraries</p>
</li>
	
	<li><p>Describe what a loss function is</p>
</li>
	
      </ul>
    </div>
  </div>

</blockquote>

<h2 id="what-is-deep-learning">What is Deep Learning?</h2>

<h3 id="deep-learning-machine-learning-and-artificial-intelligence">Deep Learning, Machine Learning and Artificial Intelligence</h3>

<p>Deep Learning (DL) is just one of many techniques collectively known as machine learning. Machine learning (ML) refers to techniques where a computer can “learn” patterns in data, usually by being shown numerous examples to train it. People often talk about machine learning being a form of artificial intelligence (AI). Definitions of artificial intelligence vary, but usually involve having computers mimic the behaviour of intelligent biological systems. Since the 1950s many works of science fiction have dealt with the idea of an artificial intelligence which matches (or exceeds) human intelligence in all areas. Although there have been great advances in AI and ML research recently we can only come close to human like intelligence in a few specialist areas and are still a long way from a general purpose AI.
The image below shows some differences between artificial intelligence, Machine Learning and Deep Learning.</p>

<p><img src="../fig/01_AI_ML_DL_differences.png" alt="An infographics showing the relation of AI, ML, NN and DL. NN are methods in DL which is a subset of ML algorithms that falls within the umbrella of AI" width="750px" />
The image above is by Tukijaaliwa, CC BY-SA 4.0, via Wikimedia Commons, <a href="https://en.wikipedia.org/wiki/File:AI-ML-DL.svg">original source</a></p>

<h4 id="neural-networks">Neural Networks</h4>

<p>A neural network is an artificial intelligence technique loosely based on the way neurons in the brain work. A neural network consists of connected computational units called <strong>neurons</strong>. Each neuron …</p>

<ul>
  <li>has one or more inputs, e.g. input data expressed as floating point numbers</li>
  <li>most of the time, each neuron conducts 3 main operations:
    <ul>
      <li>take the weighted sum of the inputs</li>
      <li>add an extra constant weight (i.e. a bias term) to this weighted sum</li>
      <li>apply a non-linear function to the output so far (using a predefined activation function)</li>
    </ul>
  </li>
  <li>return one output value, again a floating point number</li>
</ul>

<p><img src="../fig/01_neuron.png" alt="A diagram of a single artificial neuron combining inputs and weights using an activation function" width="600px" /></p>

<p>Multiple neurons can be joined together by connecting the output of one to the input of another. These connections are associated with weights that determine the ‘strength’ of the connection, the weights are adjusted during training. In this way, the combination of neurons and connections describe a computational graph, an example can be seen in the image below. In most neural networks neurons are aggregated into layers. Signals travel from the input layer to the output layer, possibly through one or more intermediate layers called hidden layers.
The image below shows an example of a neural network with three layers, each circle is a neuron, each line is an edge and the arrows indicate the direction data moves in.</p>

<p><img src="../fig/01_neural_net.png" alt="A diagram of a three layer neural network with an input layer, one hidden layer, and an output layer." width="400px" />
The image above is by Glosser.ca, CC BY-SA 3.0 <a href="https://creativecommons.org/licenses/by-sa/3.0">https://creativecommons.org/licenses/by-sa/3.0</a>, via Wikimedia Commons, <a href="https://commons.wikimedia.org/wiki/File:Colored_neural_network.svg">original source</a></p>

<blockquote class="challenge">
  <h2 id="calculate-the-output-for-one-neuron">Calculate the output for one neuron</h2>
  <p>Suppose we have</p>
  <ul>
    <li>Input: X = (0, 0.5, 1)</li>
    <li>Weights: W = (-1, -0.5, 0.5)</li>
    <li>Bias: b = 1</li>
    <li>Activation function <em>relu</em>: <code class="language-plaintext highlighter-rouge">f(x) = max(x, 0)</code></li>
  </ul>

  <p>What is the output of the neuron?</p>

  <p><em>Note: You can use whatever you like: brain only, pen&amp;paper, Python, Excel…</em></p>

  <blockquote class="solution">
    <h2 id="solution">Solution</h2>
    <p>Weighted sum of input: 0 * (-1) + 0.5 * (-0.5) + 1 * 0.5 = 0.25</p>

    <p>Add the bias: 0.25 + 1 = 1.25</p>

    <p>Apply activation function: max(1.25, 0) = 1.25</p>

    <p>So, neuron output = 1.25</p>
  </blockquote>
</blockquote>

<p>Neural networks aren’t a new technique, they have been around since the late 1940s. But until around 2010 neural networks tended to be quite small, consisting of only 10s or perhaps 100s of neurons. This limited them to only solving quite basic problems. Around 2010 improvements in computing power and the algorithms for training the networks made much larger and more powerful networks practical. These are known as deep neural networks or Deep Learning.</p>

<p>Deep Learning requires extensive training using example data which shows the network what output it should produce for a given input. One common application of Deep Learning is classifying images. Here the network will be trained by being “shown” a series of images and told what they contain. Once the network is trained it should be able to take another image and correctly classify its contents. But we are not restricted to just using images, any kind of data can be learned by a Deep Learning neural network. This makes them able to appear to learn a set of complex rules only by being shown what the inputs and outputs of those rules are instead of being taught the actual rules. Using these approaches Deep Learning networks have been taught to play video games and even drive cars. The data on which networks are trained usually has to be quite extensive, typically including thousands of examples. For this reason they are not suited to all applications and should be considered just one of many machine learning techniques which are available.</p>

<p>While traditional “shallow” networks might have had between three and five layers, deep networks often have tens or even hundreds of layers. This leads to them having millions of individual weights.
The image below shows a diagram of all the layers (there are too many neurons to draw them all) on a Deep Learning network designed to detect pedestrians in images.
The input (left most) layer of the network is an image and the final (right most) layer of the network outputs a zero or one to determine if the input data belongs to the class of data we are interested in.
This image is from the paper <a href="https://doi.org/10.1155/2018/3518959">“An Efficient Pedestrian Detection Method Based on YOLOv2” by Zhongmin Liu, Zhicai Chen, Zhanming Li, and Wenjin Hu published in Mathematical Problems in Engineering, Volume 2018</a></p>

<p><img src="../fig/01_deep_network.png" alt="An example of a deep neural network" width="600px" /></p>

<h3 id="how-do-neural-networks-learn">How do neural networks learn?</h3>
<p>What happens in a neural network during the training process?
The ultimate goal is of course to find a model that makes predictions that are as close to the target value as possible.
In other words, the goal of training is to find the best set of parameters (weights and biases)
that bring the error between prediction and expected value to a minimum.
The total error between prediction and expected value is quantified in a loss function (also called cost function).
There are lots of loss functions to pick from, and it is important that you pick one that matches your problem definition well.
We will look at an example of a loss function in the next exercise.</p>

<blockquote class="challenge">
  <h2 id="exercise-mean-squared-error">Exercise: Mean Squared Error</h2>
  <p>One of the simplest loss functions is the Mean Squared Error. MSE = $\frac{1}{n} \Sigma_{i=1}^n({y}-\hat{y})^2$ .
It is the mean of all squared errors, where the error is the difference between the predicted and expected value.
In the following table, fill in the missing values in the ‘squared error’ column. What is the MSE loss
for the predictions on these 4 samples?</p>
  <table>
<thead>
  <tr>
    <th>Prediction</th> <th>Expected value</th> <th>Squared error</th>
  </tr>
</thead>
<tbody>
  <tr>
    <td>1</td>              <td>-1</td>                 <td>4</td>
  </tr>
  <tr>
    <td>2</td>             <td>-1</td>                   <td>..</td>
  </tr>
  <tr>
    <td>0</td>             <td>0</td>                    <td>..</td>
  </tr>
  <tr>
    <td>3</td>             <td>2</td>                    <td>..</td>
  </tr>
  <tr>
    <td></td>             <td>MSE:</td>              <td>..</td>
  </tr>
</tbody>
</table>

  <blockquote class="solution">
    <h2 id="solution-1">Solution</h2>
    <table>
<thead>
  <tr>
    <th>Prediction</th> <th>Expected value</th> <th>Squared error</th>
  </tr>
</thead>
<tbody>
  <tr>
    <td>1</td>              <td>-1</td>                 <td>4</td>
  </tr>
  <tr>
    <td>2</td>             <td>-1</td>                   <td> 9 </td>
  </tr>
  <tr>
    <td>0</td>             <td>0</td>                    <td> 0 </td>
  </tr>
  <tr>
    <td>3</td>             <td>2</td>                    <td> 1 </td>
  </tr>
  <tr>
    <td></td>             <td>MSE:</td>              <td> 3.5 </td>
  </tr>
</tbody>
</table>
  </blockquote>
</blockquote>

<p>So, a loss function quantifies the total error of the model.
The process of adjusting the weights in such a way as to minimize the loss function is called ‘optimization’.
We will dive further into how optimization works in episode 3,
for now it is enough to understand that during training the weights in the network are adjusted so that the loss decreases through the process of optimization.
Ultimately resulting in a low loss, and thus predictions that are close to the expected values.</p>

<h3 id="what-sort-of-problems-can-deep-learning-solve">What sort of problems can Deep Learning solve?</h3>

<ul>
  <li>Pattern/object recognition</li>
  <li>Segmenting images (or any data)</li>
  <li>Translating between one set of data and another, for example natural language translation.</li>
  <li>Generating new data that looks similar to the training data, often used to create synthetic datasets, art or even “deepfake” videos.
    <ul>
      <li>This can also be used to give the illusion of enhancing data, for example making images look sharper, video look smoother or adding colour to black and white images. But beware of this, it is not an accurate recreation of the original data, but a recreation based on something statistically similar, effectively a digital imagination of what that data could look like.</li>
    </ul>
  </li>
</ul>

<h4 id="examples-of-deep-learning-in-research">Examples of Deep Learning in Research</h4>

<p>Here are just a few examples of how Deep Learning has been applied to some research problems. Note: some of these articles might be behind paywalls.</p>

<ul>
  <li><a href="https://arxiv.org/abs/2003.09871">Detecting COVID-19 in chest X-ray images</a></li>
  <li><a href="https://ieeexplore.ieee.org/document/7793413">Forecasting building energy load</a></li>
  <li><a href="https://pubmed.ncbi.nlm.nih.gov/29039790/">Protein function prediction</a></li>
  <li><a href="https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.98.146401">Simulating Chemical Processes</a></li>
  <li><a href="https://heritagesciencejournal.springeropen.com/articles/10.1186/s40494-020-0355-x">Help to restore ancient murals</a></li>
</ul>

<h3 id="what-sort-of-problems-can-deep-learning-not-solve">What sort of problems can Deep Learning not solve?</h3>

<ul>
  <li>Any case where only a small amount of training data is available.</li>
  <li>Tasks requiring an explanation of how the answer was arrived at.</li>
  <li>Classifying things which are nothing like their training data.</li>
</ul>

<h3 id="what-sort-of-problems-can-deep-learning-solve-but-should-not-be-used-for">What sort of problems can Deep Learning solve, but should not be used for?</h3>

<p>Deep Learning needs a lot of computational power, for this reason it often relies on specialised hardware like graphical processing units (GPUs). Many computational problems can be solved using less intensive techniques, but could still technically be solved with Deep Learning.</p>

<p>The following could technically be achieved using Deep Learning, but it would probably be a very wasteful way to do it:</p>

<ul>
  <li>Logic operations, such as computing totals, averages, ranges etc. (see <a href="https://joelgrus.com/2016/05/23/fizz-buzz-in-tensorflow">this example</a> applying Deep Learning to solve the “FizzBuzz” problem often used for programming interviews)</li>
  <li>Modelling well defined systems, where the equations governing them are known and understood.</li>
  <li>Basic computer vision tasks such as edge detection, decreasing colour depth or blurring an image.</li>
</ul>

<blockquote class="challenge">
  <h2 id="deep-learning-problems-exercise">Deep Learning Problems Exercise</h2>

  <p>Which of the following would you apply Deep Learning to?</p>
  <ol>
    <li>Recognising whether or not a picture contains a bird.</li>
    <li>Calculating the median and interquartile range of a dataset.</li>
    <li>Identifying MRI images of a rare disease when only one or two example images available for training.</li>
    <li>Identifying people in pictures after being trained only on cats and dogs.</li>
    <li>Translating English into French.</li>
  </ol>

  <blockquote class="solution">
    <h2 id="solution-2">Solution</h2>

    <ol>
      <li>and 5 are the sort of tasks often solved with Deep Learning.</li>
      <li>is technically possible but solving this with Deep Learning would be extremely wasteful, you could do the same with much less computing power using traditional techniques.</li>
      <li>will probably fail because there is not enough training data.</li>
      <li>will fail because the Deep Learning system only knows what cats and dogs look like, it might accidentally classify the people as cats or dogs.</li>
    </ol>
  </blockquote>
</blockquote>

<blockquote class="callout">
  <h2 id="how-much-data-do-you-need-for-deep-learning">How much data do you need for Deep Learning?</h2>
  <p>The rise of Deep Learning is partially due to the increased availability of very large datasets.
But how much data do you actually need to train a Deep Learning model?
Unfortunately, this question is not easy to answer. It depends, among other things, on the
complexity of the task (which you often do not know beforehand), the quality of the available dataset and the complexity of the network. For complex tasks with large neural networks, we often see that adding more data continues to improve performance. However, this is also not a generic truth: if the data you add is too similar to the data you already have, it will not give much new information to the neural network.</p>

  <p>In case you have too little data available to train a complex network from scratch, it is sometimes possible to use a pretrained network that was trained on a similar problem. Another trick is data augmentation, where you expand the dataset with artificial data points that could be real. An example of this is mirroring images when trying to classify cats and dogs. An horizontally mirrored animal retains the label, but exposes a different view.</p>
</blockquote>

<h2 id="deep-learning-workflow">Deep Learning workflow</h2>

<p>To apply Deep Learning to a problem there are several steps we need to go through:</p>

<h3 id="1-formulate-outline-the-problem">1. Formulate/ Outline the problem</h3>

<p>Firstly we must decide what it is we want our Deep Learning system to do. Is it going to classify some data into one of a few categories? For example if we have an image of some hand written characters, the neural network could classify which character it is being shown. Or is it going to perform a prediction? For example trying to predict what the price of something will be tomorrow given some historical data on pricing and current trends.</p>

<h3 id="2-identify-inputs-and-outputs">2. Identify inputs and outputs</h3>

<p>Next we need to identify what the inputs and outputs of the neural network will be. This might require looking at our data and deciding what features of the data we can use as inputs. If the data is images then the inputs could be the individual pixels of the images.</p>

<p>For the outputs we will need to look at what we want to identify from the data. If we are performing a classification problem then typically we will have one output for each potential class.</p>

<h3 id="3-prepare-data">3. Prepare data</h3>

<p>Many datasets are not ready for immediate use in a neural network and will require some preparation. Neural networks can only really deal with numerical data, so any non-numerical data (for example words) will have to be somehow converted to numerical data.</p>

<p>Next we will need to divide the data into multiple sets. One of these will be used by the training process and we will call it the training set. Another will be used to evaluate the accuracy of the training and we will call that one the test set. Sometimes we will also use a 3rd set known as a validation set to tune hyperparameters.</p>

<h3 id="4-choose-a-pre-trained-model-or-build-a-new-architecture-from-scratch">4. Choose a pre-trained model or build a new architecture from scratch</h3>

<p>Often we can use an existing neural network instead of designing one from scratch. Training a network can take a lot of time and computational resources. There are a number of well publicised networks which have been shown to perform well at certain tasks, if you know of one which already does a similar task well then it makes sense to use one of these.</p>

<p>If instead we decide we do want to design our own network then we need to think about how many input neurons it will have, how many hidden layers and how many outputs, what types of layers we use (we will explore the different types later on). This will probably need some experimentation and we might have to try tweaking the network design a few times before we see acceptable results.</p>

<h3 id="5-choose-a-loss-function-and-optimizer">5. Choose a loss function and optimizer</h3>

<p>The loss function tells the training algorithm how far away the predicted value was from the true value. We will look at choosing a loss function in more detail later on.</p>

<p>The optimizer is responsible for taking the output of the loss function and then applying some changes to the weights within the network. It is through this process that the “learning” (adjustment of the weights) is achieved.</p>

<h3 id="6-train-the-model">6. Train the model</h3>

<p>We can now go ahead and start training our neural network. We will probably keep doing this for a given number of iterations through our training dataset (referred to as <em>epochs</em>) or until the loss function gives a value under a certain threshold. The graph below show the loss against the number of <em>epochs</em>, generally the loss will go down with each <em>epoch</em>, but occasionally it will see a small rise.</p>

<p><img src="../fig/training-0_to_1500.svg" alt="A graph showing an exponentially decreasing loss over the first 1500 epochs of training an example network." /></p>

<h3 id="7-perform-a-predictionclassification">7. Perform a Prediction/Classification</h3>

<p>After training the network we can use it to perform predictions. This is the mode you would
use the network in after you have fully trained it to a satisfactory performance. Doing
predictions on a special hold-out set is used in the next step to measure the performance
of the network.</p>

<h3 id="8-measure-performance">8. Measure Performance</h3>

<p>Once we trained the network we want to measure its performance. To do this we use some additional data that was not part of the training, this is known as a test set. There are many different methods available for measuring performance and which one is best depends on the type of task we are attempting. These metrics are often published as an indication of how well our network performs.</p>

<h3 id="9-tune-hyperparameters">9. Tune Hyperparameters</h3>

<p>Hyperparameters are all the parameters set by the person configuring the machine learning instead of those learned by the algorithm itself. The hyperparameters include the number of epochs or the parameters for the optimizer. It might be necessary to adjust these and re-run the training many times before we are happy with the result.</p>

<h3 id="10-share-model">10. Share Model</h3>

<p>Now that we have a trained network that performs at a level we are happy with we can go and use it on real data to perform a prediction. At this point we might want to consider publishing a file with both the architecture of our network and the weights which it has learned (assuming we did not use a pre-trained network). This will allow others to use it as as pre-trained network for their own purposes and for them to (mostly) reproduce our result.</p>

<blockquote class="challenge">
  <h2 id="deep-learning-workflow-exercise">Deep Learning workflow exercise</h2>

  <p>Think about a problem you would like to use Deep Learning to solve.</p>
  <ol>
    <li>What do you want a Deep Learning system to be able to tell you?</li>
    <li>What data inputs and outputs will you have?</li>
    <li>Do you think you will need to train the network or will a pre-trained network be suitable?</li>
    <li>What data do you have to train with? What preparation will your data need? Consider both the data you are going to predict/classify from and the data you will use to train the network.</li>
  </ol>

  <p>Discuss your answers with the group or the person next to you.</p>
</blockquote>

<h2 id="deep-learning-libraries">Deep Learning Libraries</h2>

<p>There are many software libraries available for Deep Learning including:</p>

<h3 id="tensorflow">TensorFlow</h3>

<p><a href="https://www.tensorflow.org/">TensorFlow</a> was developed by Google and is one of the older Deep Learning libraries, ported across many languages since it was first released to the public in 2015. It is very versatile and capable of much more than Deep Learning but as a result it often takes a lot more lines of code to write Deep Learning operations in TensorFlow than in other libraries. It offers (almost) seamless integration with GPU accelerators and Google’s own TPU (Tensor Processing Unit) chips that are built specially for machine learning.</p>

<h3 id="pytorch">PyTorch</h3>

<p><a href="https://pytorch.org/">PyTorch</a> was developed by Facebook in 2016 and is a popular choice for Deep Learning applications. It was developed for Python from the start and feels a lot more “pythonic” than TensorFlow. Like TensorFlow it was designed to do more than just Deep Learning and offers some very low level interfaces. <a href="https://www.pytorchlightning.ai/">PyTorch Lightning</a> offers a higher level interface to PyTorch to set up experiments. Like TensorFlow it is also very easy to integrate PyTorch with a GPU. In many benchmarks it outperforms the other libraries.</p>

<h3 id="keras">Keras</h3>

<p><a href="https://keras.io/">Keras</a> is designed to be easy to use and usually requires fewer lines of code than other libraries. We have chosen it for this workshop for that reason. Keras can actually work on top of TensorFlow (and several other libraries), hiding away the complexities of TensorFlow while still allowing you to make use of their features.</p>

<p>The performance of Keras is sometimes not as good as other libraries and if you are going to move on to create very large networks using very large datasets then you might want to consider one of the other libraries. But for many applications the performance difference will not be enough to worry about and the time you will save with simpler code will exceed what you will save by having the code run a little faster.</p>

<p>Keras also benefits from a very good set of <a href="https://keras.io/guides/">online documentation</a> and a large user community. You will find that most of the concepts from Keras translate very well across to the other libraries if you wish to learn them at a later date.</p>

<h3 id="installing-keras-and-other-dependencies">Installing Keras and other dependencies</h3>

<p>Follow the instructions in the <a href="..//setup">setup</a> document to install Keras, Seaborn and Sklearn.</p>

<blockquote class="challenge">
  <h2 id="testing-keras-installation">Testing Keras Installation</h2>
  <p>Lets check you have a suitable version of Keras installed.
Open up a new Jupyter notebook or interactive python console and run the following commands:</p>
  <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">tensorflow</span> <span class="kn">import</span> <span class="n">keras</span>
<span class="k">print</span><span class="p">(</span><span class="n">keras</span><span class="p">.</span><span class="n">__version__</span><span class="p">)</span>
</code></pre></div>  </div>
  <blockquote class="solution">
    <h2 id="solution-3">Solution</h2>
    <p>You should get a version number reported. At the time of writing 2.12.0 is the latest version.</p>
    <div class="language-plaintext output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>2.12.0
</code></pre></div>    </div>
  </blockquote>
</blockquote>

<blockquote class="challenge">
  <h2 id="testing-seaborn-installation">Testing Seaborn Installation</h2>
  <p>Lets check you have a suitable version of seaborn installed.
In your Jupyter notebook or interactive python console run the following commands:</p>
  <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">seaborn</span>
<span class="k">print</span><span class="p">(</span><span class="n">seaborn</span><span class="p">.</span><span class="n">__version__</span><span class="p">)</span>
</code></pre></div>  </div>
  <blockquote class="solution">
    <h2 id="solution-4">Solution</h2>
    <p>You should get a version number reported. At the time of writing 0.12.2 is the latest version.</p>
    <div class="language-plaintext output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>0.12.2
</code></pre></div>    </div>
  </blockquote>
</blockquote>

<blockquote class="challenge">
  <h2 id="testing-sklearn-installation">Testing Sklearn Installation</h2>
  <p>Lets check you have a suitable version of sklearn installed.
In your Jupyter notebook or interactive python console run the following commands:</p>
  <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">sklearn</span>
<span class="k">print</span><span class="p">(</span><span class="n">sklearn</span><span class="p">.</span><span class="n">__version__</span><span class="p">)</span>
</code></pre></div>  </div>
  <blockquote class="solution">
    <h2 id="solution-5">Solution</h2>
    <p>You should get a version number reported. At the time of writing 1.2.2 is the latest version.</p>
    <div class="language-plaintext output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1.2.2
</code></pre></div>    </div>
  </blockquote>
</blockquote>

<blockquote class="keypoints">
  <h2>Key Points</h2>
  <ul>
    
    <li><p>Machine learning is the process where computers learn to recognise patterns of data.</p>
</li>
    
    <li><p>Artificial neural networks are a machine learning technique based on a model inspired by groups of neurons in the brain.</p>
</li>
    
    <li><p>Artificial neural networks can be trained on example data.</p>
</li>
    
    <li><p>Deep Learning is a machine learning technique based on using many artificial neurons arranged in layers.</p>
</li>
    
    <li><p>Neural networks learn by minimizing a loss function.</p>
</li>
    
    <li><p>Deep Learning is well suited to classification and prediction problems such as image recognition.</p>
</li>
    
    <li><p>To use Deep Learning effectively we need to go through a workflow of: defining the problem, identifying inputs and outputs, preparing data, choosing the type of network, choosing a loss function, training the model, tuning Hyperparameters, measuring performance before we can classify data.</p>
</li>
    
    <li><p>Keras is a Deep Learning library that is easier to use than many of the alternatives such as TensorFlow and PyTorch.</p>
</li>
    
  </ul>
</blockquote>

<hr />

<h1 id="classification-by-a-neural-network-using-keras" class="maintitle">Classification by a Neural Network using Keras</h1>

<blockquote class="objectives">
  <h2>Overview</h2>

  <div class="row">
    <div class="col-md-3">
      <strong>Teaching:</strong> 30-60 min
      <br />
      <strong>Exercises:</strong> 40-45 min
    </div>
    <div class="col-md-9">
      <strong>Questions</strong>
      <ul>
	
	<li><p>What is a neural network?</p>
</li>
	
	<li><p>How do I compose a Neural Network using Keras?</p>
</li>
	
	<li><p>How do I train this network on a dataset?</p>
</li>
	
	<li><p>How do I get insight into learning process?</p>
</li>
	
	<li><p>How do I measure the performance of the network?</p>
</li>
	
      </ul>
    </div>
  </div>

  <div class="row">
    <div class="col-md-3">
    </div>
    <div class="col-md-9">
      <strong>Objectives</strong>
      <ul>
	
	<li><p>Use the deep learning workflow to structure the notebook</p>
</li>
	
	<li><p>Explore the dataset using pandas and seaborn</p>
</li>
	
	<li><p>Use one-hot encoding to prepare data for classification in Keras</p>
</li>
	
	<li><p>Describe a fully connected layer</p>
</li>
	
	<li><p>Implement a fully connected layer with Keras</p>
</li>
	
	<li><p>Use Keras to train a small fully connected network on prepared data</p>
</li>
	
	<li><p>Interpret the loss curve of the training process</p>
</li>
	
	<li><p>Use a confusion matrix to measure the trained networks’ performance on a test set</p>
</li>
	
      </ul>
    </div>
  </div>

</blockquote>

<h2 id="introduction">Introduction</h2>
<p>In this episode we will learn how to create and train a Neural Network using Keras to solve a simple classification task.</p>

<p>The goal of this episode is to quickly get your hands dirty in actually defining and training a neural network, without going into depth of how neural networks work on a technical or mathematical level.
We want you to go through the most commonly used deep learning workflow that was covered
in the introduction.
As a reminder below are the steps of the deep learning workflow:</p>

<ol>
  <li>Formulate / Outline the problem</li>
  <li>Identify inputs and outputs</li>
  <li>Prepare data</li>
  <li>Choose a pretrained model or start building architecture from scratch</li>
  <li>Choose a loss function and optimizer</li>
  <li>Train the model</li>
  <li>Perform a Prediction/Classification</li>
  <li>Measure performance</li>
  <li>Tune hyperparameters</li>
  <li>Save model</li>
</ol>

<p>In this episode we will focus on a minimal example for each of these steps, later episodes will build on this knowledge to go into greater depth for some or all of these steps.</p>

<blockquote class="callout">
  <h2 id="gpu-usage">GPU usage</h2>
  <p>For this lesson having a GPU (graphics card) available is not needed.
We specifically use very small toy problems so that you do not need one.
However, Keras will use your GPU automatically when it is available.
Using a GPU becomes necessary when tackling larger datasets or complex problems which
require a more complex Neural Network.</p>
</blockquote>
<h2 id="1-formulateoutline-the-problem-penguin-classification">1. Formulate/outline the problem: penguin classification</h2>
<p>In this episode we will be using the <a href="https://zenodo.org/record/3960218">penguin dataset</a>, this is a dataset that was published in 2020 by Allison Horst and contains data on three different species of the penguins.</p>

<p>We will use the penguin dataset to train a neural network which can classify which species a
penguin belongs to, based on their physical characteristics.</p>
<blockquote class="objectives">
  <h2 id="goal">Goal</h2>
  <p>The goal is to predict a penguins’ species using the attributes available in this dataset.</p>
</blockquote>

<p>The <code class="language-plaintext highlighter-rouge">palmerpenguins</code> data contains size measurements for three penguin species observed on three islands in the Palmer Archipelago, Antarctica.
The physical attributes measured are flipper length, beak length, beak width, body mass, and sex.</p>

<p><img width="50%" src="../fig/palmer_penguins.png" alt="Illustration of the three species of penguins found in the Palmer Archipelago, Antarctica: Chinstrap, Gentoo and Adele" title="Palmer Penguins" />
<em>Artwork by @allison_horst</em></p>

<p><img width="50%" src="../fig/culmen_depth.png" alt="Illustration of the beak dimensions called culmen length and culmen depth in the dataset" title="Culmen Depth" />
<em>Artwork by @allison_horst</em></p>

<p>These data were collected from 2007 - 2009 by Dr. Kristen Gorman with the <a href="https://pal.lternet.edu/">Palmer Station Long Term Ecological Research Program</a>, part of the <a href="https://lternet.edu/">US Long Term Ecological Research Network</a>. The data were imported directly from the <a href="https://environmentaldatainitiative.org/">Environmental Data Initiative</a> (EDI) Data Portal, and are available for use by CC0 license (“No Rights Reserved”) in accordance with the <a href="https://pal.lternet.edu/data/policies">Palmer Station Data Policy</a>.</p>

<h2 id="2-identify-inputs-and-outputs">2. Identify inputs and outputs</h2>
<p>To identify the inputs and outputs that we will use to design the neural network we need to familiarize
ourselves with the dataset. This step is sometimes also called data exploration.</p>

<p>We will start by importing the <a href="https://seaborn.pydata.org/">Seaborn</a> library that will help us get the dataset and visualize it.
Seaborn is a powerful library with many visualizations. Keep in mind it requires the data to be in a
pandas dataframe, luckily the datasets available in seaborn are already in a pandas dataframe.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="n">sns</span>
</code></pre></div></div>

<p>We can load the penguin dataset using</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">penguins</span> <span class="o">=</span> <span class="n">sns</span><span class="p">.</span><span class="n">load_dataset</span><span class="p">(</span><span class="s">'penguins'</span><span class="p">)</span>
</code></pre></div></div>

<p>This will give you a pandas dataframe which contains the penguin data.</p>

<blockquote class="challenge">
  <h2 id="penguin-dataset">Penguin Dataset</h2>

  <p>Inspect the penguins dataset.</p>
  <ol>
    <li>What are the different features called in the dataframe?</li>
    <li>Are the target classes of the dataset stored as numbers or strings?</li>
    <li>How many samples does this dataset have?</li>
  </ol>

  <blockquote class="solution">
    <h2 id="solution">Solution</h2>
    <p><strong>1.</strong> Using the pandas <code class="language-plaintext highlighter-rouge">head</code> function you can see the names of the features.
Using the <code class="language-plaintext highlighter-rouge">describe</code> function we can also see some statistics for the numeric columns</p>
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">penguins</span><span class="p">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div>    </div>

    <table>
      <thead>
        <tr>
          <th style="text-align: right"> </th>
          <th style="text-align: right">species</th>
          <th style="text-align: right">island</th>
          <th style="text-align: right">bill_length_mm</th>
          <th style="text-align: right">bill_depth_mm</th>
          <th style="text-align: right">flipper_length_mm</th>
          <th style="text-align: right">body_mass_g</th>
          <th style="text-align: right">sex</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td style="text-align: right">0</td>
          <td style="text-align: right">Adelie</td>
          <td style="text-align: right">Torgersen</td>
          <td style="text-align: right">39.1</td>
          <td style="text-align: right">18.7</td>
          <td style="text-align: right">181.0</td>
          <td style="text-align: right">3750.0</td>
          <td style="text-align: right">Male</td>
        </tr>
        <tr>
          <td style="text-align: right">1</td>
          <td style="text-align: right">Adelie</td>
          <td style="text-align: right">Torgersen</td>
          <td style="text-align: right">39.5</td>
          <td style="text-align: right">17.4</td>
          <td style="text-align: right">186.0</td>
          <td style="text-align: right">3800.0</td>
          <td style="text-align: right">Female</td>
        </tr>
        <tr>
          <td style="text-align: right">2</td>
          <td style="text-align: right">Adelie</td>
          <td style="text-align: right">Torgersen</td>
          <td style="text-align: right">40.3</td>
          <td style="text-align: right">18.0</td>
          <td style="text-align: right">195.0</td>
          <td style="text-align: right">3250.0</td>
          <td style="text-align: right">Female</td>
        </tr>
        <tr>
          <td style="text-align: right">3</td>
          <td style="text-align: right">Adelie</td>
          <td style="text-align: right">Torgersen</td>
          <td style="text-align: right">NaN</td>
          <td style="text-align: right">NaN</td>
          <td style="text-align: right">NaN</td>
          <td style="text-align: right">NaN</td>
          <td style="text-align: right">NaN</td>
        </tr>
        <tr>
          <td style="text-align: right">4</td>
          <td style="text-align: right">Adelie</td>
          <td style="text-align: right">Torgersen</td>
          <td style="text-align: right">36.7</td>
          <td style="text-align: right">19.3</td>
          <td style="text-align: right">193.0</td>
          <td style="text-align: right">3450.0</td>
          <td style="text-align: right">Female</td>
        </tr>
      </tbody>
    </table>

    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">penguins</span><span class="p">.</span><span class="n">describe</span><span class="p">()</span>
</code></pre></div>    </div>

    <table>
      <thead>
        <tr>
          <th style="text-align: right"> </th>
          <th style="text-align: right">bill_length_mm</th>
          <th style="text-align: right">bill_depth_mm</th>
          <th style="text-align: right">flipper_length_mm</th>
          <th style="text-align: right">body_mass_g</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td style="text-align: right">count</td>
          <td style="text-align: right">342.000000</td>
          <td style="text-align: right">342.000000</td>
          <td style="text-align: right">342.000000</td>
          <td style="text-align: right">342.000000</td>
        </tr>
        <tr>
          <td style="text-align: right">mean</td>
          <td style="text-align: right">43.921930</td>
          <td style="text-align: right">17.151170</td>
          <td style="text-align: right">200.915205</td>
          <td style="text-align: right">4201.754386</td>
        </tr>
        <tr>
          <td style="text-align: right">std</td>
          <td style="text-align: right">5.459584</td>
          <td style="text-align: right">1.974793</td>
          <td style="text-align: right">14.061714</td>
          <td style="text-align: right">801.954536</td>
        </tr>
        <tr>
          <td style="text-align: right">min</td>
          <td style="text-align: right">32.100000</td>
          <td style="text-align: right">13.100000</td>
          <td style="text-align: right">172.000000</td>
          <td style="text-align: right">2700.000000</td>
        </tr>
        <tr>
          <td style="text-align: right">25%</td>
          <td style="text-align: right">39.225000</td>
          <td style="text-align: right">15.600000</td>
          <td style="text-align: right">190.000000</td>
          <td style="text-align: right">3550.000000</td>
        </tr>
        <tr>
          <td style="text-align: right">50%</td>
          <td style="text-align: right">44.450000</td>
          <td style="text-align: right">17.300000</td>
          <td style="text-align: right">197.000000</td>
          <td style="text-align: right">4050.000000</td>
        </tr>
        <tr>
          <td style="text-align: right">75%</td>
          <td style="text-align: right">48.500000</td>
          <td style="text-align: right">18.700000</td>
          <td style="text-align: right">213.000000</td>
          <td style="text-align: right">4750.000000</td>
        </tr>
        <tr>
          <td style="text-align: right">max</td>
          <td style="text-align: right">59.600000</td>
          <td style="text-align: right">21.500000</td>
          <td style="text-align: right">231.000000</td>
          <td style="text-align: right">6300.000000</td>
        </tr>
      </tbody>
    </table>

    <p><strong>2.</strong> We can get the unique values in the <code class="language-plaintext highlighter-rouge">species</code> column using the <code class="language-plaintext highlighter-rouge">unique</code> function of pandas.
It shows the target class is stored as a string and has 3 unique values. This type of column is
usually called a ‘categorical’ column.</p>

    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">penguins</span><span class="p">[</span><span class="s">"species"</span><span class="p">].</span><span class="n">unique</span><span class="p">()</span>
</code></pre></div>    </div>
    <div class="language-plaintext output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>array(['Adelie', 'Chinstrap', 'Gentoo'], dtype=object)
</code></pre></div>    </div>

    <p><strong>3.</strong> Using <code class="language-plaintext highlighter-rouge">describe</code> function on the species column shows there are 344 samples
unique species</p>
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">penguins</span><span class="p">[</span><span class="s">"species"</span><span class="p">].</span><span class="n">describe</span><span class="p">()</span>
</code></pre></div>    </div>
    <div class="language-plaintext output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>count        344
unique         3
top       Adelie
freq         152
Name: species, dtype: object
</code></pre></div>    </div>
  </blockquote>
</blockquote>

<h3 id="visualization">Visualization</h3>
<p>Looking at numbers like this usually does not give a very good intuition about the data we are
working with, so let us create a visualization.</p>
<h4 id="pair-plot">Pair Plot</h4>
<p>One nice visualization for datasets with relatively few attributes is the Pair Plot.
This can be created using <code class="language-plaintext highlighter-rouge">sns.pairplot(...)</code>. It shows a scatterplot of each attribute plotted against each of the other attributes.
By using the <code class="language-plaintext highlighter-rouge">hue='species'</code> setting for the pairplot the graphs on the diagonal are layered kernel density estimate plots for the different values of the <code class="language-plaintext highlighter-rouge">species</code> column.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">sns</span><span class="p">.</span><span class="n">pairplot</span><span class="p">(</span><span class="n">penguins</span><span class="p">,</span> <span class="n">hue</span><span class="o">=</span><span class="s">"species"</span><span class="p">)</span>
</code></pre></div></div>
<p><img width="66%" src="../fig/pairplot.png" alt="Pair plot showing the separability of the three species of penguin for combinations of dataset attributes" title="Pair Plot" /></p>

<blockquote class="challenge">
  <h2 id="pairplot">Pairplot</h2>

  <p>Take a look at the pairplot we created. Consider the following questions:</p>

  <ul>
    <li>Is there any class that is easily distinguishable from the others?</li>
    <li>Which combination of attributes shows the best separation for all 3 class labels at once?</li>
  </ul>

  <blockquote class="solution">
    <h2 id="solution-1">Solution</h2>
    <p>The plots show that the green class, Gentoo is somewhat more easily distinguishable from the other two.
The other two seem to be separable by a combination of bill length and bill
depth (other combinations are also possible such as bill length and flipper length).</p>
  </blockquote>
</blockquote>

<h3 id="input-and-output-selection">Input and Output Selection</h3>
<p>Now that we have familiarized ourselves with the dataset we can select the data attributes to use
as input for the neural network and the target that we want to predict.</p>

<p>In the rest of this episode we will use the <code class="language-plaintext highlighter-rouge">bill_length_mm</code>, <code class="language-plaintext highlighter-rouge">bill_depth_mm</code>, <code class="language-plaintext highlighter-rouge">flipper_length_mm</code>, <code class="language-plaintext highlighter-rouge">body_mass_g</code> attributes.
The target for the classification task will be the <code class="language-plaintext highlighter-rouge">species</code>.</p>

<blockquote class="keypoints">
  <h2 id="data-exploration">Data Exploration</h2>
  <p>Exploring the data is an important step to familiarize yourself with the problem and to help you
determine the relevant inputs and outputs.</p>
</blockquote>
<h2 id="3-prepare-data">3. Prepare data</h2>
<p>The input data and target data are not yet in a format that is suitable to use for training a neural network.</p>

<h3 id="change-types-if-needed">Change types if needed</h3>
<p>First, the species column is our categorical target, however pandas still sees it as the
generic type <code class="language-plaintext highlighter-rouge">Object</code>. We can convert this to the pandas categorical type:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">penguins</span><span class="p">[</span><span class="s">'species'</span><span class="p">]</span> <span class="o">=</span> <span class="n">penguins</span><span class="p">[</span><span class="s">'species'</span><span class="p">].</span><span class="n">astype</span><span class="p">(</span><span class="s">'category'</span><span class="p">)</span>
</code></pre></div></div>
<p>This will make later interaction with this column a little easier.</p>

<h3 id="clean-missing-values">Clean missing values</h3>
<p>During the exploration phase you may have noticed that some rows in the dataset have missing (NaN)
values, leaving such values in the input data will ruin the training, so we need to deal with them.
There are many ways to deal with missing values, but for now we will just remove the offending rows by adding a call to <code class="language-plaintext highlighter-rouge">dropna()</code>:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Drop two columns and the rows that have NaN values in them
</span><span class="n">penguins_filtered</span> <span class="o">=</span> <span class="n">penguins</span><span class="p">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s">'island'</span><span class="p">,</span> <span class="s">'sex'</span><span class="p">]).</span><span class="n">dropna</span><span class="p">()</span>

<span class="c1"># Extract columns corresponding to features
</span><span class="n">penguins_features</span> <span class="o">=</span> <span class="n">penguins_filtered</span><span class="p">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s">'species'</span><span class="p">])</span>
</code></pre></div></div>

<h3 id="prepare-target-data-for-training">Prepare target data for training</h3>
<p>Second, the target data is also in a format that cannot be used in training.
A neural network can only take numerical inputs and outputs, and learns by
calculating how “far away” the species predicted by the neural network is
from the true species.
When the target is a string category column as we have here it is very difficult to determine this “distance” or error.
Therefore we will transform this column into a more suitable format.
Again there are many ways to do this, however we will be using the one-hot encoding.
This encoding creates multiple columns, as many as there are unique values, and
puts a 1 in the column with the corresponding correct class, and 0’s in
the other columns.
For instance, for a penguin of the Adelie species the one-hot encoding would be 1 0 0</p>

<p>Fortunately pandas is able to generate this encoding for us.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>

<span class="n">target</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">get_dummies</span><span class="p">(</span><span class="n">penguins_filtered</span><span class="p">[</span><span class="s">'species'</span><span class="p">])</span>
<span class="n">target</span><span class="p">.</span><span class="n">head</span><span class="p">()</span> <span class="c1"># print out the top 5 to see what it looks like.
</span></code></pre></div></div>

<blockquote class="challenge">
  <h2 id="one-hot-encoding-vs-ordinal-encoding">One-hot encoding vs ordinal encoding</h2>

  <ol>
    <li>How many output neurons will our network have now that we
one-hot encoded the target class?</li>
    <li>Another encoding method is ‘ordinal encoding’.
Here the variable is represented by a single column,
where each category is represented by a different integer
(0, 1, 2 in the case of the 3 penguin species).
How many output neurons will a network have when ordinal encoding is used?</li>
    <li>(Optional) What would be the advantage of using one-hot versus ordinal encoding
for the task of classifying penguin species?</li>
  </ol>

  <blockquote class="solution">
    <h2 id="solution-2">Solution</h2>
    <ol>
      <li>3, one for each output variable class</li>
      <li>1, the 3 classes are represented in a single variable</li>
      <li>In this case there is no ordinal relationship between the different penguin species,
so it does not make sense to use ordinal encoding.
To give an intuition of how a machine learning model deals with ordinal encoding:
Let us say that the model predicted 0 (Gentoo) instead of the true value 2 (Adélie),
the error would in this case be 2 (2-0). But if the prediction would be 1 (Chinstrap),
the error would be 1 (2-1). A missclassification between Gentoo and Adélie would then
thus contribute more to the overall error than missclassificaiton between Chinstrap and Adélie!</li>
    </ol>

  </blockquote>
</blockquote>

<h3 id="split-data-into-training-and-test-set">Split data into training and test set</h3>
<p>Finally, we will split the dataset into a training set and a test set.
As the names imply we will use the training set to train the neural network,
while the test set is kept separate.
We will use the test set to assess the performance of the trained neural network
on unseen samples.
In many cases a validation set is also kept separate from the training and test sets (i.e. the dataset is split into 3 parts).
This validation set is then used to select the values of the parameters of the neural network and the training methods.
For this episode we will keep it at just a training and test set however.</p>

<p>To split the cleaned dataset into a training and test set we will use a very convenient
function from sklearn called <code class="language-plaintext highlighter-rouge">train_test_split</code>.
This function takes a number of parameters:</p>
<ul>
  <li>The first two are the dataset and the corresponding targets.</li>
  <li>Next is the named parameter <code class="language-plaintext highlighter-rouge">test_size</code> this is the fraction of the dataset that is
used for testing, in this case <code class="language-plaintext highlighter-rouge">0.2</code> means 20% of the data will be used for testing.</li>
  <li><code class="language-plaintext highlighter-rouge">random_state</code> controls the shuffling of the dataset, setting this value will reproduce
the same results (assuming you give the same integer) every time it is called.</li>
  <li><code class="language-plaintext highlighter-rouge">shuffle</code> which can be either <code class="language-plaintext highlighter-rouge">True</code> or <code class="language-plaintext highlighter-rouge">False</code>, it controls whether the order of the rows of the dataset is shuffled before splitting. It defaults to <code class="language-plaintext highlighter-rouge">True</code>.</li>
  <li><code class="language-plaintext highlighter-rouge">stratify</code> is a more advanced parameter that controls how the split is done. By setting it to <code class="language-plaintext highlighter-rouge">target</code> the train and test sets the function will return will have roughly the same proportions (with regards to the number of penguins of a certain species) as the dataset.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">penguins_features</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span><span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">stratify</span><span class="o">=</span><span class="n">target</span><span class="p">)</span>
</code></pre></div></div>

<blockquote class="challenge">
  <h2 id="training-and-test-sets">Training and Test sets</h2>

  <p>Take a look at the training and test set we created.</p>
  <ul>
    <li>How many samples do the training and test sets have?</li>
    <li>Are the classes in the training set well balanced?</li>
  </ul>

  <blockquote class="solution">
    <h2 id="solution-3">Solution</h2>
    <p>Using <code class="language-plaintext highlighter-rouge">y_train.shape</code> and <code class="language-plaintext highlighter-rouge">y_test.shape</code> we can see the training set has 273
samples and y_test has 69 samples.</p>

    <p>We can check the balance of classes by counting the number of ones for each
of the columns in the one-hot-encoded target,
which shows the training set has 121 Adelie, 98 Gentoo and 54 Chinstrap samples.</p>
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">y_train</span><span class="p">.</span><span class="nb">sum</span><span class="p">()</span>
</code></pre></div>    </div>
    <div class="language-plaintext output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Adelie       121
Chinstrap     54
Gentoo        98
dtype: int64
</code></pre></div>    </div>
    <p>The dataset is not perfectly balanced, but it is not orders of magnitude out of balance
either. So we will leave it as it is.</p>
  </blockquote>
</blockquote>

<h2 id="4-build-an-architecture-from-scratch-or-choose-a-pretrained-model">4. Build an architecture from scratch or choose a pretrained model</h2>

<h3 id="keras-for-neural-networks">Keras for neural networks</h3>
<p>For this lesson we will be using <a href="https://keras.io/">Keras</a> to define and train our neural network
models.
Keras is a machine learning framework with ease of use as one of its main features.
It is part of the tensorflow python package and can be imported using <code class="language-plaintext highlighter-rouge">from tensorflow import keras</code>.</p>

<p>Keras includes functions, classes and definitions to define deep learning models, cost functions and optimizers (optimizers are used to train a model).</p>

<p>Before we move on to the next section of the workflow we need to make sure we have Keras imported.
We do this as follows:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">tensorflow</span> <span class="kn">import</span> <span class="n">keras</span>
</code></pre></div></div>

<p>For this class it is useful if everyone gets the same results from their training.
Keras uses a random number generator at certain points during its execution.
Therefore we will need to set two random seeds, one for numpy and one for tensorflow:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">numpy.random</span> <span class="kn">import</span> <span class="n">seed</span>
<span class="n">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">tensorflow.random</span> <span class="kn">import</span> <span class="n">set_seed</span>
<span class="n">set_seed</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="build-a-neural-network-from-scratch">Build a neural network from scratch</h3>

<p>Now we will build a neural network from scratch, and although this sounds like
a daunting task, with Keras it is actually surprisingly straightforward.</p>

<p>With Keras you compose a neural network by creating layers and linking them
together. For now we will only use one type of layer called a fully connected
or Dense layer. In Keras this is defined by the <code class="language-plaintext highlighter-rouge">keras.layers.Dense</code> class.</p>

<p>A dense layer has a number of neurons, which is a parameter you can choose when
you create the layer.
When connecting the layer to its input and output layers every neuron in the dense
layer gets an edge (i.e. connection) to <strong><em>all</em></strong> of the input neurons and <strong><em>all</em></strong> of the output neurons.
The hidden layer in the image in the introduction of this episode is a Dense layer.</p>

<p>The input in Keras also gets special treatment, Keras automatically calculates the number of inputs
and outputs a layer needs and therefore how many edges need to be created.
This means we need to let Keras now how big our input is going to be.
We do this by instantiating a <code class="language-plaintext highlighter-rouge">keras.Input</code> class and tell it how big our input is.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">inputs</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="n">X_train</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</code></pre></div></div>

<p>We store a reference to this input class in a variable so we can pass it to the creation of
our hidden layer.
Creating the hidden layer can then be done as follows:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">hidden_layer</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">"relu"</span><span class="p">)(</span><span class="n">inputs</span><span class="p">)</span>
</code></pre></div></div>

<p>The instantiation here has 2 parameters and a seemingly strange combination of parentheses, so
let us take a closer look.
The first parameter <code class="language-plaintext highlighter-rouge">10</code> is the number of neurons we want in this layer, this is one of the
hyperparameters of our system and needs to be chosen carefully. We will get back to this in the section
on hyperparameter tuning.
The second parameter is the activation function to use, here we choose relu which is 0
for inputs that are 0 and below and the identity function (returning the same value)
for inputs above 0.
This is a commonly used activation function in deep neural networks that is proven to work well.
Next we see an extra set of parenthenses with inputs in them, this means that after creating an
instance of the Dense layer we call it as if it was a function.
This tells the Dense layer to connect the layer passed as a parameter, in this case the inputs.
Finally we store a reference so we can pass it to the output layer in a minute.</p>

<p>Now we create another layer that will be our output layer.
Again we use a Dense layer and so the call is very similar to the previous one.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">output_layer</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">"softmax"</span><span class="p">)(</span><span class="n">hidden_layer</span><span class="p">)</span>
</code></pre></div></div>
<p>Because we chose the one-hot encoding, we use <code class="language-plaintext highlighter-rouge">3</code> neurons for the output layer.</p>

<p>The softmax activation ensures that the three output neurons produce values in the range
(0, 1) and they sum to 1.
We can interpret this as a kind of ‘probability’ that the sample belongs to a certain
species.</p>

<p>Now that we have defined the layers of our neural network we can combine them into
a Keras model which facilitates training the network.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">output_layer</span><span class="p">)</span>
<span class="n">model</span><span class="p">.</span><span class="n">summary</span><span class="p">()</span>
</code></pre></div></div>

<p>The model summary here can show you some information about the neural network we have defined.</p>

<blockquote class="challenge">
  <h2 id="create-the-neural-network">Create the neural network</h2>

  <p>With the code snippets above, we defined a Keras model with 1 hidden layer with
10 neurons and an output layer with 3 neurons.</p>

  <ul>
    <li>How many parameters does the resulting model have?</li>
    <li>What happens to the number of parameters if we increase or decrease the number of neurons
in the hidden layer?</li>
  </ul>

  <blockquote class="solution">
    <h2 id="solution-4">Solution</h2>
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">inputs</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="n">X_train</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">hidden_layer</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">"relu"</span><span class="p">)(</span><span class="n">inputs</span><span class="p">)</span>
<span class="n">output_layer</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">"softmax"</span><span class="p">)(</span><span class="n">hidden_layer</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">output_layer</span><span class="p">)</span>
<span class="n">model</span><span class="p">.</span><span class="n">summary</span><span class="p">()</span>
</code></pre></div>    </div>

    <div class="language-plaintext output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Model: "model_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         [(None, 4)]               0
_________________________________________________________________
dense (Dense)                (None, 10)                50
_________________________________________________________________
dense_1 (Dense)              (None, 3)                 33
=================================================================
Total params: 83
Trainable params: 83
Non-trainable params: 0
_________________________________________________________________
</code></pre></div>    </div>

    <p>The model has 83 trainable parameters.
If you increase the number of neurons in the hidden layer the number of
trainable parameters in both the hidden and output layer increases or
decreases accordingly of neurons.
The name in quotes within the string <code class="language-plaintext highlighter-rouge">Model: "model_1"</code> may be different in your view; this detail is not important.</p>
  </blockquote>
</blockquote>

<blockquote class="callout">
  <h2 id="how-to-choose-an-architecture">How to choose an architecture?</h2>
  <p>Even for this small neural network, we had to make a choice on the number of hidden neurons.
Other choices to be made are the number of layers and type of layers (as we will see later).
You might wonder how you should make these architectural choices.
Unfortunately, there are no clear rules to follow here, and it often boils down to a lot of
trial and error. However, it is recommended to look what others have done with similar datasets and problems.
Another best practice is to start with a relatively simple architecture. Once running start to add layers and tweak the network to see if performance increases.</p>
</blockquote>

<h3 id="choose-a-pretrained-model">Choose a pretrained model</h3>
<p>If your data and problem is very similar to what others have done, you can often use a <em>pretrained network</em>.
Even if your problem is different, but the data type is common (for example images), you can use a pretrained network and finetune it for your problem.
A large number of openly available pretrained networks can be found in the <a href="https://modelzoo.co/">Model Zoo</a>, <a href="https://pytorch.org/hub/">pytorch hub</a> or <a href="https://www.tensorflow.org/hub/">tensorflow hub</a>.</p>

<h2 id="5-choose-a-loss-function-and-optimizer">5. Choose a loss function and optimizer</h2>
<p>We have now designed a neural network that in theory we should be able to
train to classify Penguins.
However, we first need to select an appropriate loss
function that we will use during training.
This loss function tells the training algorithm how wrong, or how ‘far away’ from the true
value the predicted value is.</p>

<p>For the one-hot encoding that we selected before a fitting loss function is the Categorical Crossentropy loss.
In Keras this is implemented in the <code class="language-plaintext highlighter-rouge">keras.losses.CategoricalCrossentropy</code> class.
This loss function works well in combination with the <code class="language-plaintext highlighter-rouge">softmax</code> activation function
we chose earlier.
The Categorical Crossentropy works by comparing the probabilities that the
neural network predicts with ‘true’ probabilities that we generated using the one-hot encoding.
This is a measure for how close the distribution of the three neural network outputs corresponds to the distribution of the three values in the one-hot encoding.
It is lower if the distributions are more similar.</p>

<p>For more information on the available loss functions in Keras you can check the
<a href="https://www.tensorflow.org/api_docs/python/tf/keras/losses">documentation</a>.</p>

<p>Next we need to choose which optimizer to use and, if this optimizer has parameters, what values
to use for those. Furthermore, we need to specify how many times to show the training samples to the optimizer.</p>

<p>Once more, Keras gives us plenty of choices all of which have their own pros and cons,
but for now let us go with the widely used Adam optimizer.
Adam has a number of parameters, but the default values work well for most problems.
So we will use it with its default parameters.</p>

<p>Combining this with the loss function we decided on earlier we can now compile the
model using <code class="language-plaintext highlighter-rouge">model.compile</code>.
Compiling the model prepares it to start the training.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span><span class="p">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s">'adam'</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="n">keras</span><span class="p">.</span><span class="n">losses</span><span class="p">.</span><span class="n">CategoricalCrossentropy</span><span class="p">())</span>
</code></pre></div></div>

<h2 id="6-train-model">6. Train model</h2>
<p>We are now ready to train the model.</p>

<p>Training the model is done using the <code class="language-plaintext highlighter-rouge">fit</code> method, it takes the input data and
target data as inputs and it has several other parameters for certain options
of the training.
Here we only set a different number of <code class="language-plaintext highlighter-rouge">epochs</code>.
One training epoch means that every sample in the training data has been shown
to the neural network and used to update its parameters.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
</code></pre></div></div>

<p>The fit method returns a history object that has a history attribute with the training loss and
potentially other metrics per training epoch.
It can be very insightful to plot the training loss to see how the training progresses.
Using seaborn we can do this as follow:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">sns</span><span class="p">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">history</span><span class="p">.</span><span class="n">epoch</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">history</span><span class="p">.</span><span class="n">history</span><span class="p">[</span><span class="s">'loss'</span><span class="p">])</span>
</code></pre></div></div>
<p><img width="50%" src="../fig/training_curve.png" alt="Training loss curve of the neural network training which depicts exponential decrease in loss before a plateau from ~10 epochs" title="Training Curve" /></p>

<p>This plot can be used to identify whether the training is well configured or whether there
are problems that need to be addressed.</p>

<blockquote class="challenge">
  <h2 id="the-training-curve">The Training Curve</h2>

  <p>Looking at the training curve we have just made.</p>
  <ol>
    <li>How does the training progress?
      <ul>
        <li>Does the training loss increase or decrease?</li>
        <li>Does it change quickly or slowly?</li>
        <li>Does the graph look very jittery?</li>
      </ul>
    </li>
    <li>Do you think the resulting trained network will work well on the test set?</li>
  </ol>

  <blockquote class="solution">
    <h2 id="solution-5">Solution</h2>
    <ol>
      <li>The loss curve should drop quite quickly in a smooth line with little jitter</li>
      <li>The results of the training give very little information on its performance on a test set.
You should be careful not to use it as an indication of a well trained network.</li>
    </ol>
  </blockquote>
</blockquote>

<h2 id="7-perform-a-predictionclassification">7. Perform a prediction/classification</h2>
<p>Now that we have a trained neural network, we can use it to predict new samples
of penguin using the <code class="language-plaintext highlighter-rouge">predict</code> function.</p>

<p>We will use the neural network to predict the species of the test set
using the <code class="language-plaintext highlighter-rouge">predict</code> function.
We will be using this prediction in the next step to measure the performance of our
trained network.
This will return a <code class="language-plaintext highlighter-rouge">numpy</code> matrix, which we convert
to a pandas dataframe to easily see the labels.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">prediction</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">target</span><span class="p">.</span><span class="n">columns</span><span class="p">)</span>
<span class="n">prediction</span>
</code></pre></div></div>
<blockquote class="solution">
  <h2 id="output">Output</h2>

  <table>
    <tbody>
      <tr>
        <td>0</td>
        <td>0.304484</td>
        <td>0.192893</td>
        <td>0.502623</td>
      </tr>
      <tr>
        <td>1</td>
        <td>0.527107</td>
        <td>0.095888</td>
        <td>0.377005</td>
      </tr>
      <tr>
        <td>2</td>
        <td>0.373989</td>
        <td>0.195604</td>
        <td>0.430406</td>
      </tr>
      <tr>
        <td>3</td>
        <td>0.493643</td>
        <td>0.154104</td>
        <td>0.352253</td>
      </tr>
      <tr>
        <td>4</td>
        <td>0.309051</td>
        <td>0.308646</td>
        <td>0.382303</td>
      </tr>
      <tr>
        <td>…</td>
        <td>…</td>
        <td>…</td>
        <td>…</td>
      </tr>
      <tr>
        <td>64</td>
        <td>0.406074</td>
        <td>0.191430</td>
        <td>0.402496</td>
      </tr>
      <tr>
        <td>65</td>
        <td>0.645621</td>
        <td>0.077174</td>
        <td>0.277204</td>
      </tr>
      <tr>
        <td>66</td>
        <td>0.356284</td>
        <td>0.185958</td>
        <td>0.457758</td>
      </tr>
      <tr>
        <td>67</td>
        <td>0.393868</td>
        <td>0.159575</td>
        <td>0.446557</td>
      </tr>
      <tr>
        <td>68</td>
        <td>0.509837</td>
        <td>0.144219</td>
        <td>0.345943</td>
      </tr>
    </tbody>
  </table>

</blockquote>

<p>Remember that the output of the network uses the <code class="language-plaintext highlighter-rouge">softmax</code> activation function and has three
outputs, one for each species. This dataframe shows this nicely.</p>

<p>We now need to transform this output to one penguin species per sample.
We can do this by looking for the index of highest valued output and converting that
to the corresponding species.
Pandas dataframes have the <code class="language-plaintext highlighter-rouge">idxmax</code> function, which will do exactly that.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">predicted_species</span> <span class="o">=</span> <span class="n">prediction</span><span class="p">.</span><span class="n">idxmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="s">"columns"</span><span class="p">)</span>
<span class="n">predicted_species</span>
</code></pre></div></div>
<blockquote class="solution">
  <h2 id="output-1">Output</h2>
  <div class="language-plaintext output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>0     Gentoo
1     Adelie
2     Gentoo
3     Adelie
4     Gentoo
       ...
64    Adelie
65    Adelie
66    Gentoo
67    Gentoo
68    Adelie
Length: 69, dtype: object
</code></pre></div>  </div>
</blockquote>

<h2 id="8-measuring-performance">8. Measuring performance</h2>
<p>Now that we have a trained neural network it is important to assess how well it performs.
We want to know how well it will perform in a realistic prediction scenario, measuring
performance will also come back when tuning the hyperparameters.</p>

<p>We have created a test set during the data preparation stage which we will use
now to create a confusion matrix.</p>

<h3 id="confusion-matrix">Confusion matrix</h3>
<p>With the predicted species we can now create a confusion matrix and display it
using seaborn.
To create a confusion matrix we will use another convenient function from sklearn
called <code class="language-plaintext highlighter-rouge">confusion_matrix</code>.
This function takes as a first parameter the true labels of the test set.
We can get these by using the <code class="language-plaintext highlighter-rouge">idxmax</code> method on the y_test dataframe.
The second parameter is the predicted labels which we did above.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">confusion_matrix</span>

<span class="n">true_species</span> <span class="o">=</span> <span class="n">y_test</span><span class="p">.</span><span class="n">idxmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="s">"columns"</span><span class="p">)</span>

<span class="n">matrix</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">true_species</span><span class="p">,</span> <span class="n">predicted_species</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">matrix</span><span class="p">)</span>
</code></pre></div></div>
<div class="language-plaintext output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[[22  0  8]
 [ 5  0  9]
 [ 6  0 19]]
</code></pre></div></div>

<p>Unfortunately, this matrix is kinda hard to read. Its not clear which column and which row
corresponds to which species.
So let’s convert it to a pandas dataframe with its index and columns set to the species
as follows:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Convert to a pandas dataframe
</span><span class="n">confusion_df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">matrix</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">y_test</span><span class="p">.</span><span class="n">columns</span><span class="p">.</span><span class="n">values</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">y_test</span><span class="p">.</span><span class="n">columns</span><span class="p">.</span><span class="n">values</span><span class="p">)</span>

<span class="c1"># Set the names of the x and y axis, this helps with the readability of the heatmap.
</span><span class="n">confusion_df</span><span class="p">.</span><span class="n">index</span><span class="p">.</span><span class="n">name</span> <span class="o">=</span> <span class="s">'True Label'</span>
<span class="n">confusion_df</span><span class="p">.</span><span class="n">columns</span><span class="p">.</span><span class="n">name</span> <span class="o">=</span> <span class="s">'Predicted Label'</span>
</code></pre></div></div>

<p>We can then use the <code class="language-plaintext highlighter-rouge">heatmap</code> function from seaborn to create a nice visualization of
the confusion matrix.
The <code class="language-plaintext highlighter-rouge">annot=True</code> parameter here will put the numbers from the confusion matrix in
the heatmap.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">sns</span><span class="p">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">confusion_df</span><span class="p">,</span> <span class="n">annot</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>
<p><img width="25%" src="../fig/confusion_matrix.png" alt="Confusion matrix of the test set with high accuracy for Adelie and Gentoo classification and no correctly predicted Chinstrap" title="Confusion Matrix" /></p>

<blockquote class="challenge">
  <h2 id="confusion-matrix-1">Confusion Matrix</h2>

  <p>Measure the performance of the neural network you trained and
visualize a confusion matrix.</p>

  <ul>
    <li>Did the neural network perform well on the test set?</li>
    <li>Did you expect this from the training loss you saw?</li>
    <li>What could we do to improve the performance?</li>
  </ul>

  <blockquote class="solution">
    <h2 id="solution-6">Solution</h2>

    <p>The confusion matrix shows that the predictions for Adelie and Gentoo
are decent, but could be improved. However, Chinstrap is not predicted
ever.</p>

    <p>The training loss was very low, so from that perspective this may be
surprising.
But this illustrates very well why a test set is important when training
neural networks.</p>

    <p>We can try many things to improve the performance from here.
One of the first things we can try is to balance the dataset better.
Other options include: changing the network architecture or changing the
training parameters</p>
  </blockquote>
</blockquote>

<h2 id="9-tune-hyperparameters">9. Tune hyperparameters</h2>
<p>As we discussed before the design and training of a neural network comes with
many hyper parameter choices.
We will go into more depth of these hyperparameters in later episodes.
For now it is important to realize that the parameters we chose were
somewhat arbitrary and more careful consideration needs to be taken to
pick hyperparameter values.</p>

<h2 id="10-share-model">10. Share model</h2>
<p>It is very useful to be able to use the trained neural network at a later
stage without having to retrain it.
This can be done by using the <code class="language-plaintext highlighter-rouge">save</code> method of the model.
It takes a string as a parameter which is the path of a directory where the model is stored.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span><span class="p">.</span><span class="n">save</span><span class="p">(</span><span class="s">'my_first_model'</span><span class="p">)</span>
</code></pre></div></div>

<p>This saved model can be loaded again by using the <code class="language-plaintext highlighter-rouge">load_model</code> method as follows:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">pretrained_model</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">models</span><span class="p">.</span><span class="n">load_model</span><span class="p">(</span><span class="s">'my_first_model'</span><span class="p">)</span>
</code></pre></div></div>

<p>This loaded model can be used as before to predict.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># use the pretrained model here
</span><span class="n">y_pretrained_pred</span> <span class="o">=</span> <span class="n">pretrained_model</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">pretrained_prediction</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">y_pretrained_pred</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">target</span><span class="p">.</span><span class="n">columns</span><span class="p">.</span><span class="n">values</span><span class="p">)</span>

<span class="c1"># idxmax will select the column for each row with the highest value
</span><span class="n">pretrained_predicted_species</span> <span class="o">=</span> <span class="n">pretrained_prediction</span><span class="p">.</span><span class="n">idxmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="s">"columns"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">pretrained_predicted_species</span><span class="p">)</span>
</code></pre></div></div>
<blockquote class="solution">
  <h2 id="output-2">Output</h2>

  <div class="language-plaintext output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>0     Adelie
1     Gentoo
2     Adelie
3     Gentoo
4     Gentoo
       ...
64    Gentoo
65    Gentoo
66    Adelie
67    Adelie
68    Gentoo
Length: 69, dtype: object
</code></pre></div>  </div>
</blockquote>

<blockquote class="keypoints">
  <h2>Key Points</h2>
  <ul>
    
    <li><p>The deep learning workflow is a useful tool to structure your approach, it helps to make sure you do not forget any important steps.</p>
</li>
    
    <li><p>Exploring the data is an important step to familiarize yourself with the problem and to help you determine the relavent inputs and outputs.</p>
</li>
    
    <li><p>One-hot encoding is a preprocessing step to prepare labels for classification in Keras.</p>
</li>
    
    <li><p>A fully connected layer is a layer which has connections to all neurons in the previous and subsequent layers.</p>
</li>
    
    <li><p>keras.layers.Dense is an implementation of a fully connected layer, you can set the number of neurons in the layer and the activation function used.</p>
</li>
    
    <li><p>To train a neural network with Keras we need to first define the network using layers and the Model class. Then we can train it using the model.fit function.</p>
</li>
    
    <li><p>Plotting the loss curve can be used to identify and troubleshoot the training process.</p>
</li>
    
    <li><p>The loss curve on the training set does not provide any information on how well a network performs in a real setting.</p>
</li>
    
    <li><p>Creating a confusion matrix with results from a test set gives better insight into the network’s performance.</p>
</li>
    
  </ul>
</blockquote>

<hr />

<h1 id="monitor-the-training-process" class="maintitle">Monitor the training process</h1>

<blockquote class="objectives">
  <h2>Overview</h2>

  <div class="row">
    <div class="col-md-3">
      <strong>Teaching:</strong> 135 min
      <br />
      <strong>Exercises:</strong> 80 min
    </div>
    <div class="col-md-9">
      <strong>Questions</strong>
      <ul>
	
	<li><p>How do I create a neural network for a regression task?</p>
</li>
	
	<li><p>How do neural networks learn?</p>
</li>
	
	<li><p>How do I monitor the training process?</p>
</li>
	
	<li><p>How do I detect (and avoid) overfitting?</p>
</li>
	
	<li><p>What are common options to improve the model performance?</p>
</li>
	
      </ul>
    </div>
  </div>

  <div class="row">
    <div class="col-md-3">
    </div>
    <div class="col-md-9">
      <strong>Objectives</strong>
      <ul>
	
	<li><p>Explain the importance of keeping your test set clean, by validating on the validation set instead of the test set</p>
</li>
	
	<li><p>Use the data splits to plot the training process</p>
</li>
	
	<li><p>Explain how optimization works</p>
</li>
	
	<li><p>Design a neural network for a regression task</p>
</li>
	
	<li><p>Measure the performance of your deep neural network</p>
</li>
	
	<li><p>Interpret the training plots to recognize overfitting</p>
</li>
	
	<li><p>Use normalization as preparation step for Deep Learning</p>
</li>
	
	<li><p>Implement basic strategies to prevent overfitting</p>
</li>
	
      </ul>
    </div>
  </div>

</blockquote>

<p>In this episode we will explore how to monitor the training progress, evaluate our the model predictions and finetune the model to avoid over-fitting. For that we will use a more complicated weather data-set.</p>

<h2 id="1-formulate--outline-the-problem-weather-prediction">1. Formulate / Outline the problem: weather prediction</h2>

<p>Here we want to work with the <em>weather prediction dataset</em> (the light version) which can be
<a href="https://doi.org/10.5281/zenodo.5071376">downloaded from Zenodo</a>.
It contains daily weather observations from 11 different European cities or places through the
years 2000 to 2010. For all locations the data contains the variables ‘mean temperature’, ‘max temperature’, and ‘min temperature’. In addition, for multiple locations, the following variables are provided: ‘cloud_cover’, ‘wind_speed’, ‘wind_gust’, ‘humidity’, ‘pressure’, ‘global_radiation’, ‘precipitation’, ‘sunshine’, but not all of them are provided for every location. A more extensive description of the dataset including the different physical units is given in accompanying metadata file. The full dataset comprises of 10 years (3654 days) of collected weather data across Europe.</p>

<p><img src="../fig/03_weather_prediction_dataset_map.png" alt="18 European locations in the weather prediction dataset" /></p>

<p>A very common task with weather data is to make a prediction about the weather sometime in the future, say the next day. In this episode, we will try to predict tomorrow’s sunshine hours, a challenging-to-predict feature, using a neural network with the available weather data for one location: BASEL.</p>

<h2 id="2-identify-inputs-and-outputs">2. Identify inputs and outputs</h2>

<h3 id="import-dataset">Import Dataset</h3>
<p>We will now import and explore the weather data-set:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>

<span class="n">filename_data</span> <span class="o">=</span> <span class="s">"weather_prediction_dataset_light.csv"</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">filename_data</span><span class="p">)</span>
<span class="n">data</span><span class="p">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div></div>

<table class="output">
  <thead>
    <tr>
      <th style="text-align: right"> </th>
      <th style="text-align: right">DATE</th>
      <th style="text-align: right">MONTH</th>
      <th style="text-align: right">BASEL_cloud_cover</th>
      <th style="text-align: right">BASEL_humidity</th>
      <th style="text-align: right">BASEL_pressure</th>
      <th style="text-align: right">…</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: right">0</td>
      <td style="text-align: right">20000101</td>
      <td style="text-align: right">1</td>
      <td style="text-align: right">8</td>
      <td style="text-align: right">0.89</td>
      <td style="text-align: right">1.0286</td>
      <td style="text-align: right">…</td>
    </tr>
    <tr>
      <td style="text-align: right">1</td>
      <td style="text-align: right">20000102</td>
      <td style="text-align: right">1</td>
      <td style="text-align: right">8</td>
      <td style="text-align: right">0.87</td>
      <td style="text-align: right">1.0318</td>
      <td style="text-align: right">…</td>
    </tr>
    <tr>
      <td style="text-align: right">2</td>
      <td style="text-align: right">20000103</td>
      <td style="text-align: right">1</td>
      <td style="text-align: right">5</td>
      <td style="text-align: right">0.81</td>
      <td style="text-align: right">1.0314</td>
      <td style="text-align: right">…</td>
    </tr>
    <tr>
      <td style="text-align: right">3</td>
      <td style="text-align: right">20000104</td>
      <td style="text-align: right">1</td>
      <td style="text-align: right">7</td>
      <td style="text-align: right">0.79</td>
      <td style="text-align: right">1.0262</td>
      <td style="text-align: right">…</td>
    </tr>
    <tr>
      <td style="text-align: right">4</td>
      <td style="text-align: right">20000105</td>
      <td style="text-align: right">1</td>
      <td style="text-align: right">5</td>
      <td style="text-align: right">0.90</td>
      <td style="text-align: right">1.0246</td>
      <td style="text-align: right">…</td>
    </tr>
  </tbody>
</table>

<blockquote class="callout">
  <h2 id="load-the-data">Load the data</h2>
  <p>If you have not downloaded the data yet, you can also load it directly from Zenodo:</p>
  <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">"https://zenodo.org/record/5071376/files/weather_prediction_dataset_light.csv?download=1"</span><span class="p">)</span>
</code></pre></div>  </div>
</blockquote>

<h3 id="brief-exploration-of-the-data">Brief exploration of the data</h3>
<p>Let us start with a quick look at the type of features that we find in the data.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">data</span><span class="p">.</span><span class="n">columns</span>
</code></pre></div></div>

<div class="language-plaintext output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Index(['DATE', 'MONTH', 'BASEL_cloud_cover', 'BASEL_humidity',
       'BASEL_pressure', 'BASEL_global_radiation', 'BASEL_precipitation',
       'BASEL_sunshine', 'BASEL_temp_mean', 'BASEL_temp_min', 'BASEL_temp_max',
        ...
       'SONNBLICK_temp_min', 'SONNBLICK_temp_max', 'TOURS_humidity',
       'TOURS_pressure', 'TOURS_global_radiation', 'TOURS_precipitation',
       'TOURS_temp_mean', 'TOURS_temp_min', 'TOURS_temp_max'],
      dtype='object')
</code></pre></div></div>

<blockquote class="challenge">
  <h2 id="exercise-explore-the-dataset">Exercise: Explore the dataset</h2>

  <p>Let’s get a quick idea of the dataset.</p>

  <ul>
    <li>How many data points do we have?</li>
    <li>How many features does the data have (don’t count month and date as a feature)?</li>
    <li>What are the different types of measurements (humidity etc.) in the data and how many are there?</li>
    <li>(Optional) Plot the amount of sunshine hours in Basel over the course of a year. Are there any interesting properties that you notice?</li>
  </ul>

  <blockquote class="solution">
    <h2 id="solution">Solution</h2>
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">data</span><span class="p">.</span><span class="n">shape</span>
</code></pre></div>    </div>
    <p>This will give both the number of datapoints (3654) and the number of features (89 + month +
date).</p>

    <p>To see what type of features the data contains we could run something like:</p>
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">string</span>
<span class="k">print</span><span class="p">({</span><span class="n">x</span><span class="p">.</span><span class="n">lstrip</span><span class="p">(</span><span class="n">string</span><span class="p">.</span><span class="n">ascii_uppercase</span> <span class="o">+</span> <span class="s">"_"</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">data</span><span class="p">.</span><span class="n">columns</span> <span class="k">if</span> <span class="n">x</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s">"MONTH"</span><span class="p">,</span> <span class="s">"DATE"</span><span class="p">]})</span>
</code></pre></div>    </div>
    <div class="language-plaintext output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{'cloud_cover', 'precipitation', 'sunshine', 'global_radiation', 'temp_mean', 'humidity', 'pressure', 'temp_min', 'temp_max'}
</code></pre></div>    </div>
    <p>An alternative way which is slightly more complicated but gives better results is using regex.</p>
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import re
feature_names = set()
for col in data.columns:
    feature_names.update(re.findall('[^A-Z]{2,}', col))

feature_names
</code></pre></div>    </div>
    <p>In total there are 9 different measured variables.</p>

    <h3 id="optional-exercise">Optional exercise</h3>
    <p>You can plot the sunshine hours in Basel as follows:</p>
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">data</span><span class="p">.</span><span class="n">iloc</span><span class="p">[:</span><span class="mi">365</span><span class="p">][</span><span class="s">'BASEL_sunshine'</span><span class="p">].</span><span class="n">plot</span><span class="p">(</span><span class="n">xlabel</span><span class="o">=</span><span class="s">"Day"</span><span class="p">,</span><span class="n">ylabel</span><span class="o">=</span><span class="s">"Basel sunchine hours"</span><span class="p">)</span>
</code></pre></div>    </div>
    <p><img src="../fig/03_exploration_basel_sunshine_graph.png" alt="Plot of Basel sunshine hours in one year" width="500px" />
There are a couple of things that might stand out to you. For example, it looks like the sunshine
hours are fluctuating a lot per day. There also seems to be seasonal fluctuation, with the peaks
becoming higher around the middle of the year.</p>
  </blockquote>
</blockquote>

<h2 id="3-prepare-data">3. Prepare data</h2>

<h3 id="select-a-subset-and-split-into-data-x-and-labels-y">Select a subset and split into data (X) and labels (y)</h3>
<p>The full dataset comprises of 10 years (3654 days) from which we will select only the first 3 years. The present dataset is sorted by “DATE”, so for each row <code class="language-plaintext highlighter-rouge">i</code> in the table we can pick a corresponding feature and location from row <code class="language-plaintext highlighter-rouge">i+1</code> that we later want to predict with our model. As outlined in step 1, we would like to predict the sunshine hours for the location: BASEL.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">nr_rows</span> <span class="o">=</span> <span class="mi">365</span><span class="o">*</span><span class="mi">3</span>
<span class="c1"># data
</span><span class="n">X_data</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">loc</span><span class="p">[:</span><span class="n">nr_rows</span><span class="p">].</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s">'DATE'</span><span class="p">,</span> <span class="s">'MONTH'</span><span class="p">])</span>

<span class="c1"># labels (sunshine hours the next day)
</span><span class="n">y_data</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="mi">1</span><span class="p">:(</span><span class="n">nr_rows</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)][</span><span class="s">"BASEL_sunshine"</span><span class="p">]</span>
</code></pre></div></div>

<p>In general, it is important to check if the data contains any unexpected values such as <code class="language-plaintext highlighter-rouge">9999</code> or <code class="language-plaintext highlighter-rouge">NaN</code> or <code class="language-plaintext highlighter-rouge">NoneType</code>. You can use the pandas <code class="language-plaintext highlighter-rouge">data.describe()</code> or <code class="language-plaintext highlighter-rouge">data.isnull()</code> function for this. If so, such values must be removed or replaced.
In the present case the data is luckily well prepared and shouldn’t contain such values, so that this step can be omitted.</p>

<h3 id="split-data-and-labels-into-training-validation-and-test-set">Split data and labels into training, validation, and test set</h3>

<p>As with classical machine learning techniques, it is required in deep learning to split off a hold-out <em>test set</em> which remains untouched during model training and tuning. It is later used to evaluate the model performance. On top, we will also split off an additional <em>validation set</em>, the reason of which will hopefully become clearer later in this lesson.</p>

<p>To make our lives a bit easier, we employ a trick to create these 3 datasets, <code class="language-plaintext highlighter-rouge">training set</code>, <code class="language-plaintext highlighter-rouge">test set</code> and <code class="language-plaintext highlighter-rouge">validation set</code>, by calling the <code class="language-plaintext highlighter-rouge">train_test_split</code> method of <code class="language-plaintext highlighter-rouge">scikit-learn</code> twice.</p>

<p>First we create the training set and leave the remainder of 30 % of the data to the two hold-out sets.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_holdout</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_holdout</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X_data</span><span class="p">,</span> <span class="n">y_data</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</code></pre></div></div>

<p>Now we split the 30 % of the data in two equal sized parts.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">X_val</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_val</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X_holdout</span><span class="p">,</span> <span class="n">y_holdout</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</code></pre></div></div>

<p>Setting the <code class="language-plaintext highlighter-rouge">random_state</code> to <code class="language-plaintext highlighter-rouge">0</code> is a short-hand at this point. Note however, that changing this seed of the pseudo-random number generator will also change the composition of your data sets. For the sake of reproducibility, this is one example of a parameters that should not change at all.</p>

<h2 id="4-choose-a-pretrained-model-or-start-building-architecture-from-scratch">4. Choose a pretrained model or start building architecture from scratch</h2>

<h3 id="regression-and-classification">Regression and classification</h3>

<p>In episode 2 we trained a dense neural network on a <em>classification task</em>. For this one hot encoding was used together with a <code class="language-plaintext highlighter-rouge">Categorical Crossentropy</code> loss function.
This measured how close the distribution of the neural network outputs corresponds to the distribution of the three values in the one hot encoding.
Now we want to work on a <em>regression task</em>, thus not predicting a class label (or integer number) for a datapoint. In regression, we like to predict one (and sometimes many) values of a feature. This is typically a floating point number.</p>

<blockquote class="challenge">
  <h2 id="exercise-architecture-of-the-network">Exercise: Architecture of the network</h2>
  <p>As we want to design a neural network architecture for a regression task,
see if you can first come up with the answers to the following questions:</p>
  <ol>
    <li>What must be the dimension of our input layer?</li>
    <li>We want to output the prediction of a single number. The output layer of the NN hence cannot be the same as for the classification task earlier. This is because the <code class="language-plaintext highlighter-rouge">softmax</code> activation being used had a concrete meaning with respect to the class labels which is not needed here. What output layer design would you choose for regression?
Hint: A layer with <code class="language-plaintext highlighter-rouge">relu</code> activation, with <code class="language-plaintext highlighter-rouge">sigmoid</code> activation or no activation at all?</li>
    <li>(Optional) How would we change the model if we would like to output a prediction of the precipitation in Basel in <em>addition</em> to the sunshine hours?</li>
  </ol>

  <blockquote class="solution">
    <h2 id="solution-1">Solution</h2>

    <ol>
      <li>The shape of the input layer has to correspond to the number of features in our data: 89</li>
      <li>The output is a single value per prediction, so the output layer can consist of a dense layer with only one node. The <em>softmax</em> activiation function works well for a classification task, but here we do not want to restrict the possible outcomes to the range of zero and one. In fact, we can omit the activation in the output layer.</li>
      <li>The output layer should have 2 neurons, one for each number that we try to predict. Our y_train (and val and test) then becomes a (n_samples, 2) matrix.</li>
    </ol>
  </blockquote>
</blockquote>

<p>In our example we want to predict the sunshine hours in Basel (or any other place in the dataset) for tomorrow based on the weather data of all 18 locations today. <code class="language-plaintext highlighter-rouge">BASEL_sunshine</code> is a floating point value (i.e. <code class="language-plaintext highlighter-rouge">float64</code>). The network should hence output a single float value which is why the last layer of our network will only consist of a single node.</p>

<p>We compose a network of two hidden layers to start off with something. We go by a scheme with 100 neurons in the first hidden layer and 50 neurons in the second layer. As activation function we settle on the <code class="language-plaintext highlighter-rouge">relu</code> function as a it proved very robust and widely used. To make our live easier later, we wrap the definition of the network in a method called <code class="language-plaintext highlighter-rouge">create_nn</code>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">tensorflow</span> <span class="kn">import</span> <span class="n">keras</span>

<span class="k">def</span> <span class="nf">create_nn</span><span class="p">():</span>
    <span class="c1"># Input layer
</span>    <span class="n">inputs</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">X_data</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],),</span> <span class="n">name</span><span class="o">=</span><span class="s">'input'</span><span class="p">)</span>

    <span class="c1"># Dense layers
</span>    <span class="n">layers_dense</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="s">'relu'</span><span class="p">)(</span><span class="n">inputs</span><span class="p">)</span>
    <span class="n">layers_dense</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="s">'relu'</span><span class="p">)(</span><span class="n">layers_dense</span><span class="p">)</span>

    <span class="c1"># Output layer
</span>    <span class="n">outputs</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">)(</span><span class="n">layers_dense</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">keras</span><span class="p">.</span><span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s">"weather_prediction_model"</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">create_nn</span><span class="p">()</span>
</code></pre></div></div>

<p>The shape of the input layer has to correspond to the number of features in our data: <code class="language-plaintext highlighter-rouge">89</code>. We use <code class="language-plaintext highlighter-rouge">X_data.shape[1]</code> to obtain this value dynamically</p>

<p>The output layer here is a dense layer with only 1 node. And we here have chosen to use <em>no activation function</em>.
While we might use <em>softmax</em> for a classification task, here we do not want to restrict the possible outcomes for a start.</p>

<p>In addition, we have here chosen to write the network creation as a function so that we can use it later again to initiate new models.</p>

<p>Let us check how our model looks like by calling the <code class="language-plaintext highlighter-rouge">summary</code> method.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span><span class="p">.</span><span class="n">summary</span><span class="p">()</span>
</code></pre></div></div>
<div class="language-plaintext output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Model: "weather_prediction_model"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input (InputLayer)           [(None, 89)]              0
_________________________________________________________________
dense (Dense)                (None, 100)               9000
_________________________________________________________________
dense_1 (Dense)              (None, 50)                5050
_________________________________________________________________
dense_2 (Dense)              (None, 1)                 51
=================================================================
Total params: 14,101
Trainable params: 14,101
Non-trainable params: 0
</code></pre></div></div>

<p>When compiling the model we can define a few very important aspects. We will discuss them now in more detail.</p>

<h2 id="intermezzo-how-do-neural-networks-learn">Intermezzo: How do neural networks learn?</h2>
<p>In the introduction we learned about the loss function: it quantifies the total error of the predictions made by the model.
During model training we aim to find the model parameters that minimize the loss.
This is called optimization, but how does optimization actually work?</p>

<h3 id="gradient-descent">Gradient descent</h3>
<p>Gradient descent is a widely used optimization algorithm, most other optimization algorithms are based on it.
It works as follows: Imagine a neural network with only one neuron.
Take a look at the figure below. The plot shows the loss as a function of the weight of the neuron.
As you can see there is a global loss minimum, we would like to find the weight at this point in the parabola.
To do this, we initialize the model weight with some random value. Then we compute the gradient of the loss function with respect
to the weight. This tells us how much the loss function will change if we change the weight by a small amount.
Then, we update the weight by taking a small step in the direction of the negative gradient, so down the slope.
This will slightly decrease the loss. This process is repeated until the loss function reaches a minimum.
The size of the step that is taken in each iteration is called the ‘learning rate’.
<img src="../fig/03_gradient_descent.png" alt="Plot of the loss as a function of the weights. Through gradient descent the global loss minimum is found" />.</p>

<h3 id="batch-gradient-descent">Batch gradient descent</h3>
<p>You could use the entire training dataset to perform one learning step in gradient descent,
which would mean that one epoch equals one learning step.
In practice, in each learning step we only use a subset of the training data to compute the loss and the gradients.
This subset is called a ‘batch’, the number of samples in one batch is called the ‘batch size’.</p>

<blockquote class="challenge">
  <h2 id="exercise-gradient-descent">Exercise: Gradient descent</h2>
  <h3 id="1-what-is-the-goal-of-optimization">1. What is the goal of optimization?</h3>
  <ul>
    <li>A. To find the weights that maximize the loss function</li>
    <li>B. To find the weights that minimize the loss function</li>
  </ul>

  <h3 id="2-what-happens-in-one-gradient-descent-step">2. What happens in one gradient descent step?</h3>
  <ul>
    <li>A. The weights are adjusted so that we move in the direction of the gradient, so up the slope of the loss function</li>
    <li>B. The weights are adjusted so that we move in the direction of the gradient, so down the slope of the loss function</li>
    <li>C. The weights are adjusted so that we move in the direction of the negative gradient, so up the slope of the loss function</li>
    <li>D. The weights are adjusted so that we move in the direction of the negative gradient, so down the slope of the loss function</li>
  </ul>

  <h3 id="3-when-the-batch-size-is-increased">3. When the batch size is increased:</h3>
  <p>(multiple answers might apply)</p>
  <ul>
    <li>A. The number of samples in an epoch also increases</li>
    <li>B. The number of batches in an epoch goes down</li>
    <li>C. The training progress is less jumpy, because more samples are consulted in each update step (one batch).</li>
    <li>D. The memory load (memory as in computer hardware) of the training process is increased</li>
  </ul>

  <blockquote class="solution">
    <h2 id="solution-2">Solution</h2>

    <ol>
      <li>Correct answer: B. To find the weights that minimize the loss function.
The loss function quantifies the total error of the network, we want to have the smallest error as possible, hence we minimize the loss.</li>
      <li>
        <p>Correct answer: D The weights are adjusted so that we move in the direction of the negative gradient, so down the slope of the loss function.
We want to move towards the global minimum, so in the opposite direction of the gradient.</p>
      </li>
      <li>Correct answer: B &amp; D
        <ul>
          <li>A. The number of samples in an epoch also increases (incorrect, an epoch is always defined as passing through the training data for one cycle)</li>
          <li>B. The number of batches in an epoch goes down (correct, the number of batches is the samples in an epoch divided by the batch size)</li>
          <li>C. The training progress is more jumpy, because more samples are consulted in each update step (one batch). (incorrect, more samples are consulted in each update step, but this makes the progress less jumpy since you get a more accurate estimate of the loss in the entire dataset)</li>
          <li>D. The memory load (memory as in computer hardware) of the training process is increased (correct, the data is begin loaded one batch at a time, so more samples means more memory usage)</li>
        </ul>
      </li>
    </ol>
  </blockquote>
</blockquote>

<h2 id="5-choose-a-loss-function-and-optimizer">5. Choose a loss function and optimizer</h2>
<h3 id="loss-function">Loss function</h3>
<p>The loss is what the neural network will be optimized on during training, so choosing a suitable loss function is crucial for training neural networks.
In the given case we want to stimulate that the predicted values are as close as possible to the true values. This is commonly done by using the <em>mean squared error</em> (mse) or the <em>mean absolute error</em> (mae), both of which should work OK in this case. Often, mse is preferred over mae because it “punishes” large prediction errors more severely.
In Keras this is implemented in the <code class="language-plaintext highlighter-rouge">keras.losses.MeanSquaredError</code> class (see Keras documentation: https://keras.io/api/losses/). This can be provided into the <code class="language-plaintext highlighter-rouge">model.compile</code> method with the <code class="language-plaintext highlighter-rouge">loss</code> parameter and setting it to <code class="language-plaintext highlighter-rouge">mse</code>, e.g.</p>

<!--cce:skip-->
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span><span class="p">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s">'mse'</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="optimizer">Optimizer</h3>

<p>Somewhat coupled to the loss function is the <em>optimizer</em> that we want to use.
The <em>optimizer</em> here refers to the algorithm with which the model learns to optimize on the provided loss function. A basic example for such an optimizer would be <em>stochastic gradient descent</em>. For now, we can largely skip this step and pick one of the most common optimizers that works well for most tasks: the <em>Adam optimizer</em>. Similar to activation functions, the choice of optimizer depends on the problem you are trying to solve, your model architecture and your data. <em>Adam</em> is a good starting point though, which is why we chose it.</p>

<!--cce:skip-->
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span><span class="p">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s">'adam'</span><span class="p">,</span>
              <span class="n">loss</span><span class="o">=</span><span class="s">'mse'</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="metrics">Metrics</h3>

<p>In our first example (episode 2) we plotted the progression of the loss during training.
That is indeed a good first indicator if things are working alright, i.e. if the loss is indeed decreasing as it should with the number of epochs.
However, when models become more complicated then also the loss functions often become less intuitive.
That is why it is good practice to monitor the training process with additional, more intuitive metrics.
They are not used to optimize the model, but are simply recorded during training.
With Keras such additional metrics can be added via <code class="language-plaintext highlighter-rouge">metrics=[...]</code> parameter and can contain one or multiple metrics of interest.
Here we could for instance chose to use <code class="language-plaintext highlighter-rouge">'mae'</code> the mean absolute error, or the the <em>root mean squared error</em> (RMSE) which unlike the <em>mse</em> has the same units as the predicted values. For the sake of units, we choose the latter.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span><span class="p">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s">'adam'</span><span class="p">,</span>
              <span class="n">loss</span><span class="o">=</span><span class="s">'mse'</span><span class="p">,</span>
              <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="n">keras</span><span class="p">.</span><span class="n">metrics</span><span class="p">.</span><span class="n">RootMeanSquaredError</span><span class="p">()])</span>
</code></pre></div></div>

<p>Let’s create a <code class="language-plaintext highlighter-rouge">compile_model</code> function to easily compile the model throughout this lesson:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">compile_model</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
    <span class="n">model</span><span class="p">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s">'adam'</span><span class="p">,</span>
                  <span class="n">loss</span><span class="o">=</span><span class="s">'mse'</span><span class="p">,</span>
                  <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="n">keras</span><span class="p">.</span><span class="n">metrics</span><span class="p">.</span><span class="n">RootMeanSquaredError</span><span class="p">()])</span>
<span class="n">compile_model</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</code></pre></div></div>

<p>With this, we complete the compilation of our network and are ready to start training.</p>

<h2 id="6-train-the-model">6. Train the model</h2>

<p>Now that we created and compiled our dense neural network, we can start training it.
One additional concept we need to introduce though, is the <code class="language-plaintext highlighter-rouge">batch_size</code>.
This defines how many samples from the training data will be used to estimate the error gradient before the model weights are updated.
Larger batches will produce better, more accurate gradient estimates but also less frequent updates of the weights.
Here we are going to use a batch size of 32 which is a common starting point.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span>
                    <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
                    <span class="n">epochs</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span>
                    <span class="n">verbose</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</code></pre></div></div>

<p>We can plot the training process using the <code class="language-plaintext highlighter-rouge">history</code> object returned from the model training.
We will create a function for it, because we will make use of this more often in this lesson!</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="n">sns</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="k">def</span> <span class="nf">plot_history</span><span class="p">(</span><span class="n">metrics</span><span class="p">):</span>
    <span class="s">"""
    Plot the training history

    Args:
        metrics(str, list): Metric or a list of metrics to plot
    """</span>
    <span class="n">history_df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">.</span><span class="n">from_dict</span><span class="p">(</span><span class="n">history</span><span class="p">.</span><span class="n">history</span><span class="p">)</span>
    <span class="n">sns</span><span class="p">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">history_df</span><span class="p">[</span><span class="n">metrics</span><span class="p">])</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">"epochs"</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">"RMSE"</span><span class="p">)</span>

<span class="n">plot_history</span><span class="p">(</span><span class="s">'root_mean_squared_error'</span><span class="p">)</span>
</code></pre></div></div>
<p><img src="../fig/03_training_history_1_rmse.png" alt="Plot of the RMSE over epochs for the trained model that shows a decreasing error metric" width="500px" /></p>

<p>This looks very promising! Our metric (“RMSE”) is dropping nicely and while it maybe keeps fluctuating a bit it does end up at fairly low <em>RMSE</em> values.
But the <em>RMSE</em> is just the root <em>mean</em> squared error, so we might want to look a bit more in detail how well our just trained model does in predicting the sunshine hours.</p>

<h2 id="7-perform-a-predictionclassification">7. Perform a Prediction/Classification</h2>
<p>Now that we have our model trained, we can make a prediction with the model before measuring the performance of our neural network.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">y_train_predicted</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">y_test_predicted</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="8-measure-performance">8. Measure performance</h2>

<p>There is not a single way to evaluate how a model performs. But there are at least two very common approaches. For a <em>classification task</em> that is to compute a <em>confusion matrix</em> for the test set which shows how often particular classes were predicted correctly or incorrectly.</p>

<p>For the present <em>regression task</em>, it makes more sense to compare true and predicted values in a scatter plot.</p>

<p>So, let’s look at how the predicted sunshine hour have developed with reference to their ground truth values.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># We define a function that we will reuse in this lesson
</span><span class="k">def</span> <span class="nf">plot_predictions</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y_true</span><span class="p">,</span> <span class="n">title</span><span class="p">):</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">style</span><span class="p">.</span><span class="n">use</span><span class="p">(</span><span class="s">'ggplot'</span><span class="p">)</span>  <span class="c1"># optional, that's only to define a visual style
</span>    <span class="n">plt</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y_true</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">"predicted sunshine hours"</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">"true sunshine hours"</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>

<span class="n">plot_predictions</span><span class="p">(</span><span class="n">y_train_predicted</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s">'Predictions on the training set'</span><span class="p">)</span>
</code></pre></div></div>
<p><img src="../fig/03_regression_predictions_trainset.png" alt="Scatter plot between predictions and true sunshine hours in Basel on the train set showing a concise spread" /></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>plot_predictions(y_test_predicted, y_test, title='Predictions on the test set')
</code></pre></div></div>
<p><img src="../fig/03_regression_predictions_testset.png" alt="Scatter plot between predictions and true sunshine hours in Basel on the test set showing a wide spread" /></p>

<blockquote class="language-python challenge">
  <h2 id="exercise-reflecting-on-our-results">Exercise: Reflecting on our results</h2>
  <ul>
    <li>Is the performance of the model as you expected (or better/worse)?</li>
    <li>Is there a noteable difference between training set and test set? And if so, any idea why?</li>
    <li>(Optional) When developing a model, you will often vary different aspects of your model like
which features you use, model parameters and architecture. It is important to settle on a
single-number evaluation metric to compare your models.
      <ul>
        <li>What single-number evaluation metric would you choose here and why?</li>
      </ul>
    </li>
  </ul>

  <blockquote class="solution">
    <h2 id="solution-3">Solution</h2>

    <p>While the performance on the train set seems reasonable, the performance on the test set is much worse.
This is a common problem called <strong>overfitting</strong>, which we will discuss in more detail later.</p>

    <p>Optional exercise:
Mean accuracy would be a single-value metric that you can use in this case.</p>
  </blockquote>
</blockquote>

<p>The accuracy on the training set seems fairly good.
In fact, considering that the task of predicting the daily sunshine hours is really not easy it might even be surprising how well the model predicts that
(at least on the training set). Maybe a little too good?
We also see the noticeable difference between train and test set when calculating the exact value of the RMSE:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">train_metrics</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">return_dict</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">test_metrics</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">return_dict</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Train RMSE: {:.2f}, Test RMSE: {:.2f}'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">train_metrics</span><span class="p">[</span><span class="s">'root_mean_squared_error'</span><span class="p">],</span> <span class="n">test_metrics</span><span class="p">[</span><span class="s">'root_mean_squared_error'</span><span class="p">]))</span>
</code></pre></div></div>
<div class="language-plaintext output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>24/24 [==============================] - 0s 442us/step - loss: 0.7092 - root_mean_squared_error: 0.8421
6/6 [==============================] - 0s 647us/step - loss: 16.4413 - root_mean_squared_error: 4.0548
Train RMSE: 0.84, Test RMSE: 4.05
</code></pre></div></div>

<p>For those experienced with (classical) machine learning this might look familiar.
The plots above expose the signs of <strong>overfitting</strong> which means that the model has to some extent memorized aspects of the training data.
As a result, it makes much more accurate predictions on the training data than on unseen test data.</p>

<p>Overfitting also happens in classical machine learning, but there it is usually interpreted as the model having more parameters than the training data would justify (say, a decision tree with too many branches for the number of training instances). As a consequence one would reduce the number of parameters to avoid overfitting.
In deep learning the situation is slightly different. It can - as for classical machine learning - also be a sign of having a <em>too big</em> model, meaning a model with too many parameters (layers and/or nodes). However, in deep learning higher number of model parameters are often still considered acceptable and models often perform best (in terms of prediction accuracy) when they are at the verge of overfitting. So, in a way, training deep learning models is always a bit like playing with fire…</p>

<h2 id="9-tune-hyperparameters">9. Tune hyperparameters</h2>
<h3 id="set-expectations-how-difficult-is-the-defined-problem">Set expectations: How difficult is the defined problem?</h3>

<p>Before we dive deeper into handling overfitting and (trying to) improving the model performance, let us ask the question: How well must a model perform before we consider it a good model?</p>

<p>Now that we defined a problem (predict tomorrow’s sunshine hours), it makes sense to develop an intuition for how difficult the posed problem is. Frequently, models will be evaluated against a so called <strong>baseline</strong>. A baseline can be the current standard in the field or if such a thing does not exist it could also be an intuitive first guess or toy model. The latter is exactly what we would use for our case.</p>

<p>Maybe the simplest sunshine hour prediction we can easily do is: Tomorrow we will have the same number of sunshine hours as today.
(sounds very naive, but for many observables such as temperature this is already a fairly good predictor)</p>

<p>We can take the <code class="language-plaintext highlighter-rouge">BASEL_sunshine</code> column of our data, because this contains the sunshine hours from one day before what we have as a label.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">y_baseline_prediction</span> <span class="o">=</span> <span class="n">X_test</span><span class="p">[</span><span class="s">'BASEL_sunshine'</span><span class="p">]</span>
<span class="n">plot_predictions</span><span class="p">(</span><span class="n">y_baseline_prediction</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s">'Baseline predictions on the test set'</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="../fig/03_regression_test_5_naive_baseline.png" alt="Scatter plot of predicted vs true sunshine hours in Basel for the test set where today's sunshine hours is considered as the true sunshine hours for tomorrow" width="500px" /></p>

<p>It is difficult to interpret from this plot whether our model is doing better than the baseline.
We can also have a look at the RMSE:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_squared_error</span>
<span class="n">rmse_baseline</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_baseline_prediction</span><span class="p">,</span> <span class="n">squared</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Baseline:'</span><span class="p">,</span> <span class="n">rmse_baseline</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Neural network: '</span><span class="p">,</span> <span class="n">test_metrics</span><span class="p">[</span><span class="s">'root_mean_squared_error'</span><span class="p">])</span>
</code></pre></div></div>
<div class="language-plaintext output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Baseline: 3.877323350410224
Neural network:  4.077792167663574
</code></pre></div></div>

<p>Judging from the numbers alone, our neural network prediction would be performing worse than the baseline.</p>

<blockquote class="challenge">
  <h2 id="exercise-baseline">Exercise: Baseline</h2>
  <ol>
    <li>Looking at this baseline: Would you consider this a simple or a hard problem to solve?</li>
    <li>(Optional) Can you think of other baselines?</li>
  </ol>

  <blockquote class="solution">
    <h2 id="solution-4">Solution</h2>

    <ol>
      <li>This really depends on your definition of hard! The baseline gives a more accurate prediction than just
randomly predicting a number, so the problem is not impossible to solve with machine learning. However, given the structure of the data and our expectations with respect to quality of prediction, it may remain hard to find a good algorithm which exceeds our baseline by orders of magnitude.</li>
      <li>There are a lot of possible answers. A slighly more complicated baseline would be to take the average over the last couple of days.</li>
    </ol>
  </blockquote>
</blockquote>

<h3 id="watch-your-model-training-closely">Watch your model training closely</h3>

<p>As we saw when comparing the predictions for the training and the test set, deep learning models are prone to overfitting. Instead of iterating through countless cycles of model trainings and subsequent evaluations with a reserved test set, it is common practice to work with a second split off dataset to monitor the model during training.
This is the <em>validation set</em> which can be regarded as a second test set. As with the test set, the datapoints of the <em>validation set</em> are not used for the actual model training itself. Instead, we evaluate the model with the <em>validation set</em> after every epoch during training, for instance to stop if we see signs of clear overfitting.
Since we are adapting our model (tuning our hyperparameters) based on this validation set, it is <em>very</em> important that it is kept separate from the test set. If we used the same set, we would not know whether our model truly generalizes or is only overfitting.</p>

<blockquote class="callout">
  <h2 id="test-vs-validation-set">Test vs. validation set</h2>
  <p>Not everybody agrees on the terminology of test set versus validation set. You might find
examples in literature where these terms are used the other way around.</p>

  <p>We are sticking to the definition that is consistent with the Keras API. In there, the validation
set can be used during training, and the test set is reserved for afterwards.</p>
</blockquote>

<p>Let’s give this a try!</p>

<p>We need to initiate a new model – otherwise Keras will simply assume that we want to continue training the model we already trained above.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="n">create_nn</span><span class="p">()</span>
<span class="n">compile_model</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</code></pre></div></div>

<p>But now we train it with the small addition of also passing it our validation set:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span>
                    <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
                    <span class="n">epochs</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span>
                    <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">X_val</span><span class="p">,</span> <span class="n">y_val</span><span class="p">))</span>
</code></pre></div></div>

<p>With this we can plot both the performance on the training data and on the validation data!</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plot_history</span><span class="p">([</span><span class="s">'root_mean_squared_error'</span><span class="p">,</span> <span class="s">'val_root_mean_squared_error'</span><span class="p">])</span>
</code></pre></div></div>
<p><img src="../fig/03_training_history_2_rmse.png" alt="Plot of RMSE vs epochs for the training set and the validation set which depicts a divergence between the two around 10 epochs." width="500px" /></p>
<blockquote class="challenge">
  <h2 id="exercise-plot-the-training-progress">Exercise: plot the training progress.</h2>

  <ol>
    <li>Is there a difference between the training and validation data? And if so, what would this imply?</li>
    <li>(Optional) Take a pen and paper, draw the perfect training and validation curves.
(This may seem trivial, but it will trigger you to think about what you actually would like to see)</li>
  </ol>

  <blockquote class="solution">
    <h2 id="solution-5">Solution</h2>
    <p>The difference between training and validation data shows that something is not completely right here.
The model predictions on the validation set quickly seem to reach a plateau while the performance on the training set keeps improving.
That is a common signature of <em>overfitting</em>.</p>

    <p>Optional:
Ideally you would like the training and validation curves to be identical and slope down steeply
to 0. After that the curves will just consistently stay at 0.</p>
  </blockquote>
</blockquote>

<h3 id="counteract-model-overfitting">Counteract model overfitting</h3>

<p>Overfitting is a very common issue and there are many strategies to handle it.
Most similar to classical machine learning might to <strong>reduce the number of parameters</strong>.</p>

<blockquote class="challenge">
  <h2 id="exercise-try-to-reduce-the-degree-of-overfitting-by-lowering-the-number-of-parameters">Exercise: Try to reduce the degree of overfitting by lowering the number of parameters</h2>

  <p>We can keep the network architecture unchanged (2 dense layers + a one-node output layer) and only play with the number of nodes per layer.
Try to lower the number of nodes in one or both of the two dense layers and observe the changes to the training and validation losses.
If time is short: Suggestion is to run one network with only 10 and 5 nodes in the first and second layer.</p>

  <ul>
    <li>Is it possible to get rid of overfitting this way?</li>
    <li>Does the overall performance suffer or does it mostly stay the same?</li>
    <li>How low can you go with the number of parameters without notable effect on the performance on the validation set?</li>
  </ul>

  <blockquote class="solution">
    <h2 id="solution-6">Solution</h2>
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">create_nn</span><span class="p">(</span><span class="n">nodes1</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">nodes2</span><span class="o">=</span><span class="mi">50</span><span class="p">):</span>
    <span class="c1"># Input layer
</span>    <span class="n">inputs</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">X_data</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],),</span> <span class="n">name</span><span class="o">=</span><span class="s">'input'</span><span class="p">)</span>

    <span class="c1"># Dense layers
</span>    <span class="n">layers_dense</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">nodes1</span><span class="p">,</span> <span class="s">'relu'</span><span class="p">)(</span><span class="n">inputs</span><span class="p">)</span>
    <span class="n">layers_dense</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">nodes2</span><span class="p">,</span> <span class="s">'relu'</span><span class="p">)(</span><span class="n">layers_dense</span><span class="p">)</span>

    <span class="c1"># Output layer
</span>    <span class="n">outputs</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">)(</span><span class="n">layers_dense</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">keras</span><span class="p">.</span><span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s">"model_small"</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">create_nn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="n">model</span><span class="p">.</span><span class="n">summary</span><span class="p">()</span>
</code></pre></div>    </div>

    <div class="language-plaintext output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Model: "model_small"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input (InputLayer)           [(None, 89)]              0
_________________________________________________________________
dense_9 (Dense)              (None, 10)                900
_________________________________________________________________
dense_10 (Dense)             (None, 5)                 55
_________________________________________________________________
dense_11 (Dense)             (None, 1)                 6
=================================================================
Total params: 961
Trainable params: 961
Non-trainable params: 0

</code></pre></div>    </div>

    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">compile_model</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span>
                    <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span>
                    <span class="n">epochs</span> <span class="o">=</span> <span class="mi">200</span><span class="p">,</span>
                    <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">X_val</span><span class="p">,</span> <span class="n">y_val</span><span class="p">))</span>

<span class="n">plot_history</span><span class="p">([</span><span class="s">'root_mean_squared_error'</span><span class="p">,</span> <span class="s">'val_root_mean_squared_error'</span><span class="p">])</span>
</code></pre></div>    </div>

    <p><img src="../fig/03_training_history_3_rmse_smaller_model.png" alt="Plot of RMSE vs epochs for the training set and the validation set with similar performance across the two sets." /></p>

    <p>There is no single correct solution here. But you will have noticed that the number of nodes can be reduced quite a bit!</p>

    <p>In general, it quickly becomes a very complicated search for the right “sweet spot”, i.e. the settings for which overfitting will be (nearly) avoided but which still performes equally well.</p>

  </blockquote>
</blockquote>

<p>We saw that reducing the number of parameters can be a strategy to avoid overfitting.
In practice, however, this is usually not the (main) way to go when it comes to deep learning.
One reason is, that finding the sweet spot can be really hard and time consuming. And it has to be repeated every time the model is adapted, e.g. when more training data becomes available.</p>

<blockquote class="callout">
  <h2 id="sweet-spots">Sweet Spots</h2>
  <p>Note: There is no single correct solution here. But you will have noticed that the number of nodes can be reduced quite a bit!
In general, it quickly becomes a very complicated search for the right “sweet spot”, i.e. the settings for which overfitting will be (nearly) avoided but which still performes equally well.</p>
</blockquote>

<h3 id="early-stopping-stop-when-things-are-looking-best">Early stopping: stop when things are looking best</h3>
<p>Arguable <strong>the</strong> most common technique to avoid (severe) overfitting in deep learning is called <strong>early stopping</strong>.
As the name suggests, this technique just means that you stop the model training if things do not seem to improve anymore.
More specifically, this usually means that the training is stopped if the validation loss does not (notably) improve anymore.
Early stopping is both intuitive and effective to use, so it has become a standard addition for model training.</p>

<p>To better study the effect, we can now safely go back to models with many (too many?) parameters:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="n">create_nn</span><span class="p">()</span>
<span class="n">compile_model</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</code></pre></div></div>

<p>To apply early stopping during training it is easiest to use Keras <code class="language-plaintext highlighter-rouge">EarlyStopping</code> class.
This allows to define the condition of when to stop training. In our case we will say when the validation loss is lowest.
However, since we have seen quiet some fluctuation of the losses during training above we will also set <code class="language-plaintext highlighter-rouge">patience=10</code> which means that the model will stop training if the validation loss has not gone down for 10 epochs.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">tensorflow.keras.callbacks</span> <span class="kn">import</span> <span class="n">EarlyStopping</span>

<span class="n">earlystopper</span> <span class="o">=</span> <span class="n">EarlyStopping</span><span class="p">(</span>
    <span class="n">monitor</span><span class="o">=</span><span class="s">'val_loss'</span><span class="p">,</span>
    <span class="n">patience</span><span class="o">=</span><span class="mi">10</span>
    <span class="p">)</span>

<span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span>
                    <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span>
                    <span class="n">epochs</span> <span class="o">=</span> <span class="mi">200</span><span class="p">,</span>
                    <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">X_val</span><span class="p">,</span> <span class="n">y_val</span><span class="p">),</span>
                    <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">earlystopper</span><span class="p">])</span>
</code></pre></div></div>

<p>As before, we can plot the losses during training:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plot_history</span><span class="p">([</span><span class="s">'root_mean_squared_error'</span><span class="p">,</span> <span class="s">'val_root_mean_squared_error'</span><span class="p">])</span>
</code></pre></div></div>

<p><img src="../fig/03_training_history_3_rmse_early_stopping.png" alt="Plot of RMSE vs epochs for the training set and the validation set displaying similar performance across the two sets." width="500px" /></p>

<p>This still seems to reveal the onset of overfitting, but the training stops before the discrepancy between training and validation loss can grow further.
Despite avoiding severe cases of overfitting, early stopping has the additional advantage that the number of training epochs will be regulated automatically.
Instead of comparing training runs for different number of epochs, early stopping allows to simply set the number of epochs to a desired maximum value.</p>

<p>What might be a bit unintuitive is that the training runs might now end very rapidly.
This might spark the question: have we really reached an optimum yet?
And often the answer to this is “no”, which is why early stopping frequently is combined with other approaches to avoid overfitting.
Overfitting means that a model (seemingly) performs better on seen data compared to unseen data. One then often also says that it does not “generalize” well.
Techniques to avoid overfitting, or to improve model generalization, are termed <strong>regularization techniques</strong> and we will come back to this in <strong>episode 4</strong>.</p>

<h3 id="batchnorm-the-standard-scaler-for-deep-learning">BatchNorm: the “standard scaler” for deep learning</h3>

<p>A very common step in classical machine learning pipelines is to scale the features, for instance by using sckit-learn’s <code class="language-plaintext highlighter-rouge">StandardScaler</code>.
This can in principle also be done for deep learning.
An alternative, more common approach, is to add <strong>BatchNormalization</strong> layers (<a href="https://keras.io/api/layers/normalization_layers/batch_normalization/">documentation of the batch normalization layer</a>) which will learn how to scale the input values.
Similar to dropout, batch normalization is available as a network layer in Keras and can be added to the network in a similar way.
It does not require any additional parameter setting.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">tensorflow.keras.layers</span> <span class="kn">import</span> <span class="n">BatchNormalization</span>
</code></pre></div></div>

<p>The <code class="language-plaintext highlighter-rouge">BatchNormalization</code> can be inserted as yet another layer into the architecture.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">create_nn</span><span class="p">():</span>
    <span class="c1"># Input layer
</span>    <span class="n">inputs</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">X_data</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],),</span> <span class="n">name</span><span class="o">=</span><span class="s">'input'</span><span class="p">)</span>

    <span class="c1"># Dense layers
</span>    <span class="n">layers_dense</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">BatchNormalization</span><span class="p">()(</span><span class="n">inputs</span><span class="p">)</span>
    <span class="n">layers_dense</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="s">'relu'</span><span class="p">)(</span><span class="n">layers_dense</span><span class="p">)</span>
    <span class="n">layers_dense</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="s">'relu'</span><span class="p">)(</span><span class="n">layers_dense</span><span class="p">)</span>

    <span class="c1"># Output layer
</span>    <span class="n">outputs</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">)(</span><span class="n">layers_dense</span><span class="p">)</span>

    <span class="c1"># Defining the model and compiling it
</span>    <span class="k">return</span> <span class="n">keras</span><span class="p">.</span><span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s">"model_batchnorm"</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">create_nn</span><span class="p">()</span>
<span class="n">compile_model</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="n">model</span><span class="p">.</span><span class="n">summary</span><span class="p">()</span>
</code></pre></div></div>

<p>This new layer appears in the model summary as well.</p>

<div class="language-plaintext output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Model: "model_batchnorm"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         [(None, 89)]              0
_________________________________________________________________
batch_normalization (BatchNo (None, 89)                356
_________________________________________________________________
dense (Dense)             (None, 100)               9000
_________________________________________________________________
dense_1 (Dense)             (None, 50)                5050
_________________________________________________________________
dense_2 (Dense)             (None, 1)                 51
=================================================================
Total params: 14,457
Trainable params: 14,279
Non-trainable params: 178
</code></pre></div></div>

<p>We can train the model again as follows:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span>
                    <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span>
                    <span class="n">epochs</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
                    <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">X_val</span><span class="p">,</span> <span class="n">y_val</span><span class="p">),</span>
                    <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">earlystopper</span><span class="p">])</span>

<span class="n">plot_history</span><span class="p">([</span><span class="s">'root_mean_squared_error'</span><span class="p">,</span> <span class="s">'val_root_mean_squared_error'</span><span class="p">])</span>
</code></pre></div></div>

<p><img src="../fig/03_training_history_5_rmse_batchnorm.png" alt="Output of plotting sample" width="500px" /></p>

<blockquote class="callout">
  <h2 id="batchnorm-parameters">Batchnorm parameters</h2>

  <p>You may have noticed that the number of parameters of the Batchnorm layers corresponds to
4 parameters per input node.
These are the moving mean, moving standard deviation, additional scaling factor (gamma) and offset factor (beta).
There is a difference in behavior for Batchnorm between training and prediction time.
During training time, the data is scaled with the mean and standard deviation of the batch.
During prediction time, the moving mean and moving standard deviation of the training set is used instead.
The additional parameters gamma and beta are introduced to allow for more flexibility in output values, and are used in both training and prediction,</p>

</blockquote>

<h3 id="run-on-test-set-and-compare-to-naive-baseline">Run on test set and compare to naive baseline</h3>

<p>It seems that no matter what we add, the overall loss does not decrease much further (we at least avoided overfitting though!).
Let us again plot the results on the test set:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">y_test_predicted</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">plot_predictions</span><span class="p">(</span><span class="n">y_test_predicted</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s">'Predictions on the test set'</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="../fig/03_regression_test_5_dropout_batchnorm.png" alt="Scatter plot between predictions and true sunshine hours for Basel on the test set" width="500px" /></p>

<p>Well, the above is certainly not perfect. But how good or bad is this? Maybe not good enough to plan your picnic for tomorrow.
But let’s better compare it to the naive baseline we created in the beginning. What would you say, did we improve on that?</p>

<blockquote class="challenge">
  <h2 id="exercise-simplify-the-model-and-add-data">Exercise: Simplify the model and add data</h2>

  <p>You may have been wondering why we are including weather observations from
multiple cities to predict sunshine hours only in Basel. The weather is
a complex phenomenon with correlations over large distances and time scales,
but what happens if we limit ourselves to only one city?</p>

  <ol>
    <li>Since we will be reducing the number of features quite significantly,
we should afford to include more data. Instead of using only 3 years, use
8 or 9 years!</li>
    <li>Remove all cities from the training data that are not for Basel.
You can use something like:
      <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">cols</span> <span class="o">=</span> <span class="p">[</span><span class="n">c</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">X_data</span><span class="p">.</span><span class="n">columns</span> <span class="k">if</span> <span class="n">c</span><span class="p">[:</span><span class="mi">5</span><span class="p">]</span> <span class="o">==</span> <span class="s">'BASEL'</span><span class="p">]</span>
<span class="n">X_data</span> <span class="o">=</span> <span class="n">X_data</span><span class="p">[</span><span class="n">cols</span><span class="p">]</span>
</code></pre></div>      </div>
    </li>
    <li>Now rerun the last model we defined which included the BatchNorm layer.
Recreate the scatter plot comparing your prediction with the baseline
prediction based on yesterday’s sunshine hours, and compute also the RMSE.
Note that even though we will use many more observations than previously,
the network should still train quickly because we reduce the number of
features (columns).
Is the prediction better compared to what we had before?</li>
    <li>(Optional) Try to train a model on all years that are available,
and all features from all cities. How does it perform?</li>
  </ol>

  <blockquote class="solution">
    <h2 id="solution-7">Solution</h2>

    <p>Use 9 years out of the total dataset. This means 3 times as many
rows as we used previously, but by removing columns not containing
“BASEL” we reduce the number of columns from 89 to 11.</p>
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">nr_rows</span> <span class="o">=</span> <span class="mi">365</span><span class="o">*</span><span class="mi">9</span>
<span class="c1"># data
</span><span class="n">X_data</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">loc</span><span class="p">[:</span><span class="n">nr_rows</span><span class="p">].</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s">'DATE'</span><span class="p">,</span> <span class="s">'MONTH'</span><span class="p">])</span>
<span class="c1"># labels (sunshine hours the next day)
</span><span class="n">y_data</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="mi">1</span><span class="p">:(</span><span class="n">nr_rows</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)][</span><span class="s">"BASEL_sunshine"</span><span class="p">]</span>
<span class="c1"># only use columns with 'BASEL'
</span><span class="n">cols</span> <span class="o">=</span> <span class="p">[</span><span class="n">c</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">X_data</span><span class="p">.</span><span class="n">columns</span> <span class="k">if</span> <span class="n">c</span><span class="p">[:</span><span class="mi">5</span><span class="p">]</span> <span class="o">==</span> <span class="s">'BASEL'</span><span class="p">]</span>
<span class="n">X_data</span> <span class="o">=</span> <span class="n">X_data</span><span class="p">[</span><span class="n">cols</span><span class="p">]</span>
</code></pre></div>    </div>

    <p>Do the train-test-validation split:</p>

    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">X_train</span><span class="p">,</span> <span class="n">X_holdout</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_holdout</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X_data</span><span class="p">,</span> <span class="n">y_data</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">X_val</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_val</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X_holdout</span><span class="p">,</span> <span class="n">y_holdout</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</code></pre></div>    </div>

    <p>Create the network. We can re-use the <code class="language-plaintext highlighter-rouge">create_nn</code> that we already have. Because we have reduced the number of input features
the number of parameters in the network goes down from 14457 to 6137.</p>
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># create the network and view its summary
</span><span class="n">model</span> <span class="o">=</span> <span class="n">create_nn</span><span class="p">()</span>
<span class="n">compile_model</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="n">model</span><span class="p">.</span><span class="n">summary</span><span class="p">()</span>
</code></pre></div>    </div>

    <p>Fit with early stopping and output showing performance on validation set:</p>
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span>
                    <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span>
                    <span class="n">epochs</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
                    <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">X_val</span><span class="p">,</span> <span class="n">y_val</span><span class="p">),</span>
                    <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">earlystopper</span><span class="p">],</span>
                    <span class="n">verbose</span> <span class="o">=</span> <span class="mi">2</span><span class="p">)</span>

<span class="n">plot_history</span><span class="p">([</span><span class="s">'root_mean_squared_error'</span><span class="p">,</span> <span class="s">'val_root_mean_squared_error'</span><span class="p">])</span>
</code></pre></div>    </div>

    <p>Create a scatter plot to compare with true observations:</p>
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">y_test_predicted</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">plot_predictions</span><span class="p">(</span><span class="n">y_test_predicted</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s">'Predictions on the test set'</span><span class="p">)</span>
</code></pre></div>    </div>

    <p>Compare the mean squared error with baseline prediction. It should be
similar or even a little better than what we saw with the larger model!</p>
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_squared_error</span>
<span class="n">y_baseline_prediction</span> <span class="o">=</span> <span class="n">X_test</span><span class="p">[</span><span class="s">'BASEL_sunshine'</span><span class="p">]</span>
<span class="n">rmse_nn</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_test_predicted</span><span class="p">,</span> <span class="n">squared</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">rmse_baseline</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_baseline_prediction</span><span class="p">,</span> <span class="n">squared</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'NN RMSE: {:.2f}, baseline RMSE: {:.2f}'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">rmse_nn</span><span class="p">,</span> <span class="n">rmse_baseline</span><span class="p">))</span>
</code></pre></div>    </div>
  </blockquote>
</blockquote>

<blockquote class="callout">
  <h2 id="tensorboard">Tensorboard</h2>

  <p>If we run many different experiments with different architectures,
it can be difficult to keep track of these different models or compare the achieved performance.
We can use <em>tensorboard</em>, a framework that keeps track of our experiments and shows graphs like we plotted above.
Tensorboard is included in our tensorflow installation by default.
To use it, we first need to add a <em>callback</em> to our (compiled) model that saves the progress of training performance in a logs directory:</p>
  <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">tensorflow.keras.callbacks</span> <span class="kn">import</span> <span class="n">TensorBoard</span>
<span class="kn">import</span> <span class="nn">datetime</span>
<span class="n">log_dir</span> <span class="o">=</span> <span class="s">"logs/fit/"</span> <span class="o">+</span> <span class="n">datetime</span><span class="p">.</span><span class="n">datetime</span><span class="p">.</span><span class="n">now</span><span class="p">().</span><span class="n">strftime</span><span class="p">(</span><span class="s">"%Y%m%d-%H%M%S"</span><span class="p">)</span> <span class="c1"># You can adjust this to add a more meaningful model name
</span><span class="n">tensorboard_callback</span> <span class="o">=</span> <span class="n">TensorBoard</span><span class="p">(</span><span class="n">log_dir</span><span class="o">=</span><span class="n">log_dir</span><span class="p">,</span> <span class="n">histogram_freq</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span>
                    <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span>
                    <span class="n">epochs</span> <span class="o">=</span> <span class="mi">200</span><span class="p">,</span>
                    <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">X_val</span><span class="p">,</span> <span class="n">y_val</span><span class="p">),</span>
                    <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">tensorboard_callback</span><span class="p">],</span>
                    <span class="n">verbose</span> <span class="o">=</span> <span class="mi">2</span><span class="p">)</span>
</code></pre></div>  </div>
  <p>You can launch the tensorboard interface from a Jupyter notebook, showing all trained models:
<!--cce:skip--></p>
  <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>%load_ext tensorboard
%tensorboard --logdir logs/fit
</code></pre></div>  </div>
  <p class="language-python">Which will show an interface that looks something like this:
<img src="../fig/03_tensorboard.png" alt="Screenshot of tensorboard" /></p>
</blockquote>

<h2 id="10-save-model">10. Save model</h2>

<p>Now that we have a somewhat acceptable model, let us not forget to save it for future users to benefit from our explorative efforts!</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span><span class="p">.</span><span class="n">save</span><span class="p">(</span><span class="s">'my_tuned_weather_model'</span><span class="p">)</span>
</code></pre></div></div>

<h1 id="outlook">Outlook</h1>
<p>Correctly predicting tomorrow’s sunshine hours is apparently not that simple.
Our models get the general trends right, but still predictions vary quite a bit and can even be far off.</p>

<blockquote class="challenge">
  <h2 id="open-question-what-could-be-next-steps-to-further-improve-the-model">Open question: What could be next steps to further improve the model?</h2>

  <p>With unlimited options to modify the model architecture or to play with the training parameters, deep learning can trigger very extensive hunting for better and better results.
Usually models are “well behaving” in the sense that small changes to the architectures also only result in small changes of the performance (if any).
It is often tempting to hunt for some magical settings that will lead to much better results. But do those settings exist?
Applying common sense is often a good first step to make a guess of how much better results <em>could</em> be.
In the present case we might certainly not expect to be able to reliably predict sunshine hours for the next day with 5-10 minute precision.
But how much better our model could be exactly, often remains difficult to answer.</p>

  <ul>
    <li>What changes to the model architecture might make sense to explore?</li>
    <li>Ignoring changes to the model architecture, what might notably improve the prediction quality?</li>
  </ul>

  <blockquote class="solution">
    <h2 id="solution-8">Solution</h2>
    <p>This is an open question. And we don’t actually know how far one could push this sunshine hour prediction (try it out yourself if you like! We’re curious!).
But there are a few things that might be worth exploring.</p>

    <p>Regarding the model architecture:</p>
    <ul>
      <li>In the present case we do not see a magical silver bullet to suddenly boost the performance. But it might be worth testing if <em>deeper</em> networks do better (more layers).</li>
    </ul>

    <p>Other changes that might impact the quality notably:</p>
    <ul>
      <li>The most obvious answer here would be: more data! Even this will not always work (e.g. if data is very noisy and uncorrelated, more data might not add much).</li>
      <li>Related to more data: use data augmentation. By creating realistic variations of the available data, the model might improve as well.</li>
      <li>More data can mean more data points (you can test it yourself by taking more than the 3 years we used here!)</li>
      <li>More data can also mean more features! What about adding the month?</li>
      <li>The labels we used here (sunshine hours) are highly biased, many days with no or nearly no sunshine but a few with &gt;10 hours. Techniques such as oversampling or undersampling might handle such biased labels better.
Another alternative would be to not only look at data from one day, but use the data of a longer period such as a full week.
This will turn the data into time series data which in turn might also make it worth to apply different model architectures…</li>
    </ul>

  </blockquote>
</blockquote>

<blockquote class="keypoints">
  <h2>Key Points</h2>
  <ul>
    
    <li><p>Separate training, validation, and test sets allows monitoring and evaluating your model.</p>
</li>
    
    <li><p>Batchnormalization scales the data as part of the model.</p>
</li>
    
  </ul>
</blockquote>

<hr />

<h1 id="advanced-layer-types" class="maintitle">Advanced layer types</h1>

<blockquote class="objectives">
  <h2>Overview</h2>

  <div class="row">
    <div class="col-md-3">
      <strong>Teaching:</strong> 30 min
      <br />
      <strong>Exercises:</strong> 70 min
    </div>
    <div class="col-md-9">
      <strong>Questions</strong>
      <ul>
	
	<li><p>Why do we need different types of layers?</p>
</li>
	
	<li><p>What are good network designs for image data?</p>
</li>
	
	<li><p>What is a convolutional layer?</p>
</li>
	
	<li><p>How can we use different types of layers to prevent overfitting?</p>
</li>
	
      </ul>
    </div>
  </div>

  <div class="row">
    <div class="col-md-3">
    </div>
    <div class="col-md-9">
      <strong>Objectives</strong>
      <ul>
	
	<li><p>Understand why convolutional and pooling layers are useful for image data</p>
</li>
	
	<li><p>Implement a convolutional neural network on an image dataset</p>
</li>
	
	<li><p>Use a drop-out layer to prevent overfitting</p>
</li>
	
      </ul>
    </div>
  </div>

</blockquote>

<h2 id="different-types-of-layers">Different types of layers</h2>
<p>Networks are like onions: a typical neural network consists of many layers. In fact, the word <em>deep</em> in <em>Deep Learning</em>
refers to the many layers that make the network deep.</p>

<p>So far, we have seen one type of layer, namely the <strong>fully connected</strong>, or <strong>dense</strong> layer. This layer is called fully connected, because all input neurons are taken into account by each output neuron. The number of parameters that need to be learned by the network, is thus in the order of magnitude of the number of input neurons times the number of hidden neurons.</p>

<p>However, there are many different types of layers that perform different calculations and take different inputs. In this episode we will take a look at <strong>convolutional layers</strong> and <strong>dropout layers</strong>, which are useful in the context of image data, but also in many other types of (structured) data.</p>

<h2 id="image-classification">Image classification</h2>
<p>Keras comes with a few prepared datasets. We have a look at the <a href="https://www.cs.toronto.edu/~kriz/cifar.html">CIFAR10 dataset</a>,
which is a widely known dataset for image classification.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">tensorflow</span> <span class="kn">import</span> <span class="n">keras</span>
<span class="p">(</span><span class="n">train_images</span><span class="p">,</span> <span class="n">train_labels</span><span class="p">),</span> <span class="p">(</span><span class="n">test_images</span><span class="p">,</span> <span class="n">test_labels</span><span class="p">)</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">datasets</span><span class="p">.</span><span class="n">cifar10</span><span class="p">.</span><span class="n">load_data</span><span class="p">()</span>
</code></pre></div></div>

<blockquote class="callout">
  <h2 id="cifar-10">CIFAR-10</h2>

  <p>The CIFAR-10 dataset consists of images of 10 different classes: airplanes, cars, birds, cats, deer, dogs, frogs, horses, ships, and trucks.
It is widely used as a benchmark dataset for image classification. The low resolution of the images in the dataset allows for quick loading and testing models.</p>

  <p>For more information about this dataset and how it was collected you can check out
<a href="https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf">Learning Multiple Layers of Features from Tiny Images by  Alex Krizhevsky, 2009</a>.</p>

</blockquote>

<blockquote class="callout">
  <h2 id="certificate_verify_failed-error-when-downloading-cifar-10-dataset">CERTIFICATE_VERIFY_FAILED error when downloading CIFAR-10 dataset</h2>

  <p>When loading the CIFAR-10 dataset, you might get the following error:</p>
  <div class="language-plaintext source highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1125)
</code></pre></div>  </div>
  <p>You can solve this error by adding this to your notebook:</p>
  <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">ssl</span>
<span class="n">ssl</span><span class="p">.</span><span class="n">_create_default_https_context</span> <span class="o">=</span> <span class="n">ssl</span><span class="p">.</span><span class="n">_create_unverified_context</span>
</code></pre></div>  </div>

</blockquote>

<p>We take a small sample of the data as training set for demonstration purposes.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">n</span> <span class="o">=</span> <span class="mi">5000</span>
<span class="n">train_images</span> <span class="o">=</span> <span class="n">train_images</span><span class="p">[:</span><span class="n">n</span><span class="p">]</span>
<span class="n">train_labels</span> <span class="o">=</span> <span class="n">train_labels</span><span class="p">[:</span><span class="n">n</span><span class="p">]</span>
</code></pre></div></div>

<blockquote class="challenge">
  <h2 id="explore-the-data">Explore the data</h2>

  <p>Familiarize yourself with the CIFAR10 dataset. To start, consider the following questions:</p>
  <ol>
    <li>What is the dimension of a single data point? What do you think the dimensions mean?</li>
    <li>What is the range of values that your input data takes?</li>
    <li>What is the shape of the labels, and how many labels do we have?</li>
    <li>(Optional) We are going to build a new architecture from scratch to get you
familiar with the convolutional neural network basics.
But in the real world you wouldn’t do that.
So the challenge is: Browse the web for (more) existing architectures or pre-trained models that are likely to work
well on this type of data. Try to understand why they work well for this type of data.</li>
  </ol>

  <blockquote class="solution">
    <h2 id="solution">Solution</h2>

    <p>To explore the dimensions of the input data:</p>
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">train_images</span><span class="p">.</span><span class="n">shape</span>
</code></pre></div>    </div>
    <div class="language-plaintext output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(5000, 32, 32, 3)
</code></pre></div>    </div>
    <p>The first value, <code class="language-plaintext highlighter-rouge">5000</code>, is the number of training images that we have selected.
The remainder of the shape, namely <code class="language-plaintext highlighter-rouge">32, 32, 3)</code>, denotes
the dimension of one image. The last value 3 is typical for color images,
and stands for the three color channels <strong>R</strong>ed, <strong>G</strong>reen, <strong>B</strong>lue.
We are left with <code class="language-plaintext highlighter-rouge">32, 32</code>. This denotes the width and height of our input image in number of pixels. By convention, the first entry refers to the height, the second to the width of the image. In this case, we observe a quadratic image where height equals width.</p>

    <p>We can find out the range of values of our input data as follows:</p>
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">train_images</span><span class="p">.</span><span class="nb">min</span><span class="p">(),</span> <span class="n">train_images</span><span class="p">.</span><span class="nb">max</span><span class="p">()</span>
</code></pre></div>    </div>
    <div class="language-plaintext output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(0, 255)
</code></pre></div>    </div>
    <p>So the values of the three channels range between <code class="language-plaintext highlighter-rouge">0</code> and <code class="language-plaintext highlighter-rouge">255</code>.</p>

    <p>Lastly, we inspect the dimension of the labels:</p>
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">train_labels</span><span class="p">.</span><span class="n">shape</span>
</code></pre></div>    </div>
    <div class="language-plaintext output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(5000, 1)
</code></pre></div>    </div>
    <p>So we have, for each image, a single value denoting the label.
To find out what the possible values of these labels are:</p>
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">train_labels</span><span class="p">.</span><span class="nb">min</span><span class="p">(),</span> <span class="n">train_labels</span><span class="p">.</span><span class="nb">max</span><span class="p">()</span>
</code></pre></div>    </div>
    <div class="language-plaintext output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(0, 9)
</code></pre></div>    </div>
    <p>The values of the labels range between <code class="language-plaintext highlighter-rouge">0</code> and <code class="language-plaintext highlighter-rouge">9</code>, denoting 10 different classes.</p>
  </blockquote>
</blockquote>

<p>The training set consists of 50000 images of <code class="language-plaintext highlighter-rouge">32x32</code> pixels and 3 channels (RGB values). The RGB values are between <code class="language-plaintext highlighter-rouge">0</code> and <code class="language-plaintext highlighter-rouge">255</code>. For input of neural networks, it is better to have small input values. So we normalize our data between <code class="language-plaintext highlighter-rouge">0</code> and <code class="language-plaintext highlighter-rouge">1</code>:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">train_images</span> <span class="o">=</span> <span class="n">train_images</span> <span class="o">/</span> <span class="mf">255.0</span>
<span class="n">test_images</span> <span class="o">=</span> <span class="n">test_images</span> <span class="o">/</span> <span class="mf">255.0</span>
</code></pre></div></div>

<p>The labels are single numbers denoting the class.
We map the class numbers back to the class names, taken from the documentation:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">class_names</span> <span class="o">=</span> <span class="p">[</span><span class="s">'airplane'</span><span class="p">,</span> <span class="s">'automobile'</span><span class="p">,</span> <span class="s">'bird'</span><span class="p">,</span> <span class="s">'cat'</span><span class="p">,</span> <span class="s">'deer'</span><span class="p">,</span>
               <span class="s">'dog'</span><span class="p">,</span> <span class="s">'frog'</span><span class="p">,</span> <span class="s">'horse'</span><span class="p">,</span> <span class="s">'ship'</span><span class="p">,</span> <span class="s">'truck'</span><span class="p">]</span>
</code></pre></div></div>

<p>Now we can plot a sample of the training images, using the <code class="language-plaintext highlighter-rouge">plt.imshow</code> function.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">25</span><span class="p">):</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">train_images</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="p">.</span><span class="n">cm</span><span class="p">.</span><span class="n">binary</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">axis</span><span class="p">(</span><span class="s">'off'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="n">class_names</span><span class="p">[</span><span class="n">train_labels</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="mi">0</span><span class="p">]])</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="../fig/04_cifar10.png" alt="A 5 by 5 grid of 25 sample images from the CIFAR-10 data-set. Each image is labelled with a category, for example: 'frog' or 'horse'." /></p>

<h2 id="convolutional-layers">Convolutional layers</h2>
<p>In the previous episodes, we used ‘fully connected layers’ , that connected all input values of a layer to all outputs of a layer. This results in many connections, and thus weights to be learned, in the network. Note that our input dimension is now quite high (even with small pictures of <code class="language-plaintext highlighter-rouge">32x32</code> pixels), we have:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">dim</span> <span class="o">=</span> <span class="n">train_images</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">train_images</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">*</span> <span class="n">train_images</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span>
<span class="k">print</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span>
</code></pre></div></div>
<div class="language-plaintext output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>3072
</code></pre></div></div>

<blockquote class="challenge">
  <h2 id="number-of-parameters">Number of parameters</h2>

  <p>Suppose we create a single Dense (fully connected) layer with 100 hidden units that connect to the input pixels, how many parameters does this layer have?</p>

  <blockquote class="solution">
    <h2 id="solution-1">Solution</h2>

    <p>Each entry of the input dimensions, i.e. the <code class="language-plaintext highlighter-rouge">shape</code> of one single data point, is connected with 100 neurons of our hidden layer, and each of these neurons has a bias term associated to it. So we have <code class="language-plaintext highlighter-rouge">307300</code> parameters to learn.</p>
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">width</span><span class="p">,</span> <span class="n">height</span> <span class="o">=</span> <span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span>
<span class="n">n_hidden_neurons</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">n_bias</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">n_input_items</span> <span class="o">=</span> <span class="n">width</span> <span class="o">*</span> <span class="n">height</span> <span class="o">*</span> <span class="mi">3</span>
<span class="n">n_parameters</span> <span class="o">=</span> <span class="p">(</span><span class="n">n_input_items</span> <span class="o">*</span> <span class="n">n_hidden_neurons</span><span class="p">)</span> <span class="o">+</span> <span class="n">n_bias</span>
<span class="n">n_parameters</span>
</code></pre></div>    </div>
    <div class="language-plaintext output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>307300
</code></pre></div>    </div>
    <p>We can also check this by building the layer in Keras:</p>
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">inputs</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="n">dim</span><span class="p">)</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">)(</span><span class="n">inputs</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">models</span><span class="p">.</span><span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span>
<span class="n">model</span><span class="p">.</span><span class="n">summary</span><span class="p">()</span>
</code></pre></div>    </div>
    <div class="language-plaintext output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Model: "model"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         [(None, 3072)]            0
_________________________________________________________________
dense (Dense)                (None, 100)               307300
=================================================================
Total params: 307,300
Trainable params: 307,300
Non-trainable params: 0
_________________________________________________________________
</code></pre></div>    </div>
  </blockquote>
</blockquote>

<p>We can decrease the number of units in our hidden layer, but this also decreases the number of patterns our network can remember. Moreover, if we increase the image size, the number of weights will ‘explode’, even though the task of recognizing large images is not necessarily more difficult than the task of recognizing small images.</p>

<p>The solution is that we make the network learn in a ‘smart’ way. The features that we learn should be similar both for small and large images, and similar features (e.g. edges, corners) can appear anywhere in the image (in mathematical terms: <em>translation invariant</em>). We do this by making use of a concepts from image processing that precede Deep Learning.</p>

<p>A <strong>convolution matrix</strong>, or <strong>kernel</strong>, is a matrix transformation that we ‘slide’ over the image to calculate features at each position of the image. For each pixel, we calculate the matrix product between the kernel and the pixel with its surroundings. A kernel is typically small, between 3x3 and 7x7 pixels. We can for example think of the 3x3 kernel:</p>
<div class="language-plaintext output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[[-1, -1, -1],
 [0, 0, 0]
 [1, 1, 1]]
</code></pre></div></div>
<p>This kernel will give a high value to a pixel if it is on a horizontal border between dark and light areas.
Note that for RGB images, the kernel should also have a depth of 3.</p>

<p>In the following image, we see the effect of such a kernel on the values of a single-channel image. The red cell in the output matrix is the result of multiplying and summing the values of the red square in the input, and the kernel. Applying this kernel to a real image shows that it indeed detects horizontal edges.
<img src="../fig/04_conv_matrix.png" alt="Example of a convolution matrix calculation" />
<img src="../fig/04_conv_image.png" alt="Convolution example on an image of a cat to extract features" /></p>

<p>In our <strong>convolutional layer</strong> our hidden units are a number of convolutional matrices (or kernels), where the values of the matrices are the weights that we learn in the training process. The output of a convolutional layer is an ‘image’ for each of the kernels, that gives the output of the kernel applied to each pixel.</p>

<blockquote class="callout">
  <h2 id="playing-with-convolutions">Playing with convolutions</h2>
  <p>Convolutions applied to images can be hard to grasp at first. Fortunately there are resources out
there that enable users to interactively play around with images and convolutions:</p>
  <ul>
    <li><a href="https://setosa.io/ev/image-kernels/">Image kernels explained</a> shows how different convolutions can achieve certain effects on an image, like sharpening and blurring.</li>
    <li><a href="https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-convolutional-neural-networks#">The convolutional neural network cheat sheet</a>
  shows animated examples of the different components of convolutional neural nets</li>
  </ul>

</blockquote>

<blockquote class="challenge">
  <h2 id="border-pixels">Border pixels</h2>

  <p>What, do you think, happens to the border pixels when applying a convolution?</p>

  <blockquote class="solution">
    <h2 id="solution-2">Solution</h2>

    <p>There are different ways of dealing with border pixels.
You can ignore them, which means that your output image is slightly smaller then your input.
It is also possible to ‘pad’ the borders, e.g. with the same value or with zeros, so that the convolution can also be applied to the border pixels.
In that case, the output image will have the same size as the input image.</p>
  </blockquote>
</blockquote>

<blockquote class="challenge">
  <h2 id="number-of-model-parameters">Number of model parameters</h2>

  <p>Suppose we apply a convolutional layer with 100 kernels of size 3 * 3 * 3 (the last dimension applies to the rgb channels) to our images of 32 * 32 * 3 pixels. How many parameters do we have? Assume, for simplicity, that the kernels do not use bias terms. Compare this to the answer of the previous exercise</p>

  <blockquote class="solution">
    <h2 id="solution-3">Solution</h2>

    <p>We have 100 matrices with 3 * 3 * 3 = 27 values each so that gives 27 * 100 = 2700 weights. This is a magnitude of 100 less than the fully connected layer with 100 units! Nevertheless, as we will see, convolutional networks work very well for image data. This illustrates the expressiveness of convolutional layers.</p>
  </blockquote>
</blockquote>

<p>So let us look at a network with a few convolutional layers. We need to finish with a Dense layer to connect the output cells of the convolutional layer to the outputs for our classes.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">inputs</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="n">train_images</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">)(</span><span class="n">inputs</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Flatten</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s">"cifar_model_small"</span><span class="p">)</span>

<span class="n">model</span><span class="p">.</span><span class="n">summary</span><span class="p">()</span>
</code></pre></div></div>

<blockquote class="challenge">
  <h2 id="convolutional-neural-network">Convolutional Neural Network</h2>

  <p>Inspect the network above:</p>
  <ul>
    <li>What do you think is the function of the <code class="language-plaintext highlighter-rouge">Flatten</code> layer?</li>
    <li>Which layer has the most parameters? Do you find this intuitive?</li>
    <li>(optional) Pick a model from https://paperswithcode.com/sota/image-classification-on-cifar-10 . Try to understand how it works.</li>
  </ul>

  <blockquote class="solution">
    <h2 id="solution-4">Solution</h2>

    <ul>
      <li>The Flatten layer converts the 28x28x50 output of the convolutional layer into a single one-dimensional vector, that can be used as input for a dense layer.</li>
      <li>The last dense layer has the most parameters. This layer connects every single output ‘pixel’ from the convolutional layer to the 10 output classes.
 That results in a large number of connections, so a large number of parameters. This undermines a bit the expressiveness of the convolutional layers, that have much fewer parameters.</li>
    </ul>
  </blockquote>
</blockquote>

<p>Often in convolutional neural networks, the convolutional layers are intertwined with <strong>Pooling layers</strong>. As opposed to the convolutional layer, the pooling layer actually alters the dimensions of the image and reduces it by a scaling factor. It is basically decreasing the resolution of your picture. The rationale behind this is that higher layers of the network should focus on higher-level features of the image. By introducing a pooling layer, the subsequent convolutional layer has a broader ‘view’ on the original image.</p>

<p>Let’s put it into practice. We compose a Convolutional network with two convolutional layers and two pooling layers.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">inputs</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="n">train_images</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">)(</span><span class="n">inputs</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">MaxPooling2D</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">MaxPooling2D</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Flatten</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s">"cifar_model_small"</span><span class="p">)</span>

<span class="n">model</span><span class="p">.</span><span class="n">summary</span><span class="p">()</span>
</code></pre></div></div>
<div class="language-plaintext output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Model: "cifar_model"
_________________________________________________________________
 Layer (type)                Output Shape              Param #
=================================================================
 input_6 (InputLayer)        [(None, 32, 32, 3)]       0

 conv2d_13 (Conv2D)          (None, 30, 30, 50)        1400

 max_pooling2d_8 (MaxPooling  (None, 15, 15, 50)       0
 2D)

 conv2d_14 (Conv2D)          (None, 13, 13, 50)        22550

 max_pooling2d_9 (MaxPooling  (None, 6, 6, 50)         0
 2D)

 conv2d_15 (Conv2D)          (None, 4, 4, 50)          22550

 flatten_5 (Flatten)         (None, 800)               0

 dense_9 (Dense)             (None, 50)                40050

 dense_10 (Dense)            (None, 10)                510

=================================================================
Total params: 87,060
Trainable params: 87,060
Non-trainable params: 0
_________________________________________________________________
</code></pre></div></div>

<p>We compile the model using the adam optimizer (other optimizers could also be used here!).
Similar to the penguin classification task, we will use the crossentropy function to calculate the model’s loss.
This loss function is appropriate to use when the data has two or more label classes.</p>

<p>To calculate crossentropy loss for data that has its classes represented by integers (i.e., not one-hot encoded), we use the SparseCategoricalCrossentropy() function:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span><span class="p">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s">'adam'</span><span class="p">,</span>
              <span class="n">loss</span><span class="o">=</span><span class="n">keras</span><span class="p">.</span><span class="n">losses</span><span class="p">.</span><span class="n">SparseCategoricalCrossentropy</span><span class="p">(</span><span class="n">from_logits</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span>
              <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s">'accuracy'</span><span class="p">])</span>
</code></pre></div></div>

<p>We then train the model for 10 epochs:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_images</span><span class="p">,</span> <span class="n">train_labels</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                    <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">test_images</span><span class="p">,</span> <span class="n">test_labels</span><span class="p">))</span>
</code></pre></div></div>

<p>We can plot the training process using the history:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="n">sns</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>

<span class="n">history_df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">.</span><span class="n">from_dict</span><span class="p">(</span><span class="n">history</span><span class="p">.</span><span class="n">history</span><span class="p">)</span>
<span class="n">sns</span><span class="p">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">history_df</span><span class="p">[[</span><span class="s">'accuracy'</span><span class="p">,</span> <span class="s">'val_accuracy'</span><span class="p">]])</span>
</code></pre></div></div>
<p><img src="../fig/04_training_history_1.png" alt="Plot of training accuracy and validation accuracy vs epochs for the trained model" /></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">sns</span><span class="p">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">history_df</span><span class="p">[[</span><span class="s">'loss'</span><span class="p">,</span> <span class="s">'val_loss'</span><span class="p">]])</span>
</code></pre></div></div>
<p><img src="../fig/04_training_history_loss_1.png" alt="OPlot of training loss and validation loss vs epochs for the trained model" /></p>

<p>It seems that the model is overfitting somewhat, because the validation accuracy and loss stagnates.</p>

<blockquote class="challenge">
  <h2 id="network-depth">Network depth</h2>

  <p>What, do you think, will be the effect of adding a convolutional layer to your model? Will this model have more or fewer parameters?
Try it out. Create a <code class="language-plaintext highlighter-rouge">model</code> that has an additional <code class="language-plaintext highlighter-rouge">Conv2d</code> layer with 50 filters after the last MaxPooling2D layer. Train it for 20 epochs and plot the results.</p>

  <p><strong>HINT</strong>:
The model definition that we used previously needs to be adjusted as follows:</p>
  <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>inputs = keras.Input(shape=train_images.shape[1:])
x = keras.layers.Conv2D(50, (3, 3), activation='relu')(inputs)
x = keras.layers.MaxPooling2D((2, 2))(x)
x = keras.layers.Conv2D(50, (3, 3), activation='relu')(x)
x = keras.layers.MaxPooling2D((2, 2))(x)
# Add your extra layer here
x = keras.layers.Flatten()(x)
x = keras.layers.Dense(50, activation='relu')(x)
outputs = keras.layers.Dense(10)(x)
</code></pre></div>  </div>
  <blockquote class="solution">

    <h2 id="solution-5">Solution</h2>
    <p>We add an extra Conv2D layer after the second pooling layer:</p>
    <div class="language-plaintext output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>inputs = keras.Input(shape=train_images.shape[1:])
x = keras.layers.Conv2D(50, (3, 3), activation='relu')(inputs)
x = keras.layers.MaxPooling2D((2, 2))(x)
x = keras.layers.Conv2D(50, (3, 3), activation='relu')(x)
x = keras.layers.MaxPooling2D((2, 2))(x)
x = keras.layers.Conv2D(50, (3, 3), activation='relu')(x)
x = keras.layers.Flatten()(x)
x = keras.layers.Dense(50, activation='relu')(x)
outputs = keras.layers.Dense(10)(x)

model = keras.Model(inputs=inputs, outputs=outputs, name="cifar_model")
</code></pre></div>    </div>
    <p>With the model defined above, we can inspect the number of parameters:</p>
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span><span class="p">.</span><span class="n">summary</span><span class="p">()</span>
</code></pre></div>    </div>
    <div class="language-plaintext output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Model: "cifar_model"
_________________________________________________________________
 Layer (type)                Output Shape              Param #
=================================================================
 input_7 (InputLayer)        [(None, 32, 32, 3)]       0

 conv2d_16 (Conv2D)          (None, 30, 30, 50)        1400

 max_pooling2d_10 (MaxPoolin  (None, 15, 15, 50)       0
 g2D)

 conv2d_17 (Conv2D)          (None, 13, 13, 50)        22550

 max_pooling2d_11 (MaxPoolin  (None, 6, 6, 50)         0
 g2D)

 conv2d_18 (Conv2D)          (None, 4, 4, 50)          22550

 flatten_6 (Flatten)         (None, 800)               0

 dense_11 (Dense)            (None, 50)                40050

 dense_12 (Dense)            (None, 10)                510

=================================================================
Total params: 87,060
Trainable params: 87,060
Non-trainable params: 0
_________________________________________________________________
</code></pre></div>    </div>
    <p>The number of parameters has decreased by adding this layer.
We can see that the conv layer decreases the resolution from 6x6 to 4x4,
as a result, the input of the Dense layer is smaller than in the previous network.</p>

    <p>To train the network and plot the results:</p>
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span><span class="p">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s">'adam'</span><span class="p">,</span>
              <span class="n">loss</span><span class="o">=</span><span class="n">keras</span><span class="p">.</span><span class="n">losses</span><span class="p">.</span><span class="n">SparseCategoricalCrossentropy</span><span class="p">(</span><span class="n">from_logits</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span>
              <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s">'accuracy'</span><span class="p">])</span>

<span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_images</span><span class="p">,</span> <span class="n">train_labels</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
                    <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">test_images</span><span class="p">,</span> <span class="n">test_labels</span><span class="p">))</span>

<span class="n">history_df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">.</span><span class="n">from_dict</span><span class="p">(</span><span class="n">history</span><span class="p">.</span><span class="n">history</span><span class="p">)</span>
<span class="n">sns</span><span class="p">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">history_df</span><span class="p">[[</span><span class="s">'accuracy'</span><span class="p">,</span> <span class="s">'val_accuracy'</span><span class="p">]])</span>
</code></pre></div>    </div>
    <p><img src="../fig/04_training_history_2.png" alt="Plot of training accuracy and validation accuracy vs epochs for the trained model" /></p>
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">sns</span><span class="p">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">history_df</span><span class="p">[[</span><span class="s">'loss'</span><span class="p">,</span> <span class="s">'val_loss'</span><span class="p">]])</span>
</code></pre></div>    </div>
    <p><img src="../fig/04_training_history_loss_2.png" alt="Plot of training loss and validation loss vs epochs for the trained model" /></p>
  </blockquote>
</blockquote>

<blockquote class="callout">
  <h2 id="other-types-of-data">Other types of data</h2>

  <p>Convolutional and Pooling layers are also applicable to different types of
data than image data. Whenever the data is ordered in a (spatial) dimension,
and <em>translation invariant</em> features are expected to be useful, convolutions
can be used. Think for example of time series data from an accelerometer,
audio data for speech recognition, or 3d structures of chemical compounds.</p>
</blockquote>

<blockquote class="challenge">
  <h2 id="why-and-when-to-use-convolutional-neural-networks">Why and when to use convolutional neural networks</h2>

  <ol>
    <li>Would it make sense to train a convolutional neural network (CNN) on the penguins dataset and why?</li>
    <li>Would it make sense to train a CNN on the weather dataset and why?</li>
    <li>(Optional) Can you think of a different machine learning task that would benefit from a
CNN architecture?</li>
  </ol>

  <blockquote class="solution">

    <h2 id="solution-6">Solution</h2>
    <ol>
      <li>No that would not make sense. Convolutions only work when the features of the data can be ordered 
in a meaningful way. Pixels for example are ordered in a spatial dimension. 
This kind of order cannot be applied to the features of the penguin dataset.
If we would have pictures or audio recordings of the penguins as input data
it would make sense to use a CNN architecture.</li>
      <li>It would make sense, but only if we approach the problem from a different angle then we did before.
Namely, 1D convolutions work quite well on sequential data such as timeseries. If we have as our input a matrix
of the different weather conditions over time in the past x days, a CNN would be suited to quickly grasp
the temporal relationship over days.</li>
      <li>Some example domains in which CNNs are applied:
        <ul>
          <li>Text data</li>
          <li>Timeseries, specifically audio</li>
          <li>Molecular structures</li>
        </ul>
      </li>
    </ol>

  </blockquote>
</blockquote>

<h2 id="dropout">Dropout</h2>

<p>Note that the training loss continues to decrease, while the validation loss stagnates, and even starts to increase over the course of the epochs. Similarly, the accuracy for the validation set does not improve anymore after some epochs. This means we are overfitting on our training data set.</p>

<p>Techniques to avoid overfitting, or to improve model generalization, are termed <strong>regularization techniques</strong>.
One of the most versatile regularization technique is <strong>dropout</strong> (<a href="https://jmlr.org/papers/v15/srivastava14a.html">Srivastava et al., 2014</a>).
Dropout essentially means that during each training cycle a random fraction of the dense layer nodes are turned off. This is described with the dropout rate between 0 and 1 which determines the fraction of nodes to silence at a time.
<img src="../fig/neural_network_sketch_dropout.png" alt="A sketch of a neural network with and without dropout" />
The intuition behind dropout is that it enforces redundancies in the network by constantly removing different elements of a network. The model can no longer rely on individual nodes and instead must create multiple “paths”. In addition, the model has to make predictions with much fewer nodes and weights (connections between the nodes).
As a result, it becomes much harder for a network to memorize particular features. At first this might appear a quiet drastic approach which affects the network architecture strongly.
In practice, however, dropout is computationally a very elegant solution which does not affect training speed. And it frequently works very well.</p>

<p><strong>Important to note:</strong> Dropout layers will only randomly silence nodes during training! During a predictions step, all nodes remain active (dropout is off). During training, the sample of nodes that are silenced are different for each training instance, to give all nodes a chance to observe enough training data to learn its weights.</p>

<p>Let us add one dropout layer towards the end of the network, that randomly drops 20% of the input units.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">inputs</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="n">train_images</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">)(</span><span class="n">inputs</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">MaxPooling2D</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">MaxPooling2D</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.8</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span> <span class="c1"># This is new!
</span><span class="n">x</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Flatten</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>

<span class="n">model_dropout</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s">"cifar_model"</span><span class="p">)</span>

<span class="n">model_dropout</span><span class="p">.</span><span class="n">summary</span><span class="p">()</span>
</code></pre></div></div>
<div class="language-plaintext output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Model: "cifar_model"
_________________________________________________________________
 Layer (type)                Output Shape              Param #
=================================================================
 input_8 (InputLayer)        [(None, 32, 32, 3)]       0

 conv2d_19 (Conv2D)          (None, 30, 30, 50)        1400

 max_pooling2d_12 (MaxPoolin  (None, 15, 15, 50)       0
 g2D)

 conv2d_20 (Conv2D)          (None, 13, 13, 50)        22550

 max_pooling2d_13 (MaxPoolin  (None, 6, 6, 50)         0
 g2D)

 conv2d_21 (Conv2D)          (None, 4, 4, 50)          22550

 dropout_2 (Dropout)         (None, 4, 4, 50)          0

 flatten_7 (Flatten)         (None, 800)               0

 dense_13 (Dense)            (None, 50)                40050

 dense_14 (Dense)            (None, 10)                510

=================================================================
Total params: 87,060
Trainable params: 87,060
Non-trainable params: 0
_________________________________________________________________
</code></pre></div></div>
<p>We can see that the dropout does not alter the dimensions of the image, and has zero parameters.</p>

<p>We again compile and train the model.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model_dropout</span><span class="p">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s">'adam'</span><span class="p">,</span>
              <span class="n">loss</span><span class="o">=</span><span class="n">keras</span><span class="p">.</span><span class="n">losses</span><span class="p">.</span><span class="n">SparseCategoricalCrossentropy</span><span class="p">(</span><span class="n">from_logits</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span>
              <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s">'accuracy'</span><span class="p">])</span>

<span class="n">history_dropout</span> <span class="o">=</span> <span class="n">model_dropout</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_images</span><span class="p">,</span> <span class="n">train_labels</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
                    <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">test_images</span><span class="p">,</span> <span class="n">test_labels</span><span class="p">))</span>
</code></pre></div></div>

<p>And inspect the training results:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">history_df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">.</span><span class="n">from_dict</span><span class="p">(</span><span class="n">history_dropout</span><span class="p">.</span><span class="n">history</span><span class="p">)</span>
<span class="n">history_df</span><span class="p">[</span><span class="s">'epoch'</span><span class="p">]</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">history_df</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
<span class="n">history_df</span> <span class="o">=</span> <span class="n">history_df</span><span class="p">.</span><span class="n">set_index</span><span class="p">(</span><span class="s">'epoch'</span><span class="p">)</span>
<span class="n">sns</span><span class="p">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">history_df</span><span class="p">[[</span><span class="s">'accuracy'</span><span class="p">,</span> <span class="s">'val_accuracy'</span><span class="p">]])</span>

<span class="n">test_loss</span><span class="p">,</span> <span class="n">test_acc</span> <span class="o">=</span> <span class="n">model_dropout</span><span class="p">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">test_images</span><span class="p">,</span>  <span class="n">test_labels</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</code></pre></div></div>
<div class="language-plaintext output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>313/313 - 2s - loss: 1.4683 - accuracy: 0.5307
</code></pre></div></div>
<p><img src="../fig/04_training_history_3.png" alt="Plot of training accuracy and validation accuracy vs epochs for the trained model" /></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">sns</span><span class="p">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">history_df</span><span class="p">[[</span><span class="s">'loss'</span><span class="p">,</span> <span class="s">'val_loss'</span><span class="p">]])</span>
</code></pre></div></div>
<p><img src="../fig/04_training_history_loss_3.png" alt="Plot of training loss and validation loss vs epochs for the trained model" /></p>

<p>Now we see that the gap between the training accuracy and validation accuracy is much smaller, and that the final accuracy on the validation set is higher than without dropout.
Nevertheless, there is still some difference between the training loss and validation loss, so we could experiment with regularization even more.</p>

<blockquote class="challenge">
  <h2 id="vary-dropout-rate">Vary dropout rate</h2>

  <ol>
    <li>What do you think would happen if you lower the dropout rate? Try it out, and
see how it affects the model training.</li>
    <li>You are varying the dropout rate and checking its effect on the model performance,
what is the term associated to this procedure?</li>
  </ol>

  <blockquote class="solution">

    <h2 id="solution-7">Solution</h2>
    <h3 id="1-varying-the-dropout-rate">1. Varying the dropout rate</h3>
    <p>The code below instantiates and trains a model with varying dropout rates.
You can see from the resulting plot that the ideal dropout rate in this case is around 0.45.
This is where the test loss is lowest.</p>

    <ul>
      <li>NB1: It takes a while to train these 5 networks.</li>
      <li>NB2: In the real world you should do this with a validation set and not with the test set!</li>
    </ul>

    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">dropout_rates</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.15</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.45</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.75</span><span class="p">]</span>
<span class="n">test_losses</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">dropout_rate</span> <span class="ow">in</span> <span class="n">dropout_rates</span><span class="p">:</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="n">train_images</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">)(</span><span class="n">inputs</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">MaxPooling2D</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">MaxPooling2D</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout_rate</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Flatten</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>

    <span class="n">model_dropout</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s">"cifar_model"</span><span class="p">)</span>

    <span class="n">model_dropout</span><span class="p">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s">'adam'</span><span class="p">,</span>
              <span class="n">loss</span><span class="o">=</span><span class="n">keras</span><span class="p">.</span><span class="n">losses</span><span class="p">.</span><span class="n">SparseCategoricalCrossentropy</span><span class="p">(</span><span class="n">from_logits</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span>
              <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s">'accuracy'</span><span class="p">])</span>

    <span class="n">model_dropout</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_images</span><span class="p">,</span> <span class="n">train_labels</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
                    <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">test_images</span><span class="p">,</span> <span class="n">test_labels</span><span class="p">))</span>

    <span class="n">test_loss</span><span class="p">,</span> <span class="n">test_acc</span> <span class="o">=</span> <span class="n">model_dropout</span><span class="p">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">test_images</span><span class="p">,</span>  <span class="n">test_labels</span><span class="p">)</span>
    <span class="n">test_losses</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_loss</span><span class="p">)</span>

<span class="n">loss_df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s">'dropout_rate'</span><span class="p">:</span> <span class="n">dropout_rates</span><span class="p">,</span> <span class="s">'test_loss'</span><span class="p">:</span> <span class="n">test_losses</span><span class="p">})</span>

<span class="n">sns</span><span class="p">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">loss_df</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s">'dropout_rate'</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s">'test_loss'</span><span class="p">)</span>
</code></pre></div>    </div>
    <p><img src="../fig/04_vary_dropout_rate.png" alt="Plot of test loss vs dropout rate used in the model. The test loss varies between 1.26 and 1.40 and is lowest with a dropout_rate around 0.45." /></p>

    <h3 id="2-term-associated-to-this-procedure">2. Term associated to this procedure</h3>
    <p>This is called hyperparameter tuning.</p>
  </blockquote>
</blockquote>

<blockquote class="keypoints">
  <h2>Key Points</h2>
  <ul>
    
    <li><p>Convolutional layers make efficient reuse of model parameters.</p>
</li>
    
    <li><p>Pooling layers decrease the resolution of your input</p>
</li>
    
    <li><p>Dropout is a way to prevent overfitting</p>
</li>
    
  </ul>
</blockquote>

<hr />


</article>


      
      






<footer>
  <hr/>
  <div class="row">
    <div class="col-md-6 license" id="license-info" align="left">
	
        Licensed under <a href="https://creativecommons.org/licenses/by/4.0/">CC-BY 4.0</a> 2023 by <a href="../CITATION">the authors</a>.
	
    </div>
    <div class="col-md-6 help-links" align="right">
	
	<a href="/edit//aio.md" data-checker-ignore>Edit on GitHub</a>
	
	/
	<a href="/blob//CONTRIBUTING.md" data-checker-ignore>Contributing</a>
	/
	<a href="/">Source</a>
	/
	<a href="/blob//CITATION" data-checker-ignore>Cite</a>
	/
	<a href="mailto:d.vankuppevelt@esciencecenter.nl">Contact</a>
    </div>
  </div>
  <p class="text-muted text-right">
    <small><i>Using <a href="https://github.com/carpentries/carpentries-theme/">The Carpentries theme</a> &mdash; Site last built on: 2023-05-17 15:41:55 +0200.</i></small>
  </p>
</footer>

      
    </div>
    
<script src="../assets/js/jquery.min.js"></script>
<script src="../assets/js/bootstrap.min.js"></script>
<script src="../assets/js/lesson.js"></script>


<!-- Matomo -->
<script>
  var _paq = window._paq = window._paq || [];
  /* tracker methods like "setCustomDimension" should be called before "trackPageView" */
  _paq.push(["setDocumentTitle", document.domain + "/" + document.title]);
  _paq.push(["setDomains", ["*.lessons.carpentries.org","*.datacarpentry.github.io","*.datacarpentry.org","*.librarycarpentry.github.io","*.librarycarpentry.org","*.swcarpentry.github.io"]]);
  _paq.push(["setDoNotTrack", true]);
  _paq.push(["disableCookies"]);
  _paq.push(['trackPageView']);
  _paq.push(['enableLinkTracking']);
  (function() {
    var u="https://carpentries.matomo.cloud/";
    _paq.push(['setTrackerUrl', u+'matomo.php']);
    _paq.push(['setSiteId', '1']);
    var d=document, g=d.createElement('script'), s=d.getElementsByTagName('script')[0];
    g.async=true; g.src='//cdn.matomo.cloud/carpentries.matomo.cloud/matomo.js'; s.parentNode.insertBefore(g,s);
  })();
</script>
<!-- End Matomo Code -->


<script src="../assets/js/anchor.min.js"></script>
<script>
    anchors.add();
</script>

  </body>
</html>
