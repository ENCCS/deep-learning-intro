

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="./">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Intro to Deep Learning documentation</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
      <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
      <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
      <link rel="stylesheet" type="text/css" href="_static/sphinx_lesson.css?v=0c089442" />
      <link rel="stylesheet" type="text/css" href="_static/term_role_formatting.css?v=4194e21c" />
      <link rel="stylesheet" type="text/css" href="_static/sphinx_rtd_theme_ext_color_contrast.css?v=8e8ea19f" />
      <link rel="stylesheet" type="text/css" href="_static/overrides.css?v=345019e7" />

  
    <link rel="shortcut icon" href="_static/favicon.ico"/>
      <script src="_static/jquery.js?v=5d32c60e"></script>
      <script src="_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="_static/documentation_options.js?v=d6d90d09"></script>
      <script src="_static/doctools.js?v=9bcbadda"></script>
      <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
      <script src="_static/clipboard.min.js?v=a7894cd8"></script>
      <script src="_static/copybutton.js?v=35a8b989"></script>
      <script src="_static/minipres.js?v=a0d29692"></script>
      <script>let toggleHintShow = 'Click to show';</script>
      <script>let toggleHintHide = 'Click to hide';</script>
      <script>let toggleOpenOnPrint = 'true';</script>
      <script src="_static/togglebutton.js?v=4a39c7ea"></script>
      <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
      <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
      <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
      <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="#" class="icon icon-home">
            Intro to Deep Learning
              <img src="_static/ENCCS.jpg" class="logo" alt="Logo"/>
          </a>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Preparation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="#document-setup">Setup</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">The lesson</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="#document-1-introduction">1. Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="#document-2-keras">2. Classification by a neural network using Keras</a></li>
<li class="toctree-l1"><a class="reference internal" href="#document-3-monitor-the-model">3. Monitor the training process</a></li>
<li class="toctree-l1"><a class="reference internal" href="#document-4-advanced-layer-types">4. Advanced layer types</a></li>
<li class="toctree-l1"><a class="reference internal" href="#document-5-transfer-learning">5. Transfer learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="#document-6-outlook">6. Outlook</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="#document-reference">Reference for learners</a></li>
<li class="toctree-l1"><a class="reference internal" href="#document-instructor-notes">Instructor notes</a></li>
<li class="toctree-l1"><a class="reference internal" href="#document-learner-profiles">Learner profiles</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="#">Intro to Deep Learning</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="#" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Intro to Deep Learning  documentation</li>
      <li class="wy-breadcrumbs-aside">
              <a href="https://github.com/ENCCS/deep-learning-intro/blob/sphinx/content/index" class="fa fa-github"> Edit on GitHub</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section class="tex2jax_ignore mathjax_ignore" id="introduction-to-deep-learning">
<h1>Introduction to Deep Learning<a class="headerlink" href="#introduction-to-deep-learning" title="Link to this heading"></a></h1>
<p>This is a hands-on introduction to the first steps in deep learning, intended for researchers who are familiar with (non-deep) machine learning.</p>
<p>The use of deep learning has seen a sharp increase of popularity and applicability over the last decade.
While deep learning can be a useful tool for researchers from a wide range of domains,
taking the first steps in the world of deep learning can be somewhat intimidating.
This introduction covers the basics of deep learning in a practical and hands-on manner,
so that upon completion, you will be able to train your first neural network and understand what next steps to take to improve the model.</p>
<p>We start with explaining the basic concepts of neural networks, and then go through the different steps of a deep learning workflow.
Learners will learn how to prepare data for deep learning, how to implement a basic deep learning model in Python with Keras,
how to monitor and troubleshoot the training process and how to implement different layer types such as convolutional layers.</p>
<section id="id1">
<h2><a class="headerlink" href="#id1" title="Link to this heading"></a></h2>
<div class="toctree-wrapper compound">
<span id="document-setup"></span><section class="tex2jax_ignore mathjax_ignore" id="setup">
<h3>Setup<a class="headerlink" href="#setup" title="Link to this heading"></a></h3>
<section id="software-setup">
<h4>Software Setup<a class="headerlink" href="#software-setup" title="Link to this heading"></a></h4>
<div class="admonition-discussion discussion important admonition" id="discussion-0">
<p class="admonition-title">Discussion</p>
<p class="rubric" id="installing-python">Installing Python</p>
<p><a class="reference external" href="https://python.org">Python</a> is a popular language for scientific computing, and a frequent choice
for machine learning as well.
To install Python, follow the <a class="reference external" href="https://wiki.python.org/moin/BeginnersGuide/Download">Beginner’s Guide</a> or head straight to the <a class="reference external" href="https://www.python.org/downloads/">download page</a>.</p>
<p>Please set up your python environment at least a day in advance of the workshop.
If you encounter problems with the installation procedure, ask your workshop organizers via e-mail for assistance so
you are ready to go as soon as the workshop begins.</p>
</div>
</section>
<section id="installing-the-required-packages">
<span id="packages"></span><h4>Installing the required packages<a class="headerlink" href="#installing-the-required-packages" title="Link to this heading"></a></h4>
<p><a class="reference external" href="https://pip.pypa.io/en/stable/">Pip</a> is the package management system built into Python.
Pip should be available in your system once you installed Python successfully.</p>
<p>Open a terminal (Mac/Linux) or Command Prompt (Windows) and run the following commands.</p>
<ol class="arabic simple">
<li><p>Create a <a class="reference external" href="https://packaging.python.org/en/latest/guides/installing-using-pip-and-virtual-environments/#create-and-use-virtual-environments">virtual environment</a> called <code class="docutils literal notranslate"><span class="pre">dl_workshop</span></code>:</p></li>
</ol>
<div class="admonition-on-linux-macos spoiler important dropdown admonition" id="spoiler-0">
<p class="admonition-title">On Linux/macOs</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python3<span class="w"> </span>-m<span class="w"> </span>venv<span class="w"> </span>dl_workshop
</pre></div>
</div>
</div>
<div class="admonition-on-windows spoiler important dropdown admonition" id="spoiler-1">
<p class="admonition-title">On Windows</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>py<span class="w"> </span>-m<span class="w"> </span>venv<span class="w"> </span>dl_workshop
</pre></div>
</div>
</div>
<ol class="arabic simple" start="2">
<li><p>Activate the newly created virtual environment:</p></li>
</ol>
<div class="admonition-on-linux-macos spoiler important dropdown admonition" id="spoiler-2">
<p class="admonition-title">On Linux/macOs</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">source</span><span class="w"> </span>dl_workshop/bin/activate
</pre></div>
</div>
</div>
<div class="admonition-on-windows spoiler important dropdown admonition" id="spoiler-3">
<p class="admonition-title">On Windows</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>dl_workshop<span class="se">\S</span>cripts<span class="se">\a</span>ctivate
</pre></div>
</div>
</div>
<p>Remember that you need to activate your environment every time you restart your terminal!</p>
<ol class="arabic simple" start="3">
<li><p>Install the required packages:</p></li>
</ol>
<div class="admonition-on-linux-macos spoiler important dropdown admonition" id="spoiler-4">
<p class="admonition-title">On Linux/macOs</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python3<span class="w"> </span>-m<span class="w"> </span>pip<span class="w"> </span>install<span class="w"> </span>jupyter<span class="w"> </span>seaborn<span class="w"> </span>scikit-learn<span class="w"> </span>pandas<span class="w"> </span>tensorflow<span class="w"> </span>pydot
</pre></div>
</div>
<p>Note for MacOS users: there is a package <code class="docutils literal notranslate"><span class="pre">tensorflow-metal</span></code> which accelerates the training of machine learning models with TensorFlow on a recent Mac with a Silicon chip (M1/M2/M3).
However, the installation is currently broken in the most recent version (as of January 2025), see the <a class="reference external" href="https://developer.apple.com/forums/thread/772147">developer forum</a>.</p>
</div>
<div class="admonition-on-windows spoiler important dropdown admonition" id="spoiler-5">
<p class="admonition-title">On Windows</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>py<span class="w"> </span>-m<span class="w"> </span>pip<span class="w"> </span>install<span class="w"> </span>jupyter<span class="w"> </span>seaborn<span class="w"> </span>scikit-learn<span class="w"> </span>pandas<span class="w"> </span>tensorflow<span class="w"> </span>pydot
</pre></div>
</div>
</div>
<p>Note: Tensorflow makes Keras available as a module too.</p>
<p>An <a class="reference internal" href="#episodes/2-keras.md"><span class="xref myst">optional challenge in episode 2</span></a> requires installation of Graphviz
and instructions for doing that can be found
<a class="reference external" href="https://graphviz.org/download/">by following this link</a>.</p>
</section>
<section id="starting-jupyter-lab">
<h4>Starting Jupyter Lab<a class="headerlink" href="#starting-jupyter-lab" title="Link to this heading"></a></h4>
<p>We will teach using Python in <a class="reference external" href="http://jupyter.org/">Jupyter Lab</a>, a programming environment that runs in a web browser.
Jupyter Lab is compatible with Firefox, Chrome, Safari and Chromium-based browsers.
Note that Internet Explorer and Edge are <em>not</em> supported.
See the <a class="reference external" href="https://jupyterlab.readthedocs.io/en/latest/getting_started/accessibility.html#compatibility-with-browsers-and-assistive-technology">Jupyter Lab documentation</a> for an up-to-date list of supported browsers.</p>
<p>To start Jupyter Lab, open a terminal (Mac/Linux) or Command Prompt (Windows),
make sure that you activated the virtual environment you created for this course,
and type the command:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>jupyter<span class="w"> </span>lab
</pre></div>
</div>
</section>
<section id="check-your-setup">
<h4>Check your setup<a class="headerlink" href="#check-your-setup" title="Link to this heading"></a></h4>
<p>To check whether all packages installed correctly, start a jupyter notebook in jupyter lab as
explained above. Run the following lines of code:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">sklearn</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;sklearn version: &#39;</span><span class="p">,</span> <span class="n">sklearn</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">seaborn</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;seaborn version: &#39;</span><span class="p">,</span> <span class="n">seaborn</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;pandas version: &#39;</span><span class="p">,</span> <span class="n">pandas</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">tensorflow</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Tensorflow version: &#39;</span><span class="p">,</span> <span class="n">tensorflow</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span>
</pre></div>
</div>
<p>This should output the versions of all required packages without giving errors.
Most versions will work fine with this lesson, but:</p>
<ul class="simple">
<li><p>For Keras and Tensorflow, the minimum version is 2.12.0</p></li>
<li><p>For sklearn, the minimum version is 1.2.2</p></li>
</ul>
</section>
<section id="fallback-option-cloud-environment">
<h4>Fallback option: cloud environment<a class="headerlink" href="#fallback-option-cloud-environment" title="Link to this heading"></a></h4>
<p>If a local installation does not work for you, it is also possible to run this lesson in <a class="reference external" href="https://mybinder.org/v2/gh/carpentries-incubator/deep-learning-intro/scaffolds">Binder Hub</a>. This should give you an environment with all the required software and data to run this lesson, nothing which is saved will be stored, please copy any files you want to keep. Note that if you are the first person to launch this in the last few days it can take several minutes to startup. The second person who loads it should find it loads in under a minute. Instructors who intend to use this option should start it themselves shortly before the workshop begins.</p>
<p>Alternatively you can use <a class="reference external" href="https://colab.research.google.com/">Google colab</a>. If you open a jupyter notebook here, the required packages are already pre-installed. Note that google colab uses jupyter notebook instead of Jupyter Lab.</p>
</section>
<section id="downloading-the-required-datasets">
<h4>Downloading the required datasets<a class="headerlink" href="#downloading-the-required-datasets" title="Link to this heading"></a></h4>
<p>Download the <a class="reference external" href="https://zenodo.org/record/5071376/files/weather_prediction_dataset_light.csv?download=1">weather dataset prediction csv</a> and <a class="reference external" href="https://zenodo.org/api/records/10970014/files-archive">Dollar street dataset (4 files in total)</a></p>
</section>
</section>
</div>
<div class="toctree-wrapper compound">
<span id="document-1-introduction"></span><section class="tex2jax_ignore mathjax_ignore" id="introduction">
<h3>1. Introduction<a class="headerlink" href="#introduction" title="Link to this heading"></a></h3>
<div class="admonition-questions questions admonition" id="questions-0">
<p class="admonition-title">Questions</p>
<ul class="simple">
<li><p>What is deep learning?</p></li>
<li><p>What is a neural network?</p></li>
<li><p>Which operations are performed by a single neuron?</p></li>
<li><p>How do neural networks learn?</p></li>
<li><p>When does it make sense to use and not use deep learning?</p></li>
<li><p>What are tools involved in deep learning?</p></li>
<li><p>What is the workflow for deep learning?</p></li>
<li><p>Why did we choose to use Keras in this lesson?</p></li>
</ul>
</div>
<div class="admonition-objectives objectives admonition" id="objectives-0">
<p class="admonition-title">Objectives</p>
<ul class="simple">
<li><p>Define deep learning</p></li>
<li><p>Describe how a neural network is build up</p></li>
<li><p>Explain the operations performed by a single neuron</p></li>
<li><p>Describe what a loss function is</p></li>
<li><p>Recall the sort of problems for which deep learning is a useful tool</p></li>
<li><p>List some of the available tools for deep learning</p></li>
<li><p>Recall the steps of a deep learning workflow</p></li>
<li><p>Test that you have correctly installed the Keras, Seaborn and scikit-learn libraries</p></li>
</ul>
</div>
<section id="what-is-deep-learning">
<h4>What is Deep Learning?<a class="headerlink" href="#what-is-deep-learning" title="Link to this heading"></a></h4>
<section id="deep-learning-machine-learning-and-artificial-intelligence">
<h5>Deep Learning, Machine Learning and Artificial Intelligence<a class="headerlink" href="#deep-learning-machine-learning-and-artificial-intelligence" title="Link to this heading"></a></h5>
<p>Deep learning (DL) is just one of many techniques collectively known as machine learning. Machine learning (ML) refers to techniques where a computer can “learn” patterns in data, usually by being shown numerous examples to train it. People often talk about machine learning being a form of artificial intelligence (AI). Definitions of artificial intelligence vary, but usually involve having computers mimic the behaviour of intelligent biological systems. Since the 1950s many works of science fiction have dealt with the idea of an artificial intelligence which matches (or exceeds) human intelligence in all areas. Although there have been great advances in AI and ML research recently we can only come close to human like intelligence in a few specialist areas and are still a long way from a general purpose AI.
The image below shows some differences between artificial intelligence, machine learning and deep learning.</p>
<p><a class="reference internal" href="_images/01_AI_ML_DL_differences.png"><img alt="" class="align-center" src="_images/01_AI_ML_DL_differences.png" style="width: 408px;" /></a></p>
<section id="neural-networks">
<h6>Neural Networks<a class="headerlink" href="#neural-networks" title="Link to this heading"></a></h6>
<p>A neural network is an artificial intelligence technique loosely based on the way neurons in the brain work.
A neural network consists of connected computational units called <strong>neurons</strong>.
Let’s look at the operations of a single neuron.</p>
<section id="a-single-neuron">
<h6 aria-level="7">A single neuron<a class="headerlink" href="#a-single-neuron" title="Link to this heading"></a></h6>
<p>Each neuron …</p>
<ul class="simple">
<li><p>has one or more inputs (<span class="math notranslate nohighlight">\(x_1, x_2, ...\)</span>), e.g. input data expressed as floating point numbers</p></li>
<li><p>most of the time, each neuron conducts 3 main operations:</p>
<ul>
<li><p>take the weighted sum of the inputs where (<span class="math notranslate nohighlight">\(w_1, w_2, ...\)</span>) indicate weights</p></li>
<li><p>add an extra constant weight (i.e. a bias term) to this weighted sum</p></li>
<li><p>apply an <strong>activation function</strong> to the output so far, we will explain activation functions</p></li>
</ul>
</li>
<li><p>return one output value, again a floating point number.</p></li>
<li><p>one example equation to calculate the output for a neuron is: <span class="math notranslate nohighlight">\(output = Activation(\sum_{i} (x_i*w_i) + bias)\)</span></p></li>
</ul>
<p><a class="reference internal" href="_images/01_neuron.png"><img alt="" class="align-center" src="_images/01_neuron.png" style="width: 600px;" /></a></p>
</section>
<section id="activation-functions">
<h6 aria-level="7">Activation functions<a class="headerlink" href="#activation-functions" title="Link to this heading"></a></h6>
<p>The goal of the activation function is to convert the weighted sum of the inputs to the output signal of the neuron.
This output is then passed on to the next layer of the network.
There are many different activation functions, 3 of them are introduced in the exercise below.</p>
<div class="admonition-activation-functions exercise important admonition" id="exercise-0">
<p class="admonition-title">Activation functions</p>
<p>Look at the following activation functions:</p>
<p><strong>A. Sigmoid activation function</strong>
The sigmoid activation function is given by:
<div class="math notranslate nohighlight">
\[ f(x) = \frac{1}{1 + e^{-x}} \]</div>
</p>
<p><a class="reference internal" href="_images/01_sigmoid.svg"><img alt="" class="align-center" src="_images/01_sigmoid.svg" style="width: 476px;" /></a>
<br clear="all" /></p>
<p><strong>B. ReLU activation function</strong>
The Rectified Linear Unit (ReLU) activation function is defined as:
<div class="math notranslate nohighlight">
\[ f(x) = \max(0, x) \]</div>
</p>
<p>This involves a simple comparison and maximum calculation, which are basic operations that are computationally inexpensive.
It is also simple to compute the gradient: 1 for positive inputs and 0 for negative inputs.</p>
<p><a class="reference internal" href="_images/01_relu.svg"><img alt="" class="align-center" src="_images/01_relu.svg" style="width: 476px;" /></a>
<br clear="all" /></p>
<p><strong>C. Linear (or identity) activation function (output=input)</strong>
The linear activation function is simply the identity function:
<div class="math notranslate nohighlight">
\[ f(x) = x \]</div>
</p>
<p><a class="reference internal" href="_images/01_identity_function.svg"><img alt="" class="align-center" src="_images/01_identity_function.svg" style="width: 476px;" /></a>
<br clear="all" /></p>
<p>Combine the following statements to the correct activation function:</p>
<ol class="arabic simple">
<li><p>This function enforces the activation of a neuron to be between 0 and 1</p></li>
<li><p>This function is useful in regression tasks when applied to an output neuron</p></li>
<li><p>This function is the most popular activation function in hidden layers, since it introduces non-linearity in a computationally efficient way.</p></li>
<li><p>This function is useful in classification tasks when applied to an output neuron</p></li>
<li><p>(optional) For positive values this function results in the same activations as the identity function.</p></li>
<li><p>(optional) This function is not differentiable at 0</p></li>
<li><p>(optional) This function is the default for Dense layers (search the Keras documentation!)</p></li>
</ol>
<p><em>Activation function plots by Laughsinthestocks - Own work, CC BY-SA 4.0, https://commons.wikimedia.org/w/index.php?curid=44920411,
https://commons.wikimedia.org/w/index.php?curid=44920600, https://commons.wikimedia.org/w/index.php?curid=44920533</em></p>
<div class="admonition-solution solution important dropdown admonition" id="solution-0">
<p class="admonition-title">Solution</p>
<ol class="arabic simple">
<li><p>A</p></li>
<li><p>C</p></li>
<li><p>B</p></li>
<li><p>A</p></li>
<li><p>B</p></li>
<li><p>B</p></li>
<li><p>C</p></li>
</ol>
</div>
</div>
<div class="docutils">
<p class="rubric" id="combining-multiple-neurons-into-a-network">Combining multiple neurons into a network</p>
<p>Multiple neurons can be joined together by connecting the output of one to the input of another. These connections are associated with weights that determine the ‘strength’ of the connection, the weights are adjusted during training. In this way, the combination of neurons and connections describe a computational graph, an example can be seen in the image below.</p>
<p>In most neural networks, neurons are aggregated into layers. Signals travel from the input layer to the output layer, possibly through one or more intermediate layers called hidden layers.
The image below shows an example of a neural network with three layers, each circle is a neuron, each line is an edge and the arrows indicate the direction data moves in.</p>
<p><img alt="" class="align-center" src="_images/01_neural_net.png" /></p>
<div class="note docutils">
<p>Image credit: Glosser.ca, CC BY-SA 3.0 <a class="reference external" href="https://creativecommons.org/licenses/by-sa/3.0">https://creativecommons.org/licenses/by-sa/3.0</a>, via Wikimedia Commons,
original source: <a class="reference external" href="https://commons.wikimedia.org/wiki/File:Colored_neural_network.svg">https://commons.wikimedia.org/wiki/File:Colored_neural_network.svg</a></p>
</div>
</div>
<div class="admonition-neural-network-calculations exercise important admonition" id="exercise-1">
<p class="admonition-title">Neural network calculations</p>
<p>.</p>
<p class="rubric" id="calculate-the-output-for-one-neuron">1. Calculate the output for one neuron</p>
<p>Suppose we have:</p>
<ul class="simple">
<li><p>Input: X = (0, 0.5, 1)</p></li>
<li><p>Weights: W = (-1, -0.5, 0.5)</p></li>
<li><p>Bias: b = 1</p></li>
<li><p>Activation function <em>relu</em>: <code class="docutils literal notranslate"><span class="pre">f(x)</span> <span class="pre">=</span> <span class="pre">max(x,</span> <span class="pre">0)</span></code></p></li>
</ul>
<p>What is the output of the neuron?</p>
<p><em>Note: You can use whatever you like: brain only, pen&amp;paper, Python, Excel…</em></p>
<p class="rubric" id="optional-calculate-outputs-for-a-network">2. (optional) Calculate outputs for a network</p>
<p>Have a look at the following network where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(X_1\)</span> and <span class="math notranslate nohighlight">\(X_2\)</span> denote the two inputs of the network.</p></li>
<li><p><span class="math notranslate nohighlight">\(h_1\)</span> and <span class="math notranslate nohighlight">\(h_2\)</span> denote the two neurons in the hidden layer. They both have ReLU activation functions.</p></li>
<li><p><span class="math notranslate nohighlight">\(h_1\)</span> and <span class="math notranslate nohighlight">\(h_2\)</span> denotes the output neuron. It has a ReLU activation function.</p></li>
<li><p>The value on the arrows represent the weight associated to that input to the neuron.</p></li>
<li><p><span class="math notranslate nohighlight">\(b_i\)</span> denotes the bias term of that specific neuron
<a class="reference internal" href="_images/01_xor_exercise.png"><img alt="" class="align-center" src="_images/01_xor_exercise.png" style="width: 400px;" /></a></p></li>
</ul>
<p>a. Calculate the output of the network for the following combinations of inputs:</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>x1</p></th>
<th class="head"><p>x2</p></th>
<th class="head"><p>y</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>0</p></td>
<td><p>0</p></td>
<td><p>..</p></td>
</tr>
<tr class="row-odd"><td><p>0</p></td>
<td><p>1</p></td>
<td><p>..</p></td>
</tr>
<tr class="row-even"><td><p>1</p></td>
<td><p>0</p></td>
<td><p>..</p></td>
</tr>
<tr class="row-odd"><td><p>1</p></td>
<td><p>1</p></td>
<td><p>..</p></td>
</tr>
</tbody>
</table>
<p>b. What logical problem does this network solve?</p>
<div class="admonition-solution solution important dropdown admonition" id="solution-1">
<p class="admonition-title">Solution</p>
<p class="rubric" id="id1">1: calculate the output for one neuron</p>
<p>You can calculate the output as follows:</p>
<ul class="simple">
<li><p>Weighted sum of input: <code class="docutils literal notranslate"><span class="pre">0</span> <span class="pre">*</span> <span class="pre">(-1)</span> <span class="pre">+</span> <span class="pre">0.5</span> <span class="pre">*</span> <span class="pre">(-0.5)</span> <span class="pre">+</span> <span class="pre">1</span> <span class="pre">*</span> <span class="pre">0.5</span> <span class="pre">=</span> <span class="pre">0.25</span></code></p></li>
<li><p>Add the bias: <code class="docutils literal notranslate"><span class="pre">0.25</span> <span class="pre">+</span> <span class="pre">1</span> <span class="pre">=</span> <span class="pre">1.25</span></code></p></li>
<li><p>Apply activation function: <code class="docutils literal notranslate"><span class="pre">max(1.25,</span> <span class="pre">0)</span> <span class="pre">=</span> <span class="pre">1.25</span></code></p></li>
</ul>
<p>So, the neuron’s output is <code class="docutils literal notranslate"><span class="pre">1.25</span></code></p>
<p class="rubric" id="calculate-outputs-for-a-network">2: Calculate outputs for a network</p>
<p>a.</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>x1</p></th>
<th class="head"><p>x2</p></th>
<th class="head"><p>y</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>0</p></td>
<td><p>0</p></td>
<td><p><strong>0</strong></p></td>
</tr>
<tr class="row-odd"><td><p>0</p></td>
<td><p>1</p></td>
<td><p><strong>1</strong></p></td>
</tr>
<tr class="row-even"><td><p>1</p></td>
<td><p>1</p></td>
<td><p><strong>0</strong></p></td>
</tr>
<tr class="row-odd"><td><p>1</p></td>
<td><p>0</p></td>
<td><p><strong>1</strong></p></td>
</tr>
</tbody>
</table>
<p>b. This solves the XOR logical problem, the output is 1 if only one of the two inputs is 1.</p>
</div>
</div>
<div class="docutils">
<p class="rubric" id="what-makes-deep-learning-deep-learning">What makes deep learning deep learning?</p>
<p>Neural networks are not a new technique, they have been around since the late 1940s. But until around 2010 neural networks tended to be quite small, consisting of only 10s or perhaps 100s of neurons. This limited them to only solving quite basic problems. Around 2010, improvements in computing power and the algorithms for training the networks made much larger and more powerful networks practical. These are known as deep neural networks or deep learning.</p>
<p>Deep learning requires extensive training using example data which shows the network what output it should produce for a given input. One common application of deep learning is <a class="reference external" href="https://glosario.carpentries.org/en/#classification">classifying</a> images. Here the network will be trained by being “shown” a series of images and told what they contain. Once the network is trained it should be able to take another image and correctly classify its contents.</p>
<p>But we are not restricted to just using images, any kind of data can be learned by a deep learning neural network. This makes them able to appear to learn a set of complex rules only by being shown what the inputs and outputs of those rules are instead of being taught the actual rules. Using these approaches, deep learning networks have been taught to play video games and even drive cars.</p>
<p>The data on which networks are trained usually has to be quite extensive, typically including thousands of examples. For this reason they are not suited to all applications and should be considered just one of many machine learning techniques which are available.</p>
<p>While traditional “shallow” networks might have had between three and five layers, deep networks often have tens or even hundreds of layers. This leads to them having millions of individual weights.
The image below shows a diagram of all the layers on a deep learning network designed to detect pedestrians in images.</p>
<p>This image is from the paper <a class="reference external" href="https://doi.org/10.1155/2018/3518959">“An Efficient Pedestrian Detection Method Based on YOLOv2”
by Zhongmin Liu, Zhicai Chen, Zhanming Li, and Wenjin Hu
published in Mathematical Problems in Engineering, Volume 2018</a></p>
<p><img alt="" class="align-center" src="_images/01_deep_network.png" /></p>
<div class="note docutils">
<p><strong>A visual representation of a deep neural network used to detect pedestrians in images.</strong>
There are too many neurons to draw all of them, so each layer is represented by a panel, with values indicating how many neurons are in each dimension of the layer.
Note that this model has 3-dimensional layers instead of the 1-dimensional layers that we introduced before.
The input (left most) layer of the network is an image of 448 x 448 pixels and 3 RGB channels.
The final (right most) layer of the network outputs a zero or one to determine if the input data belongs to the class of data we are interested in.
The output of the previous layer is the input to the next layer.
Note that the color coding refers to different layer types that will be introduced one by one
as we proceed in this lesson.</p>
</div>
</div>
</section>
</section>
</section>
<section id="how-do-neural-networks-learn">
<h5>How do neural networks learn?<a class="headerlink" href="#how-do-neural-networks-learn" title="Link to this heading"></a></h5>
<p>What happens in a neural network during the training process?
The ultimate goal is of course to find a model that makes predictions that are as close to the target value as possible.
In other words, the goal of training is to find the best set of parameters (weights and biases)
that bring the error between prediction and expected value to a minimum.
The total error between prediction and expected value is quantified in a loss function (also called cost function).
There are lots of loss functions to pick from, and it is important that you pick one that matches your problem definition well.
We will look at an example of a loss function in the next exercise.</p>
<div class="admonition-instructor instructor dropdown admonition" id="instructor-0">
<p class="admonition-title">Instructor</p>
<p>There is an issue when rendering the MSE formula in the following box involving the Chrome browser on MacOS.
To solve it:</p>
<ol class="arabic simple">
<li><p>Right-click on some of the misrendered MathJax.</p></li>
<li><p>Click on “Math Settings”.</p></li>
<li><p>Click on “Math Renderer”.</p></li>
<li><p>Click on “Common HTML”.</p></li>
</ol>
<p>from: https://physics.meta.stackexchange.com/questions/14408/bug-in-mathjax-rendering-using-chrome</p>
</div>
<div class="admonition-exercise-loss-function exercise important admonition" id="exercise-2">
<p class="admonition-title">Exercise: Loss function</p>
<p class="rubric" id="compute-the-mean-squared-error">1. Compute the Mean Squared Error</p>
<p>One of the simplest loss functions is the Mean Squared Error. MSE = <span class="math notranslate nohighlight">\(\frac{1}{n} \Sigma_{i=1}^n({y}-\hat{y})^2\)</span> .
It is the mean of all squared errors, where the error is the difference between the predicted and expected value.
In the following table, fill in the missing values in the ‘squared error’ column. What is the MSE loss
for the predictions on these 4 samples?</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p><strong>Prediction</strong></p></th>
<th class="head"><p><strong>Expected value</strong></p></th>
<th class="head"><p><strong>Squared error</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>1</p></td>
<td><p>-1</p></td>
<td><p>4</p></td>
</tr>
<tr class="row-odd"><td><p>2</p></td>
<td><p>-1</p></td>
<td><p>..</p></td>
</tr>
<tr class="row-even"><td><p>0</p></td>
<td><p>0</p></td>
<td><p>..</p></td>
</tr>
<tr class="row-odd"><td><p>3</p></td>
<td><p>2</p></td>
<td><p>..</p></td>
</tr>
<tr class="row-even"><td><p></p></td>
<td><p><strong>MSE:</strong></p></td>
<td><p>..</p></td>
</tr>
</tbody>
</table>
<p class="rubric" id="optional-huber-loss">2. (optional) Huber loss</p>
<p>A more complicated and less used loss function for regression is the <a class="reference external" href="https://keras.io/api/losses/regression_losses/#huber-class">Huber loss</a>.</p>
<p>Below you see the Huber loss (green, delta = 1) and Squared error loss (blue)
as a function of <code class="docutils literal notranslate"><span class="pre">y_true</span> <span class="pre">-</span> <span class="pre">y_pred</span></code>.</p>
<p><a class="reference internal" href="_images/01_huber_loss.png"><img alt="" class="align-center" src="_images/01_huber_loss.png" style="width: 400px;" /></a></p>
<p>Which loss function is more sensitive to outliers?</p>
<div class="admonition-solution solution important dropdown admonition" id="solution-2">
<p class="admonition-title">Solution</p>
<p class="rubric" id="id2">1. ‘Compute the Mean Squared Error’</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p><strong>Prediction</strong></p></th>
<th class="head"><p><strong>Expected value</strong></p></th>
<th class="head"><p><strong>Squared error</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>1</p></td>
<td><p>-1</p></td>
<td><p>4</p></td>
</tr>
<tr class="row-odd"><td><p>2</p></td>
<td><p>-1</p></td>
<td><p>9</p></td>
</tr>
<tr class="row-even"><td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-odd"><td><p>3</p></td>
<td><p>2</p></td>
<td><p>1</p></td>
</tr>
<tr class="row-even"><td><p></p></td>
<td><p><strong>MSE:</strong></p></td>
<td><p>3.5</p></td>
</tr>
</tbody>
</table>
<p class="rubric" id="huber-loss">2. ‘Huber loss’</p>
<p>The squared error loss is more sensitive to outliers. Errors between -1 and 1 result in the same loss value
for both loss functions. But, larger errors (in other words: outliers) result in quadratically larger losses for
the Mean Squared Error, while for the Huber loss they only increase linearly.</p>
</div>
</div>
<div class="docutils">
<p>So, a loss function quantifies the total error of the model.
The process of adjusting the weights in such a way as to minimize the loss function is called ‘optimization’.
We will dive further into how optimization works in episode 3.
For now, it is enough to understand that during training the weights in the network are adjusted so that the loss decreases through the process of optimization.
This ultimately results in a low loss, and this, generally, implies predictions that are closer to the expected values.</p>
<div class="admonition-break instructor dropdown admonition" id="instructor-1">
<p class="admonition-title">BREAK</p>
<p>This is a good time for switching instructor and/or a break.</p>
</div>
</div>
</section>
<section id="what-sort-of-problems-can-deep-learning-solve">
<h5>What sort of problems can deep learning solve?<a class="headerlink" href="#what-sort-of-problems-can-deep-learning-solve" title="Link to this heading"></a></h5>
<ul class="simple">
<li><p>Pattern/object recognition</p></li>
<li><p>Segmenting images (or any data)</p></li>
<li><p>Translating between one set of data and another, for example natural language translation.</p></li>
<li><p>Generating new data that looks similar to the training data, often used to create synthetic datasets, art or even “deepfake” videos.</p>
<ul>
<li><p>This can also be used to give the illusion of enhancing data, for example making images look sharper, video look smoother or adding colour to black and white images. But beware of this, it is not an accurate recreation of the original data, but a recreation based on something statistically similar, effectively a digital imagination of what that data could look like.</p></li>
</ul>
</li>
</ul>
<section id="examples-of-deep-learning-in-research">
<h6>Examples of Deep Learning in Research<a class="headerlink" href="#examples-of-deep-learning-in-research" title="Link to this heading"></a></h6>
<p>Here are just a few examples of how deep learning has been applied to some research problems. Note: some of these articles might be behind paywalls.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://arxiv.org/abs/2003.09871">Detecting COVID-19 in chest X-ray images</a></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/1610.09460">Forecasting building energy load</a></p></li>
<li><p><a class="reference external" href="https://pubmed.ncbi.nlm.nih.gov/29039790/">Protein function prediction</a></p></li>
<li><p><a class="reference external" href="https://pubs.rsc.org/en/content/articlelanding/2018/sc/c7sc04934j">Simulating Chemical Processes</a></p></li>
<li><p><a class="reference external" href="https://heritagesciencejournal.springeropen.com/articles/10.1186/s40494-020-0355-x">Help to restore ancient murals</a></p></li>
</ul>
</section>
</section>
<section id="what-sort-of-problems-can-deep-learning-not-solve">
<h5>What sort of problems can deep learning not solve?<a class="headerlink" href="#what-sort-of-problems-can-deep-learning-not-solve" title="Link to this heading"></a></h5>
<ul class="simple">
<li><p>Any case where only a small amount of training data is available.</p></li>
<li><p>Tasks requiring an explanation of how the answer was arrived at.</p></li>
<li><p>Classifying things which are nothing like their training data.</p></li>
</ul>
</section>
<section id="what-sort-of-problems-can-deep-learning-solve-but-should-not-be-used-for">
<h5>What sort of problems can deep learning solve, but should not be used for?<a class="headerlink" href="#what-sort-of-problems-can-deep-learning-solve-but-should-not-be-used-for" title="Link to this heading"></a></h5>
<p>Deep learning needs a lot of computational power, for this reason it often relies on specialised hardware like <a class="reference external" href="https://glosario.carpentries.org/en/#gpu">graphical processing units (GPUs)</a>. Many computational problems can be solved using less intensive techniques, but could still technically be solved with deep learning.</p>
<p>The following could technically be achieved using deep learning, but it would probably be a very wasteful way to do it:</p>
<ul class="simple">
<li><p>Logic operations, such as computing totals, averages, ranges etc. (see <a class="reference external" href="https://joelgrus.com/2016/05/23/fizz-buzz-in-tensorflow">this example</a> applying deep learning to solve the <a class="reference external" href="https://en.wikipedia.org/wiki/Fizz_buzz">“FizzBuzz” problem</a> often used for programming interviews)</p></li>
<li><p>Modelling well defined systems, where the equations governing them are known and understood.</p></li>
<li><p>Basic computer vision tasks such as <a class="reference external" href="https://en.wikipedia.org/wiki/Edge_detection">edge detection</a>, decreasing colour depth or blurring an image.</p></li>
</ul>
<div class="admonition-deep-learning-problems-exercise exercise important admonition" id="exercise-3">
<p class="admonition-title">Deep Learning Problems Exercise</p>
<p>Which of the following would you apply deep learning to?</p>
<ol class="arabic simple">
<li><p>Recognising whether or not a picture contains a bird.</p></li>
<li><p>Calculating the median and interquartile range of a dataset.</p></li>
<li><p>Identifying MRI images of a rare disease when only one or two example images available for training.</p></li>
<li><p>Identifying people in pictures after being trained only on cats and dogs.</p></li>
<li><p>Translating English into French.</p></li>
</ol>
<div class="admonition-solution solution important dropdown admonition" id="solution-3">
<p class="admonition-title">Solution</p>
<ol class="arabic simple">
<li><p>and 5 are the sort of tasks often solved with deep learning.</p></li>
<li><p>is technically possible but solving this with deep learning would be extremely wasteful, you could do the same with much less computing power using traditional techniques.</p></li>
<li><p>will probably fail because there is not enough training data.</p></li>
<li><p>will fail because the deep learning system only knows what cats and dogs look like, it might accidentally classify the people as cats or dogs.</p></li>
</ol>
</div>
</div>
<div class="docutils">
<p class="rubric" id="how-much-data-do-you-need-for-deep-learning">How much data do you need for deep learning?</p>
<p>The rise of deep learning is partially due to the increased availability of very large datasets.
But how much data do you actually need to train a deep learning model?
Unfortunately, this question is not easy to answer. It depends, among other things, on the
complexity of the task (which you often do not know beforehand), the quality of the available dataset and the complexity of the network. For complex tasks with large neural networks, we often see that adding more data continues to improve performance. However, this is also not a generic truth: if the data you add is too similar to the data you already have, it will not give much new information to the neural network.</p>
<div class="admonition-what-if-i-do-not-have-enough-data callout admonition" id="callout-0">
<p class="admonition-title">What if I do not have enough data?</p>
<p>In case you have too little data available to train a complex network from scratch, it is sometimes possible to use a pretrained network that was trained on a similar problem. Another trick is data augmentation, where you expand the dataset with artificial data points that could be real. An example of this is mirroring images when trying to classify cats and dogs. An horizontally mirrored animal retains the label, but exposes a different view.</p>
</div>
</div>
</section>
</section>
<section id="deep-learning-workflow">
<h4>Deep learning workflow<a class="headerlink" href="#deep-learning-workflow" title="Link to this heading"></a></h4>
<p>To apply deep learning to a problem there are several steps we need to go through:</p>
<section id="formulate-outline-the-problem">
<h5>1. Formulate/Outline the problem<a class="headerlink" href="#formulate-outline-the-problem" title="Link to this heading"></a></h5>
<p>Firstly we must decide what it is we want our deep learning system to do. Is it going to classify some data into one of a few categories? For example if we have an image of some hand written characters, the neural network could classify which character it is being shown. Or is it going to perform a prediction? For example trying to predict what the price of something will be tomorrow given some historical data on pricing and current trends.</p>
</section>
<section id="identify-inputs-and-outputs">
<h5>2. Identify inputs and outputs<a class="headerlink" href="#identify-inputs-and-outputs" title="Link to this heading"></a></h5>
<p>Next we need to identify what the inputs and outputs of the neural network will be. This might require looking at our data and deciding what features of the data we can use as inputs. If the data is images then the inputs could be the individual pixels of the images.</p>
<p>For the outputs we will need to look at what we want to identify from the data. If we are performing a classification problem then typically we will have one output for each potential class.</p>
</section>
<section id="prepare-data">
<h5>3. Prepare data<a class="headerlink" href="#prepare-data" title="Link to this heading"></a></h5>
<p>Many datasets are not ready for immediate use in a neural network and will require some preparation. Neural networks can only really deal with numerical data, so any non-numerical data (for example words) will have to be somehow converted to numerical data.</p>
<p>Next we will need to divide the data into multiple sets.
One of these will be used by the training process and we will call it the training set.
Another will be used to evaluate the accuracy of the training and we will call that one the test set.
Sometimes we will also use a 3rd set known as a validation set to refine the model.</p>
</section>
<section id="choose-a-pre-trained-model-or-build-a-new-architecture-from-scratch">
<h5>4. Choose a pre-trained model or build a new architecture from scratch<a class="headerlink" href="#choose-a-pre-trained-model-or-build-a-new-architecture-from-scratch" title="Link to this heading"></a></h5>
<p>Often we can use an existing neural network instead of designing one from scratch. Training a network can take a lot of time and computational resources. There are a number of well publicised networks which have been shown to perform well at certain tasks, if you know of one which already does a similar task well then it makes sense to use one of these.</p>
<p>If instead we decide we do want to design our own network then we need to think about how many input neurons it will have, how many hidden layers and how many outputs, what types of layers we use (we will explore the different types later on). This will probably need some experimentation and we might have to try tweaking the network design a few times before we see acceptable results.</p>
</section>
<section id="choose-a-loss-function-and-optimizer">
<h5>5. Choose a loss function and optimizer<a class="headerlink" href="#choose-a-loss-function-and-optimizer" title="Link to this heading"></a></h5>
<p>The loss function tells the training algorithm how far away the predicted value was from the true value. We will look at choosing a loss function in more detail later on.</p>
<p>The optimizer is responsible for taking the output of the loss function and then applying some changes to the weights within the network. It is through this process that the “learning” (adjustment of the weights) is achieved.</p>
</section>
<section id="train-the-model">
<h5>6. Train the model<a class="headerlink" href="#train-the-model" title="Link to this heading"></a></h5>
<p>We can now go ahead and start training our neural network. We will probably keep doing this for a given number of iterations through our training dataset (referred to as <em>epochs</em>) or until the loss function gives a value under a certain threshold. The graph below show the loss against the number of <em>epochs</em>, generally the loss will go down with each <em>epoch</em>, but occasionally it will see a small rise.</p>
<p><img alt="" class="align-center" src="_images/training-0_to_1500.svg" /></p>
</section>
<section id="perform-a-prediction-classification">
<h5>7. Perform a Prediction/Classification<a class="headerlink" href="#perform-a-prediction-classification" title="Link to this heading"></a></h5>
<p>After training the network we can use it to perform predictions. This is the mode you would
use the network in after you have fully trained it to a satisfactory performance. Doing
predictions on a special hold-out set is used in the next step to measure the performance
of the network.</p>
</section>
<section id="measure-performance">
<h5>8. Measure Performance<a class="headerlink" href="#measure-performance" title="Link to this heading"></a></h5>
<p>Once we trained the network we want to measure its performance. To do this we use some additional data that was not part of the training, this is known as a test set. There are many different methods available for measuring performance and which one is best depends on the type of task we are attempting. These metrics are often published as an indication of how well our network performs.</p>
</section>
<section id="refine-the-model">
<h5>9. Refine the model<a class="headerlink" href="#refine-the-model" title="Link to this heading"></a></h5>
<p>We refine the model further. We can for example slightly change the architecture of the model, or change the number of nodes in a layer.
Hyperparameters are all the parameters set by the person configuring the machine learning instead of those learned by the algorithm itself.
The hyperparameters include the number of epochs or the parameters for the optimizer.
It might be necessary to adjust these and re-run the training many times before we are happy with the result, this is often done automatically and that is referred to as hyperparameter tuning.</p>
</section>
<section id="share-model">
<h5>10. Share Model<a class="headerlink" href="#share-model" title="Link to this heading"></a></h5>
<p>Now that we have a trained network that performs at a level we are happy with we can go and use it on real data to perform a prediction. At this point we might want to consider publishing a file with both the architecture of our network and the weights which it has learned (assuming we did not use a pre-trained network). This will allow others to use it as as pre-trained network for their own purposes and for them to (mostly) reproduce our result.</p>
<div class="admonition-deep-learning-workflow-exercise exercise important admonition" id="exercise-4">
<p class="admonition-title">Deep learning workflow exercise</p>
<p>Think about a problem you would like to use deep learning to solve.</p>
<ol class="arabic simple">
<li><p>What do you want a deep learning system to be able to tell you?</p></li>
<li><p>What data inputs and outputs will you have?</p></li>
<li><p>Do you think you will need to train the network or will a pre-trained network be suitable?</p></li>
<li><p>What data do you have to train with? What preparation will your data need? Consider both the data you are going to predict/classify from and the data you will use to train the network.</p></li>
</ol>
</div>
<div class="admonition-solution solution important dropdown admonition" id="solution-4">
<p class="admonition-title">Solution</p>
<p>Discuss your answers with the group or the person next to you.</p>
</div>
</section>
</section>
<section id="deep-learning-libraries">
<h4>Deep Learning Libraries<a class="headerlink" href="#deep-learning-libraries" title="Link to this heading"></a></h4>
<p>There are many software libraries available for deep learning including:</p>
<section id="tensorflow">
<h5>TensorFlow<a class="headerlink" href="#tensorflow" title="Link to this heading"></a></h5>
<p><a class="reference external" href="https://www.tensorflow.org/">TensorFlow</a> was developed by Google and is one of the older deep learning libraries, ported across many languages since it was first released to the public in 2015. It is very versatile and capable of much more than deep learning but as a result it often takes a lot more lines of code to write deep learning operations in TensorFlow than in other libraries. It offers (almost) seamless integration with GPU accelerators and Google’s own TPU (Tensor Processing Unit) chips that are built specially for machine learning.</p>
</section>
<section id="pytorch">
<h5>PyTorch<a class="headerlink" href="#pytorch" title="Link to this heading"></a></h5>
<p><a class="reference external" href="https://pytorch.org/">PyTorch</a> was developed by Facebook in 2016 and is a popular choice for deep learning applications. It was developed for Python from the start and feels a lot more “pythonic” than TensorFlow. Like TensorFlow it was designed to do more than just deep learning and offers some very low level interfaces. <a class="reference external" href="https://www.pytorchlightning.ai/">PyTorch Lightning</a> offers a higher level interface to PyTorch to set up experiments. Like TensorFlow it is also very easy to integrate PyTorch with a GPU. In many benchmarks it outperforms the other libraries.</p>
</section>
<section id="keras">
<h5>Keras<a class="headerlink" href="#keras" title="Link to this heading"></a></h5>
<p><a class="reference external" href="https://keras.io/">Keras</a> is designed to be easy to use and usually requires fewer lines of code than other libraries. We have chosen it for this lesson for that reason. Keras can actually work on top of TensorFlow (and several other libraries), hiding away the complexities of TensorFlow while still allowing you to make use of their features.</p>
<p>The processing speed of Keras is sometimes not as high as with other libraries and if you are going to move on to create very large networks using very large datasets then you might want to consider one of the other libraries. But for many applications, the difference will not be enough to worry about and the time you will save with simpler code will exceed what you will save by having the code run a little faster.</p>
<p>Keras also benefits from a very good set of <a class="reference external" href="https://keras.io/guides/">online documentation</a> and a large user community. You will find that most of the concepts from Keras translate very well across to the other libraries if you wish to learn them at a later date.</p>
</section>
<section id="installing-keras-and-other-dependencies">
<h5>Installing Keras and other dependencies<a class="headerlink" href="#installing-keras-and-other-dependencies" title="Link to this heading"></a></h5>
<p>Follow the <a class="reference internal" href="#learners/setup.md#packages"><span class="xref myst">setup instructions</span></a> to install Keras, Seaborn and scikit-learn.</p>
</section>
</section>
<section id="testing-keras-installation">
<h4>Testing Keras Installation<a class="headerlink" href="#testing-keras-installation" title="Link to this heading"></a></h4>
<p>Keras is available as a module within TensorFlow, as described in the <a class="reference internal" href="#learners/setup.md#packages"><span class="xref myst">setup instructions</span></a>.
Let’s therefore check whether you have a suitable version of TensorFlow installed.
Open up a new Jupyter notebook or interactive python console and run the following commands:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">tensorflow</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tensorflow</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-output notranslate"><div class="highlight"><pre><span></span><span class="go">2.17.0</span>
</pre></div>
</div>
<p>You should get a version number reported. At the time of writing 2.17.0 is the latest version.</p>
</section>
<section id="testing-seaborn-installation">
<h4>Testing Seaborn Installation<a class="headerlink" href="#testing-seaborn-installation" title="Link to this heading"></a></h4>
<p>Lets check you have a suitable version of seaborn installed.
In your Jupyter notebook or interactive python console run the following commands:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">seaborn</span>
<span class="nb">print</span><span class="p">(</span><span class="n">seaborn</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-output notranslate"><div class="highlight"><pre><span></span><span class="go">0.13.2</span>
</pre></div>
</div>
<p>You should get a version number reported. At the time of writing 0.13.2 is the latest version.</p>
</section>
<section id="testing-scikit-learn-installation">
<h4>Testing scikit-learn Installation<a class="headerlink" href="#testing-scikit-learn-installation" title="Link to this heading"></a></h4>
<p>Lets check you have a suitable version of scikit-learn installed.
In your Jupyter notebook or interactive python console run the following commands:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">sklearn</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sklearn</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-output notranslate"><div class="highlight"><pre><span></span><span class="go">1.5.1</span>
</pre></div>
</div>
<p>You should get a version number reported. At the time of writing 1.5.1 is the latest version.</p>
<div class="admonition-keypoints keypoints admonition" id="keypoints-0">
<p class="admonition-title">Keypoints</p>
<ul class="simple">
<li><p>Machine learning is the process where computers learn to recognise patterns of data.</p></li>
<li><p>Artificial neural networks are a machine learning technique based on a model inspired by groups of neurons in the brain.</p></li>
<li><p>Artificial neural networks can be trained on example data.</p></li>
<li><p>Deep learning is a machine learning technique based on using many artificial neurons arranged in layers.</p></li>
<li><p>Neural networks learn by minimizing a loss function.</p></li>
<li><p>Deep learning is well suited to classification and prediction problems such as image recognition.</p></li>
<li><p>To use deep learning effectively we need to go through a workflow of: defining the problem, identifying inputs and outputs, preparing data, choosing the type of network, choosing a loss function, training the model, refine the model, measuring performance before we can classify data.</p></li>
<li><p>Keras is a deep learning library that is easier to use than many of the alternatives such as TensorFlow and PyTorch.</p></li>
</ul>
</div>
</section>
</section>
<span id="document-2-keras"></span><section class="tex2jax_ignore mathjax_ignore" id="classification-by-a-neural-network-using-keras">
<h3>2. Classification by a neural network using Keras<a class="headerlink" href="#classification-by-a-neural-network-using-keras" title="Link to this heading"></a></h3>
<div class="admonition-questions questions admonition" id="questions-0">
<p class="admonition-title">Questions</p>
<ul class="simple">
<li><p>How do I compose a neural network using Keras?</p></li>
<li><p>How do I train this network on a dataset?</p></li>
<li><p>How do I get insight into learning process?</p></li>
<li><p>How do I measure the performance of the network?</p></li>
</ul>
</div>
<div class="admonition-objectives objectives admonition" id="objectives-0">
<p class="admonition-title">Objectives</p>
<ul class="simple">
<li><p>Use the deep learning workflow to structure the notebook</p></li>
<li><p>Explore the dataset using pandas and seaborn</p></li>
<li><p>Identify the inputs and outputs of a deep neural network.</p></li>
<li><p>Use one-hot encoding to prepare data for classification in Keras</p></li>
<li><p>Describe a fully connected layer</p></li>
<li><p>Implement a fully connected layer with Keras</p></li>
<li><p>Use Keras to train a small fully connected network on prepared data</p></li>
<li><p>Interpret the loss curve of the training process</p></li>
<li><p>Use a confusion matrix to measure the trained networks’ performance on a test set</p></li>
</ul>
</div>
<section id="introduction">
<h4>Introduction<a class="headerlink" href="#introduction" title="Link to this heading"></a></h4>
<p>In this episode we will learn how to create and train a neural network using Keras to solve a simple classification task.</p>
<p>The goal of this episode is to quickly get your hands dirty in actually defining and training a neural network,
without going into depth of how neural networks work on a technical or mathematical level.
We want you to go through the full deep learning workflow once before going into more details.</p>
<p>In fact, this is also what we would recommend you to do when working on real-world problems:
First quickly build a working pipeline, while taking shortcuts.
Then, slowly make the pipeline more advanced while you keep on evaluating the approach.</p>
<p>In <a class="reference internal" href="#episodes/3-monitor-the-model.md"><span class="xref myst">episode 3</span></a> we will expand on the concepts that are lightly introduced in this episode.
Some of these concepts include: how to monitor the training progress and how optimization works.</p>
<div class="admonition-instructor instructor dropdown admonition" id="instructor-0">
<p class="admonition-title">Instructor</p>
<p>It is good to stress the goal for this episode a few times, because learners will usually have a lot of questions like:
‘Why don’t we normalize our features’ or ‘Why do we choose Adam optimizer?’.
It can be a good idea to park some of these questions for discussion in episode 3 and 4.</p>
</div>
<p>As a reminder below are the steps of the deep learning workflow:</p>
<ol class="arabic simple">
<li><p>Formulate / Outline the problem</p></li>
<li><p>Identify inputs and outputs</p></li>
<li><p>Prepare data</p></li>
<li><p>Choose a pretrained model or start building architecture from scratch</p></li>
<li><p>Choose a loss function and optimizer</p></li>
<li><p>Train the model</p></li>
<li><p>Perform a Prediction/Classification</p></li>
<li><p>Measure performance</p></li>
<li><p>Refine the model</p></li>
<li><p>Save model</p></li>
</ol>
<p>In this episode we will focus on a minimal example for each of these steps, later episodes will build on this knowledge to go into greater depth for some or all of these steps.</p>
<div class="admonition-gpu-usage callout admonition" id="callout-0">
<p class="admonition-title">GPU usage</p>
<p>For this lesson having a <a class="reference external" href="https://glosario.carpentries.org/en/#gpu">GPU (graphics processing unit)</a> available is not needed.
We specifically use very small toy problems so that you do not need one.
However, Keras will use your GPU automatically when it is available.
Using a GPU becomes necessary when tackling larger datasets or complex problems which
require a more complex neural network.</p>
</div>
</section>
<section id="formulate-outline-the-problem-penguin-classification">
<h4>1. Formulate/outline the problem: penguin classification<a class="headerlink" href="#formulate-outline-the-problem-penguin-classification" title="Link to this heading"></a></h4>
<p>In this episode we will be using the <a class="reference external" href="https://zenodo.org/record/3960218">penguin dataset</a>. This is a dataset that was published in 2020 by Allison Horst and contains data on three different species of the penguins.</p>
<p>We will use the penguin dataset to train a neural network which can classify which species a
penguin belongs to, based on their physical characteristics.</p>
<div class="admonition-goal callout admonition" id="callout-1">
<p class="admonition-title">Goal</p>
<p>The goal is to predict a penguins’ species using the attributes available in this dataset.</p>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">palmerpenguins</span></code> data contains size measurements for three penguin species observed on three islands in the Palmer Archipelago, Antarctica.
The physical attributes measured are flipper length, beak length, beak width, body mass, and sex.</p>
<figure class="align-default" id="id3">
<img alt="Illustration of the three species of penguins found in the Palmer Archipelago, Antarctica: Chinstrap, Gentoo and Adele" src="_images/palmer_penguins.png" />
<figcaption>
<p><span class="caption-text">“Palmer Penguins”</span><a class="headerlink" href="#id3" title="Link to this image"></a></p>
</figcaption>
</figure>
<figure class="align-default" id="id4">
<img alt="Illustration of how the beak dimensions were measured. In the raw data, bill dimensions are recorded as &quot;culmen length&quot; and &quot;culmen depth&quot;. The culmen is the dorsal ridge atop the bill." src="_images/culmen_depth.png" />
<figcaption>
<p><span class="caption-text">“Culmen Depth”</span><a class="headerlink" href="#id4" title="Link to this image"></a></p>
</figcaption>
</figure>
<p>These data were collected from 2007 - 2009 by Dr. Kristen Gorman with the <a class="reference external" href="https://lternet.edu/site/palmer-antarctica-lter/">Palmer Station Long Term Ecological Research Program</a>, part of the <a class="reference external" href="https://lternet.edu/">US Long Term Ecological Research Network</a>. The data were imported directly from the <a class="reference external" href="https://edirepository.org/">Environmental Data Initiative</a> (EDI) Data Portal, and are available for use by CC0 license (“No Rights Reserved”) in accordance with the <a class="reference external" href="https://lternet.edu/data-access-policy/">Palmer Station Data Policy</a>.</p>
</section>
<section id="identify-inputs-and-outputs">
<h4>2. Identify inputs and outputs<a class="headerlink" href="#identify-inputs-and-outputs" title="Link to this heading"></a></h4>
<p>To identify the inputs and outputs that we will use to design the neural network we need to familiarize
ourselves with the dataset. This step is sometimes also called data exploration.</p>
<p>We will start by importing the <a class="reference external" href="https://seaborn.pydata.org/">Seaborn</a> library that will help us get the dataset and visualize it.
Seaborn is a powerful library with many visualizations. Keep in mind it requires the data to be in a
pandas dataframe, luckily the datasets available in seaborn are already in a pandas dataframe.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">seaborn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">sns</span>
</pre></div>
</div>
<p>We can load the penguin dataset using</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">penguins</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">load_dataset</span><span class="p">(</span><span class="s1">&#39;penguins&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>This will give you a pandas dataframe which contains the penguin data.</p>
<section id="inspecting-the-data">
<h5>Inspecting the data<a class="headerlink" href="#inspecting-the-data" title="Link to this heading"></a></h5>
<p>Using the pandas <code class="docutils literal notranslate"><span class="pre">head</span></code> function gives us a quick look at the data:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">penguins</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-right"><p></p></th>
<th class="head text-right"><p>species</p></th>
<th class="head text-right"><p>island</p></th>
<th class="head text-right"><p>bill_length_mm</p></th>
<th class="head text-right"><p>bill_depth_mm</p></th>
<th class="head text-right"><p>flipper_length_mm</p></th>
<th class="head text-right"><p>body_mass_g</p></th>
<th class="head text-right"><p>sex</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-right"><p>0</p></td>
<td class="text-right"><p>Adelie</p></td>
<td class="text-right"><p>Torgersen</p></td>
<td class="text-right"><p>39.1</p></td>
<td class="text-right"><p>18.7</p></td>
<td class="text-right"><p>181.0</p></td>
<td class="text-right"><p>3750.0</p></td>
<td class="text-right"><p>Male</p></td>
</tr>
<tr class="row-odd"><td class="text-right"><p>1</p></td>
<td class="text-right"><p>Adelie</p></td>
<td class="text-right"><p>Torgersen</p></td>
<td class="text-right"><p>39.5</p></td>
<td class="text-right"><p>17.4</p></td>
<td class="text-right"><p>186.0</p></td>
<td class="text-right"><p>3800.0</p></td>
<td class="text-right"><p>Female</p></td>
</tr>
<tr class="row-even"><td class="text-right"><p>2</p></td>
<td class="text-right"><p>Adelie</p></td>
<td class="text-right"><p>Torgersen</p></td>
<td class="text-right"><p>40.3</p></td>
<td class="text-right"><p>18.0</p></td>
<td class="text-right"><p>195.0</p></td>
<td class="text-right"><p>3250.0</p></td>
<td class="text-right"><p>Female</p></td>
</tr>
<tr class="row-odd"><td class="text-right"><p>3</p></td>
<td class="text-right"><p>Adelie</p></td>
<td class="text-right"><p>Torgersen</p></td>
<td class="text-right"><p>NaN</p></td>
<td class="text-right"><p>NaN</p></td>
<td class="text-right"><p>NaN</p></td>
<td class="text-right"><p>NaN</p></td>
<td class="text-right"><p>NaN</p></td>
</tr>
<tr class="row-even"><td class="text-right"><p>4</p></td>
<td class="text-right"><p>Adelie</p></td>
<td class="text-right"><p>Torgersen</p></td>
<td class="text-right"><p>36.7</p></td>
<td class="text-right"><p>19.3</p></td>
<td class="text-right"><p>193.0</p></td>
<td class="text-right"><p>3450.0</p></td>
<td class="text-right"><p>Female</p></td>
</tr>
</tbody>
</table>
<p>We can use all columns as features to predict the species of the penguin, except for the <code class="docutils literal notranslate"><span class="pre">species</span></code> column itself.</p>
<p>Let’s look at the shape of the dataset:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">penguins</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
<p>There are 344 samples and 7 columns (plus the index column), so 6 features.</p>
</section>
<section id="visualization">
<h5>Visualization<a class="headerlink" href="#visualization" title="Link to this heading"></a></h5>
<p>Looking at numbers like this usually does not give a very good intuition about the data we are
working with, so let us create a visualization.</p>
<section id="pair-plot">
<h6>Pair Plot<a class="headerlink" href="#pair-plot" title="Link to this heading"></a></h6>
<p>One nice visualization for datasets with relatively few attributes is the Pair Plot.
This can be created using <code class="docutils literal notranslate"><span class="pre">sns.pairplot(...)</span></code>. It shows a scatterplot of each attribute plotted against each of the other attributes.
By using the <code class="docutils literal notranslate"><span class="pre">hue='species'</span></code> setting for the pairplot the graphs on the diagonal are layered kernel density estimate plots for the different values of the <code class="docutils literal notranslate"><span class="pre">species</span></code> column.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">sns</span><span class="o">.</span><span class="n">pairplot</span><span class="p">(</span><span class="n">penguins</span><span class="p">,</span> <span class="n">hue</span><span class="o">=</span><span class="s2">&quot;species&quot;</span><span class="p">)</span>
</pre></div>
</div>
<figure class="align-default" id="id5">
<img alt="Grid of scatter plots and histograms comparing observed values of the four physicial attributes (features) measured in the penguins sampled. Scatter plots illustrate the distribution of values observed for each pair of features. On the diagonal, where one feature would be compared with itself, histograms are displayed that show the distribution of values observed for that feature, coloured according to the species of the individual sampled. The pair plot shows distinct but overlapping clusters of data points representing the different species, with no pair of features providing a clean separation of clusters on its own." src="_images/pairplot.png" />
<figcaption>
<p><span class="caption-text">“Pair Plot”</span><a class="headerlink" href="#id5" title="Link to this image"></a></p>
</figcaption>
</figure>
<div class="admonition-pairplot exercise important admonition" id="exercise-0">
<p class="admonition-title">Pairplot</p>
<p>Take a look at the pairplot we created. Consider the following questions:</p>
<ul class="simple">
<li><p>Is there any class that is easily distinguishable from the others?</p></li>
<li><p>Which combination of attributes shows the best separation for all 3 class labels at once?</p></li>
<li><p>(optional) Create a similar pairplot, but with <code class="docutils literal notranslate"><span class="pre">hue=&quot;sex&quot;</span></code>. Explain the patterns you see.
Which combination of features distinguishes the two sexes best?</p></li>
</ul>
<div class="admonition-solution solution important dropdown admonition" id="solution-0">
<p class="admonition-title">Solution</p>
<ul class="simple">
<li><p>The plots show that the green class, Gentoo is somewhat more easily distinguishable from the other two.</p></li>
<li><p>The other two seem to be separable by a combination of bill length and bill
depth (other combinations are also possible such as bill length and flipper length).</p></li>
</ul>
<p>Answer to optional question:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">sns</span><span class="o">.</span><span class="n">pairplot</span><span class="p">(</span><span class="n">penguins</span><span class="p">,</span> <span class="n">hue</span><span class="o">=</span><span class="s1">&#39;sex&#39;</span><span class="p">)</span>
</pre></div>
</div>
<figure class="align-default" id="id6">
<img alt="Grid of scatter plots and histograms comparing observed values of the four physicial attributes (features) measured in the penguins sampled, with data points coloured according to the sex of the individual sampled. The pair plot shows similarly-shaped distribution of values observed for each feature in male and female penguins, with the distribution of measurements for females skewed towards smaller values." src="_images/02_sex_pairplot.png" />
<figcaption>
<p><span class="caption-text">“Pair plot grouped by sex”</span><a class="headerlink" href="#id6" title="Link to this image"></a></p>
</figcaption>
</figure>
<p>You see that for each species females have smaller bills and flippers, as well as a smaller body mass.
You would need a combination of the species and the numerical features to successfully distinguish males from females.
The combination of <code class="docutils literal notranslate"><span class="pre">bill_depth_mm</span></code> and <code class="docutils literal notranslate"><span class="pre">body_mass_g</span></code> gives the best separation.</p>
</div>
</div>
<div class="docutils">
<p class="rubric" id="input-and-output-selection">Input and Output Selection</p>
<p>Now that we have familiarized ourselves with the dataset we can select the data attributes to use
as input for the neural network and the target that we want to predict.</p>
<p>In the rest of this episode we will use the <code class="docutils literal notranslate"><span class="pre">bill_length_mm</span></code>, <code class="docutils literal notranslate"><span class="pre">bill_depth_mm</span></code>, <code class="docutils literal notranslate"><span class="pre">flipper_length_mm</span></code>, <code class="docutils literal notranslate"><span class="pre">body_mass_g</span></code> attributes.
The target for the classification task will be the <code class="docutils literal notranslate"><span class="pre">species</span></code>.</p>
<div class="admonition-data-exploration callout admonition" id="callout-2">
<p class="admonition-title">Data Exploration</p>
<p>Exploring the data is an important step to familiarize yourself with the problem and to help you
determine the relevant inputs and outputs.</p>
</div>
</div>
</section>
</section>
</section>
<section id="prepare-data">
<h4>3. Prepare data<a class="headerlink" href="#prepare-data" title="Link to this heading"></a></h4>
<p>The input data and target data are not yet in a format that is suitable to use for training a neural network.</p>
<p>For now we will only use the numerical features <code class="docutils literal notranslate"><span class="pre">bill_length_mm</span></code>, <code class="docutils literal notranslate"><span class="pre">bill_depth_mm</span></code>, <code class="docutils literal notranslate"><span class="pre">flipper_length_mm</span></code>, <code class="docutils literal notranslate"><span class="pre">body_mass_g</span></code> only,
so let’s drop the categorical columns:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Drop categorical columns</span>
<span class="n">penguins_filtered</span> <span class="o">=</span> <span class="n">penguins</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;island&#39;</span><span class="p">,</span> <span class="s1">&#39;sex&#39;</span><span class="p">])</span>
</pre></div>
</div>
<section id="clean-missing-values">
<h5>Clean missing values<a class="headerlink" href="#clean-missing-values" title="Link to this heading"></a></h5>
<p>During the exploration phase you may have noticed that some rows in the dataset have missing (NaN)
values, leaving such values in the input data will ruin the training, so we need to deal with them.
There are many ways to deal with missing values, but for now we will just remove the offending rows by adding a call to <code class="docutils literal notranslate"><span class="pre">dropna()</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Drop the rows that have NaN values in them</span>
<span class="n">penguins_filtered</span> <span class="o">=</span> <span class="n">penguins_filtered</span><span class="o">.</span><span class="n">dropna</span><span class="p">()</span>
</pre></div>
</div>
<p>Finally, we select only the features</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Extract columns corresponding to features</span>
<span class="n">features</span> <span class="o">=</span> <span class="n">penguins_filtered</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;species&#39;</span><span class="p">])</span>
</pre></div>
</div>
</section>
<section id="prepare-target-data-for-training">
<h5>Prepare target data for training<a class="headerlink" href="#prepare-target-data-for-training" title="Link to this heading"></a></h5>
<p>Second, the target data is also in a format that cannot be used in training.
A neural network can only take numerical inputs and outputs, and learns by
calculating how “far away” the species predicted by the neural network is
from the true species.</p>
<p>When the target is a string category column as we have here, we need to transform this column into a numerical format first.
Again, there are many ways to do this. We will be using the one-hot encoding.
This encoding creates multiple columns, as many as there are unique values, and
puts a 1 in the column with the corresponding correct class, and 0’s in
the other columns.
For instance, for a penguin of the Adelie species the one-hot encoding would be 1 0 0.</p>
<p>Fortunately, Pandas is able to generate this encoding for us.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>

<span class="n">target</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">get_dummies</span><span class="p">(</span><span class="n">penguins_filtered</span><span class="p">[</span><span class="s1">&#39;species&#39;</span><span class="p">])</span>
<span class="n">target</span><span class="o">.</span><span class="n">head</span><span class="p">()</span> <span class="c1"># print out the top 5 to see what it looks like.</span>
</pre></div>
</div>
<div class="admonition-one-hot-encoding exercise important admonition" id="exercise-1">
<p class="admonition-title">One-hot encoding</p>
<p>How many output neurons will our network have now that we one-hot encoded the target class?</p>
<ul class="simple">
<li><p>A: 1</p></li>
<li><p>B: 2</p></li>
<li><p>C: 3</p></li>
</ul>
</div>
<div class="admonition-solution solution important dropdown admonition" id="solution-1">
<p class="admonition-title">Solution</p>
<p>C: 3, one for each output variable class</p>
</div>
</section>
<section id="split-data-into-training-and-test-set">
<h5>Split data into training and test set<a class="headerlink" href="#split-data-into-training-and-test-set" title="Link to this heading"></a></h5>
<p>Finally, we will split the dataset into a training set and a test set.
As the names imply we will use the training set to train the neural network,
while the test set is kept separate.
We will use the test set to assess the performance of the trained neural network
on unseen samples.
In many cases a validation set is also kept separate from the training and test sets (i.e. the dataset is split into 3 parts).
This validation set is then used to select the values of the parameters of the neural network and the training methods.
For this episode we will keep it at just a training and test set however.</p>
<p>To split the cleaned dataset into a training and test set we will use a very convenient
function from sklearn called <code class="docutils literal notranslate"><span class="pre">train_test_split</span></code>.</p>
<p>This function takes a number of parameters which are extensively explained in <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html">the scikit-learn documentation</a> :</p>
<ul class="simple">
<li><p>The first two parameters are the dataset (in our case <code class="docutils literal notranslate"><span class="pre">features</span></code>) and the corresponding targets (i.e. defined as target).</p></li>
<li><p>Next is the named parameter <code class="docutils literal notranslate"><span class="pre">test_size</span></code> this is the fraction of the dataset that is
used for testing, in this case <code class="docutils literal notranslate"><span class="pre">0.2</span></code> means 20% of the data will be used for testing.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">random_state</span></code> controls the shuffling of the dataset, setting this value will reproduce
the same results (assuming you give the same integer) every time it is called.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">shuffle</span></code> which can be either <code class="docutils literal notranslate"><span class="pre">True</span></code> or <code class="docutils literal notranslate"><span class="pre">False</span></code>, it controls whether the order of the rows of the dataset is shuffled before splitting. It defaults to <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">stratify</span></code> is a more advanced parameter that controls how the split is done. By setting it to <code class="docutils literal notranslate"><span class="pre">target</span></code> the train and test sets the function will return will have roughly the same proportions (with regards to the number of penguins of a certain species) as the dataset.</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">train_test_split</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">stratify</span><span class="o">=</span><span class="n">target</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition-importance-of-using-the-same-train-test-split callout admonition" id="callout-3">
<p class="admonition-title">Importance of using the same train-test split</p>
<p>By setting <code class="docutils literal notranslate"><span class="pre">random_state=0</span></code> we ensure that everyone has the same train-test split.
When doing machine learning and deep learning it is crucial that you use the same train and test dataset for different experiments.
Comparing evaluation metrics between experiments run on different data splits is meaningless,
because the accuracy of a model depends on the data used to train and test it.</p>
</div>
<div class="admonition-break instructor dropdown admonition" id="instructor-1">
<p class="admonition-title">BREAK</p>
<p>This is a good time for switching instructor and/or a break.</p>
</div>
</section>
</section>
<section id="build-an-architecture-from-scratch">
<h4>4. Build an architecture from scratch<a class="headerlink" href="#build-an-architecture-from-scratch" title="Link to this heading"></a></h4>
<section id="keras-for-neural-networks">
<h5>Keras for neural networks<a class="headerlink" href="#keras-for-neural-networks" title="Link to this heading"></a></h5>
<p>Keras is a machine learning framework with ease of use as one of its main features.
It is part of the tensorflow python package and can be imported using <code class="docutils literal notranslate"><span class="pre">from</span> <span class="pre">tensorflow</span> <span class="pre">import</span> <span class="pre">keras</span></code>.</p>
<p>Keras includes functions, classes and definitions to define deep learning models, cost functions and optimizers (optimizers are used to train a model).</p>
<p>Before we move on to the next section of the workflow we need to make sure we have Keras imported.
We do this as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">tensorflow</span><span class="w"> </span><span class="kn">import</span> <span class="n">keras</span>
</pre></div>
</div>
<p>For this episode it is useful if everyone gets the same results from their training.
Keras uses a random number generator at certain points during its execution.
Therefore we will need to set two random seeds, one for numpy and one for tensorflow:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">numpy.random</span><span class="w"> </span><span class="kn">import</span> <span class="n">seed</span>
<span class="n">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">keras</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">set_random_seed</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition-when-to-use-random-seeds callout admonition" id="callout-4">
<p class="admonition-title">When to use random seeds?</p>
<p>We use a random seed here to ensure that we get the same results every time we run this code.
This makes our results reproducible and allows us to better compare results between different experiments.</p>
<p>Please note that even though you have selected a random seed, this seed is used to generate a
<strong>different</strong> random number every time you execute a Jupyter cell.
So, to get truly replicable deep learning pipelines you need to run the notebook from start to end in one go.</p>
</div>
</section>
<section id="build-a-neural-network-from-scratch">
<h5>Build a neural network from scratch<a class="headerlink" href="#build-a-neural-network-from-scratch" title="Link to this heading"></a></h5>
<p>We will now build a simple neural network from scratch using Keras.</p>
<p>With Keras you compose a neural network by creating layers and linking them
together. For now we will only use one type of layer called a fully connected
or Dense layer. In Keras this is defined by the <code class="docutils literal notranslate"><span class="pre">keras.layers.Dense</span></code> class.</p>
<p>A dense layer has a number of neurons, which is a parameter you can choose when
you create the layer.
When connecting the layer to its input and output layers every neuron in the dense
layer gets an edge (i.e. connection) to <em><strong>all</strong></em> of the input neurons and <em><strong>all</strong></em> of the output neurons.
The hidden layer in the image in the introduction of this episode is a Dense layer.</p>
<p>The input in Keras also gets special treatment, Keras automatically calculates the number of inputs
and outputs a layer needs and therefore how many edges need to be created.
This means we need to inform Keras how big our input is going to be. We do this by instantiating a <code class="docutils literal notranslate"><span class="pre">keras.Input</span></code> class and tell it how big our input is, thus the number of columns it contains.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">inputs</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],))</span>
</pre></div>
</div>
<p>We store a reference to this input class in a variable so we can pass it to the creation of
our hidden layer.
Creating the hidden layer can then be done as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">hidden_layer</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">)(</span><span class="n">inputs</span><span class="p">)</span>
</pre></div>
</div>
<p>The instantiation here has 2 parameters and a seemingly strange combination of parentheses, so
let us take a closer look.
The first parameter <code class="docutils literal notranslate"><span class="pre">10</span></code> is the number of neurons we want in this layer, this is one of the
hyperparameters of our system and needs to be chosen carefully. We will get back to this in the section
on refining the model.</p>
<p>The second parameter is the activation function to use. We choose <code class="docutils literal notranslate"><span class="pre">relu</span></code> which returns 0
for inputs that are 0 and below and the identity function (returning the same value)
for inputs above 0.
This is a commonly used activation function in deep neural networks that is proven to work well.</p>
<p>Next we see an extra set of parenthenses with inputs in them. This means that after creating an
instance of the Dense layer we call it as if it was a function.
This tells the Dense layer to connect the layer passed as a parameter, in this case the inputs.</p>
<p>Finally we store a reference in the <code class="docutils literal notranslate"><span class="pre">hidden_layer</span></code> variable so we can pass it to the output layer in a minute.</p>
<p>Now we create another layer that will be our output layer.
Again we use a Dense layer and so the call is very similar to the previous one.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">output_layer</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;softmax&quot;</span><span class="p">)(</span><span class="n">hidden_layer</span><span class="p">)</span>
</pre></div>
</div>
<p>Because we chose the one-hot encoding, we use three neurons for the output layer.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">softmax</span></code> activation ensures that the three output neurons produce values in the range
(0, 1) and they sum to 1.
We can interpret this as a kind of ‘probability’ that the sample belongs to a certain
species.</p>
<p>Now that we have defined the layers of our neural network we can combine them into
a Keras model which facilitates training the network.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">output_layer</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
<p>The model summary here can show you some information about the neural network we have defined.</p>
<div class="admonition-trainable-and-non-trainable-parameters callout admonition" id="callout-5">
<p class="admonition-title">Trainable and non-trainable parameters</p>
<p>Keras distinguishes between two types of weights, namely:</p>
<ul class="simple">
<li><p>trainable parameters: these are weights of the neurons that are modified when we train the model in order to minimize our loss function (we will learn about loss functions shortly!).</p></li>
<li><p>non-trainable parameters: these are weights of the neurons that are not changed when we train the model. These could be for many reasons - using a pre-trained model, choice of a particular filter for a convolutional neural network, and statistical weights for batch normalization are some examples.</p></li>
</ul>
<p>If these reasons are not clear right away, don’t worry! In later episodes of this course, we will touch upon a couple of these concepts.</p>
</div>
<div class="admonition-instructor instructor dropdown admonition" id="instructor-2">
<p class="admonition-title">Instructor</p>
<p>For optional question 3 in the challenge below named ‘Visualizing the model’, the goal is to visualize the network. It supplements the textual explanation of output from <code class="docutils literal notranslate"><span class="pre">model.summary()</span></code>.
You could choose to show and discuss the resulting visualization to the learners, so that learners who did not finish the optional exercise can also learn from the visualization of the model.</p>
</div>
<div class="admonition-create-the-neural-network exercise important admonition" id="exercise-2">
<p class="admonition-title">Create the neural network</p>
<p>With the code snippets above, we defined a Keras model with 1 hidden layer with
10 neurons and an output layer with 3 neurons.</p>
<ol class="arabic simple">
<li><p>How many parameters does the resulting model have?</p></li>
<li><p>What happens to the number of parameters if we increase or decrease the number of neurons
in the hidden layer?</p></li>
</ol>
<p class="rubric" id="optional-visualizing-the-model">(optional) Visualizing the model</p>
<p>Optionally, you can also visualize the same information as <code class="docutils literal notranslate"><span class="pre">model.summary()</span></code> in graph form.
This step requires the command-line tool <code class="docutils literal notranslate"><span class="pre">dot</span></code> from Graphviz installed, you installed it by following the setup instructions.
You can check that the installation was successful by executing <code class="docutils literal notranslate"><span class="pre">dot</span> <span class="pre">-V</span></code> in the command line. You should get something
as follows:</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span>dot<span class="w"> </span>-V
dot<span class="w"> </span>-<span class="w"> </span>graphviz<span class="w"> </span>version<span class="w"> </span><span class="m">2</span>.43.0<span class="w"> </span><span class="o">(</span><span class="m">0</span><span class="o">)</span>
</pre></div>
</div>
<ol class="arabic simple" start="3">
<li><p>(optional) Provided you have <code class="docutils literal notranslate"><span class="pre">dot</span></code> installed, execute the <code class="docutils literal notranslate"><span class="pre">plot_model</span></code> function
as shown below.</p></li>
</ol>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">keras</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">plot_model</span><span class="p">(</span>
    <span class="n">model</span><span class="p">,</span>
    <span class="n">show_shapes</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">show_layer_names</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">show_layer_activations</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">show_trainable</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>
</pre></div>
</div>
<p class="rubric" id="optional-keras-sequential-vs-functional-api">(optional) Keras Sequential vs Functional API</p>
<p>So far we have used the <a class="reference external" href="https://keras.io/guides/functional_api/">Functional API</a> of Keras.
You can also implement neural networks using <a class="reference external" href="https://keras.io/guides/sequential_model/">the Sequential model</a>.
As you can read in the documentation, the Sequential model is appropriate for <strong>a plain stack of layers</strong>
where each layer has <strong>exactly one input tensor and one output tensor</strong>.</p>
<ol class="arabic simple" start="4">
<li><p>(optional) Use the Sequential model to implement the same network</p></li>
</ol>
<div class="admonition-solution solution important dropdown admonition" id="solution-2">
<p class="admonition-title">Solution</p>
<p>Have a look at the output of <code class="docutils literal notranslate"><span class="pre">model.summary()</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-output notranslate"><div class="highlight"><pre><span></span><span class="go">Model: &quot;functional&quot;</span>

<span class="go">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓</span>
<span class="go">┃ Layer (type)               ┃ Output Shape   ┃    Param # ┃</span>
<span class="go">┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩</span>
<span class="go">│ input_layer (InputLayer)   │ (None, 4)      │          0 │</span>
<span class="go">├────────────────────────────┼────────────────┼────────────┤</span>
<span class="go">│ dense (Dense)              │ (None, 10)     │         50 │</span>
<span class="go">├────────────────────────────┼────────────────┼────────────┤</span>
<span class="go">│ dense_1 (Dense)            │ (None, 3)      │         33 │</span>
<span class="go">└────────────────────────────┴────────────────┴────────────┘</span>

<span class="go"> Total params: 83 (332.00 B)</span>

<span class="go"> Trainable params: 83 (332.00 B)</span>

<span class="go"> Non-trainable params: 0 (0.00 B)</span>
</pre></div>
</div>
<p>The model has 83 trainable parameters. Each of the 10 neurons in the in the <code class="docutils literal notranslate"><span class="pre">dense</span></code> hidden layer is connected to each of
the 4 inputs in the input layer resulting in 40 weights that can be trained. The 10 neurons in the hidden layer are also
connected to each of the 3 outputs in the <code class="docutils literal notranslate"><span class="pre">dense_1</span></code> output layer, resulting in a further 30 weights that can be trained.
By default <code class="docutils literal notranslate"><span class="pre">Dense</span></code> layers in Keras also contain 1 bias term for each neuron, resulting in a further 10 bias values for the
hidden layer and 3 bias terms for the output layer. <code class="docutils literal notranslate"><span class="pre">40+30+10+3=83</span></code> trainable parameters.</p>
<p>The value <code class="docutils literal notranslate"><span class="pre">(332.00</span> <span class="pre">B)</span></code> next to it describes the memory footprint for model weights and this depends on their data type.
Take a look at what <code class="docutils literal notranslate"><span class="pre">model.dtype</span></code> is.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-output notranslate"><div class="highlight"><pre><span></span><span class="go">float32</span>
</pre></div>
</div>
<p>The model weights are represented using <code class="docutils literal notranslate"><span class="pre">float32</span></code> data type, which consumes 32 bits or 4 bytes for each weight.
We have 83 parameters, and therefore in total, the model requires <code class="docutils literal notranslate"><span class="pre">83*4=332</span></code> bytes of memory to load
into the computer’s memory.</p>
<p>If you increase the number of neurons in the hidden layer the number of
trainable parameters in both the hidden and output layer increases or
decreases in accordance with the number of neurons added.
Each extra neuron has 4 weights connected to the input layer, 1 bias term, and 3 weights connected to the output layer.
So in total 8 extra parameters.</p>
<p><em>The name in quotes within the string <code class="docutils literal notranslate"><span class="pre">Model:</span> <span class="pre">&quot;functional&quot;</span></code> may be different in your view; this detail is not important.</em></p>
<p class="rubric" id="id1">(optional) Visualizing the model</p>
<ol class="arabic simple" start="3">
<li><p>Upon executing the <code class="docutils literal notranslate"><span class="pre">plot_model</span></code> function, you should see the following image.</p></li>
</ol>
<figure class="align-default" id="id7">
<img alt="A directed graph showing the three layers of the neural network connected by arrows. First layer is of type InputLayer. Second layer is of type Dense with a relu activation. The third layer is also of type Dense, with a softmax activation. The input and output shapes of every layer are also mentioned. Only the second and third layers contain trainable parameters." src="_images/02_plot_model.png" />
<figcaption>
<p><span class="caption-text">“Output of keras.utils.plot_model() function”</span><a class="headerlink" href="#id7" title="Link to this image"></a></p>
</figcaption>
</figure>
<p class="rubric" id="id2">(optional) Keras Sequential vs Functional API</p>
<ol class="arabic simple" start="4">
<li><p>This implements the same model using the Sequential API:</p></li>
</ol>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
    <span class="p">[</span>
        <span class="n">keras</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],)),</span>
        <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">),</span>
        <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;softmax&quot;</span><span class="p">),</span>
    <span class="p">]</span>
<span class="p">)</span>
</pre></div>
</div>
<p>We will use the Functional API for the remainder of this course, since it is more flexible and more explicit.</p>
</div>
</div>
<div class="docutils">
<div class="admonition-how-to-choose-an-architecture callout admonition" id="callout-6">
<p class="admonition-title">How to choose an architecture?</p>
<p>Even for this small neural network, we had to make a choice on the number of hidden neurons.
Other choices to be made are the number of layers and type of layers (as we will see later).
You might wonder how you should make these architectural choices.
Unfortunately, there are no clear rules to follow here, and it often boils down to a lot of
trial and error. However, it is recommended to look what others have done with similar datasets and problems.
Another best practice is to start with a relatively simple architecture. Once running start to add layers and tweak the network to see if performance increases.</p>
</div>
</div>
</section>
<section id="choose-a-pretrained-model">
<h5>Choose a pretrained model<a class="headerlink" href="#choose-a-pretrained-model" title="Link to this heading"></a></h5>
<p>If your data and problem is very similar to what others have done, you can often use a <em>pretrained network</em>.
Even if your problem is different, but the data type is common (for example images), you can use a pretrained network and finetune it for your problem.
A large number of openly available pretrained networks can be found on <a class="reference external" href="https://huggingface.co/models">Hugging Face</a> (especially LLMs), <a class="reference external" href="https://monai.io/">MONAI</a> (medical imaging), the <a class="reference external" href="https://modelzoo.co/">Model Zoo</a>, <a class="reference external" href="https://pytorch.org/hub/">pytorch hub</a> or <a class="reference external" href="https://www.tensorflow.org/hub/">tensorflow hub</a>.</p>
<p>We will cover the concept of Transfer Learning in <a class="reference internal" href="#./5-transfer-learning.html"><span class="xref myst">episode 5</span></a></p>
</section>
</section>
<section id="choose-a-loss-function-and-optimizer">
<h4>5. Choose a loss function and optimizer<a class="headerlink" href="#choose-a-loss-function-and-optimizer" title="Link to this heading"></a></h4>
<p>We have now designed a neural network that in theory we should be able to
train to classify Penguins.
However, we first need to select an appropriate loss
function that we will use during training.
This loss function tells the training algorithm how wrong, or how ‘far away’ from the true
value the predicted value is.</p>
<p>For the one-hot encoding that we selected earlier a suitable loss function is the Categorical Crossentropy loss.
In Keras this is implemented in the <code class="docutils literal notranslate"><span class="pre">keras.losses.CategoricalCrossentropy</span></code> class.
This loss function works well in combination with the <code class="docutils literal notranslate"><span class="pre">softmax</span></code> activation function
we chose earlier.
The Categorical Crossentropy works by comparing the probabilities that the
neural network predicts with ‘true’ probabilities that we generated using the one-hot encoding.
This is a measure for how close the distribution of the three neural network outputs corresponds to the distribution of the three values in the one-hot encoding.
It is lower if the distributions are more similar.</p>
<p>For more information on the available loss functions in Keras you can check the
<a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/keras/losses">documentation</a>.</p>
<p>Next we need to choose which optimizer to use and, if this optimizer has parameters, what values
to use for those. Furthermore, we need to specify how many times to show the training samples to the optimizer.</p>
<p>Once more, Keras gives us plenty of choices all of which have their own pros and cons,
but for now let us go with the widely used <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam">Adam optimizer</a>.
Adam has a number of parameters, but the default values work well for most problems.
So we will use it with its default parameters.</p>
<p>Combining this with the loss function we decided on earlier we can now compile the
model using <code class="docutils literal notranslate"><span class="pre">model.compile</span></code>.
Compiling the model prepares it to start the training.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;adam&#39;</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="n">keras</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">CategoricalCrossentropy</span><span class="p">())</span>
</pre></div>
</div>
</section>
<section id="train-model">
<h4>6. Train model<a class="headerlink" href="#train-model" title="Link to this heading"></a></h4>
<p>We are now ready to train the model.</p>
<p>Training the model is done using the <code class="docutils literal notranslate"><span class="pre">fit</span></code> method, it takes the input data and
target data as inputs and it has several other parameters for certain options
of the training.
Here we only set a different number of <code class="docutils literal notranslate"><span class="pre">epochs</span></code>.
One training epoch means that every sample in the training data has been shown
to the neural network and used to update its parameters.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
</pre></div>
</div>
<p>The fit method returns a history object that has a history attribute with the training loss and
potentially other metrics per training epoch.
It can be very insightful to plot the training loss to see how the training progresses.
Using seaborn we can do this as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">sns</span><span class="o">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">history</span><span class="o">.</span><span class="n">epoch</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;loss&#39;</span><span class="p">])</span>
</pre></div>
</div>
<figure class="align-default" id="id8">
<img alt="Training loss curve of the neural network training which depicts exponential decrease in loss before a plateau from ~10 epochs" src="_images/02_training_curve.png" />
<figcaption>
<p><span class="caption-text">“Training Curve”</span><a class="headerlink" href="#id8" title="Link to this image"></a></p>
</figcaption>
</figure>
<div class="admonition-i-get-a-different-plot callout admonition" id="callout-7">
<p class="admonition-title">I get a different plot</p>
<p>It could be that you get a different plot than the one shown here.
This could be because of a different random initialization of the model or a different split of the data.
This difference can be avoided by setting <code class="docutils literal notranslate"><span class="pre">random_state</span></code> and random seed in the same way like we discussed
in <a class="reference internal" href="#when-to-use-random-seeds"><span class="xref myst">When to use random seeds?</span></a>.</p>
</div>
<p>This plot can be used to identify whether the training is well configured or whether there
are problems that need to be addressed.</p>
<div class="admonition-the-training-curve exercise important admonition" id="exercise-3">
<p class="admonition-title">The Training Curve</p>
<p>Looking at the training curve we have just made.</p>
<ol class="arabic simple">
<li><p>How does the training progress?</p>
<ul class="simple">
<li><p>Does the training loss increase or decrease?</p></li>
<li><p>Does it change quickly or slowly?</p></li>
<li><p>Does the graph look very jittery?</p></li>
</ul>
</li>
<li><p>Do you think the resulting trained network will work well on the test set?</p></li>
</ol>
<p>When the training process does not go well:</p>
<ol class="arabic simple" start="3">
<li><p>(optional) Something went wrong here during training. What could be the problem, and how do you see that in the training curve?
Also compare the range on the y-axis with the previous training curve.
<figure class="align-default" id="id9">
<img alt="Very jittery training curve with the loss value jumping back and forth between 2 and 4. The range of the y-axis is from 2 to 4, whereas in the previous training curve it was from 0 to 2. The loss seems to decrease a litle bit, but not as much as compared to the previous plot where it dropped to almost 0. The minimum loss in the end is somewhere around 2." src="_images/02_bad_training_history_1.png" />
<figcaption>
<p><span class="caption-text">“Training Curve Gone Wrong”</span><a class="headerlink" href="#id9" title="Link to this image"></a></p>
</figcaption>
</figure>
</p></li>
</ol>
</div>
<div class="admonition-solution solution important dropdown admonition" id="solution-3">
<p class="admonition-title">Solution</p>
<ol class="arabic simple">
<li><p>The training loss decreases quickly. It drops in a smooth line with little jitter.
This is ideal for a training curve.</p></li>
<li><p>The results of the training give very little information on its performance on a test set.
You should be careful not to use it as an indication of a well trained network.</p></li>
<li><p>(optional) The loss does not go down at all, or only very slightly. This means that the model is not learning anything.
It could be that something went wrong in the data preparation (for example the labels are not attached to the right features).
In addition, the graph is very jittery. This means that for every update step,
the weights in the network are updated in such a way that the loss sometimes increases a lot and sometimes decreases a lot.
This could indicate that the weights are updated too much at every learning step and you need a smaller learning rate
(we will go into more details on this in the next episode).
Or there is a high variation in the data, leading the optimizer to change the weights in different directions at every learning step.
This could be addressed by presenting more data at every learning step (or in other words increasing the batch size).
In this case the graph was created by training on nonsense data, so this a training curve for a problem where nothing can be learned really.</p></li>
</ol>
<p>We will take a closer look at training curves in the next episode. Some of the concepts touched upon here will also be further explained there.</p>
</div>
</section>
<section id="perform-a-prediction-classification">
<h4>7. Perform a prediction/classification<a class="headerlink" href="#perform-a-prediction-classification" title="Link to this heading"></a></h4>
<p>Now that we have a trained neural network, we can use it to predict new samples
of penguin using the <code class="docutils literal notranslate"><span class="pre">predict</span></code> function.</p>
<p>We will use the neural network to predict the species of the test set
using the <code class="docutils literal notranslate"><span class="pre">predict</span></code> function.
We will be using this prediction in the next step to measure the performance of our
trained network.
This will return a <code class="docutils literal notranslate"><span class="pre">numpy</span></code> matrix, which we convert
to a pandas dataframe to easily see the labels.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">prediction</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">target</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
<span class="n">prediction</span>
</pre></div>
</div>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-right"><p></p></th>
<th class="head text-right"><p></p></th>
<th class="head text-right"><p></p></th>
<th class="head text-right"><p></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-right"><p>0</p></td>
<td class="text-right"><p>0.304484</p></td>
<td class="text-right"><p>0.192893</p></td>
<td class="text-right"><p>0.502623</p></td>
</tr>
<tr class="row-odd"><td class="text-right"><p>1</p></td>
<td class="text-right"><p>0.527107</p></td>
<td class="text-right"><p>0.095888</p></td>
<td class="text-right"><p>0.377005</p></td>
</tr>
<tr class="row-even"><td class="text-right"><p>2</p></td>
<td class="text-right"><p>0.373989</p></td>
<td class="text-right"><p>0.195604</p></td>
<td class="text-right"><p>0.430406</p></td>
</tr>
<tr class="row-odd"><td class="text-right"><p>3</p></td>
<td class="text-right"><p>0.493643</p></td>
<td class="text-right"><p>0.154104</p></td>
<td class="text-right"><p>0.352253</p></td>
</tr>
<tr class="row-even"><td class="text-right"><p>4</p></td>
<td class="text-right"><p>0.309051</p></td>
<td class="text-right"><p>0.308646</p></td>
<td class="text-right"><p>0.382303</p></td>
</tr>
<tr class="row-odd"><td class="text-right"><p>…</p></td>
<td class="text-right"><p>…</p></td>
<td class="text-right"><p>…</p></td>
<td class="text-right"><p>…</p></td>
</tr>
<tr class="row-even"><td class="text-right"><p>64</p></td>
<td class="text-right"><p>0.406074</p></td>
<td class="text-right"><p>0.191430</p></td>
<td class="text-right"><p>0.402496</p></td>
</tr>
<tr class="row-odd"><td class="text-right"><p>65</p></td>
<td class="text-right"><p>0.645621</p></td>
<td class="text-right"><p>0.077174</p></td>
<td class="text-right"><p>0.277204</p></td>
</tr>
<tr class="row-even"><td class="text-right"><p>66</p></td>
<td class="text-right"><p>0.356284</p></td>
<td class="text-right"><p>0.185958</p></td>
<td class="text-right"><p>0.457758</p></td>
</tr>
<tr class="row-odd"><td class="text-right"><p>67</p></td>
<td class="text-right"><p>0.393868</p></td>
<td class="text-right"><p>0.159575</p></td>
<td class="text-right"><p>0.446557</p></td>
</tr>
<tr class="row-even"><td class="text-right"><p>68</p></td>
<td class="text-right"><p>0.509837</p></td>
<td class="text-right"><p>0.144219</p></td>
<td class="text-right"><p>0.345943</p></td>
</tr>
</tbody>
</table>
<p>Remember that the output of the network uses the <code class="docutils literal notranslate"><span class="pre">softmax</span></code> activation function and has three
outputs, one for each species. This dataframe shows this nicely.</p>
<p>We now need to transform this output to one penguin species per sample.
We can do this by looking for the index of highest valued output and converting that
to the corresponding species.
Pandas dataframes have the <code class="docutils literal notranslate"><span class="pre">idxmax</span></code> function, which will do exactly that.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">predicted_species</span> <span class="o">=</span> <span class="n">prediction</span><span class="o">.</span><span class="n">idxmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="s2">&quot;columns&quot;</span><span class="p">)</span>
<span class="n">predicted_species</span>
</pre></div>
</div>
<div class="highlight-output notranslate"><div class="highlight"><pre><span></span><span class="go">0     Gentoo</span>
<span class="go">1     Adelie</span>
<span class="go">2     Gentoo</span>
<span class="go">3     Adelie</span>
<span class="go">4     Gentoo</span>
<span class="go">      ...</span>
<span class="go">64    Adelie</span>
<span class="go">65    Adelie</span>
<span class="go">66    Gentoo</span>
<span class="go">67    Gentoo</span>
<span class="go">68    Adelie</span>
<span class="go">Length: 69, dtype: object</span>
</pre></div>
</div>
<div class="admonition-break instructor dropdown admonition" id="instructor-3">
<p class="admonition-title">BREAK</p>
<p>This is a good time for switching instructor and/or a break.</p>
</div>
</section>
<section id="measuring-performance">
<h4>8. Measuring performance<a class="headerlink" href="#measuring-performance" title="Link to this heading"></a></h4>
<p>Now that we have a trained neural network it is important to assess how well it performs.
We want to know how well it will perform in a realistic prediction scenario, measuring
performance will also come back when refining the model.</p>
<p>We have created a test set (i.e. y_test) during the data preparation stage which we will use
now to create a confusion matrix.</p>
<section id="confusion-matrix">
<h5>Confusion matrix<a class="headerlink" href="#confusion-matrix" title="Link to this heading"></a></h5>
<p>With the predicted species we can now create a confusion matrix and display it using seaborn.</p>
<p>A confusion matrix is an <code class="docutils literal notranslate"><span class="pre">N</span> <span class="pre">x</span> <span class="pre">N</span></code> matrix used for evaluating the performance of a classification model, where <code class="docutils literal notranslate"><span class="pre">N</span></code> is the number of target classes.
The matrix compares the actual target values with those predicted from the classification model, which gives a holistic view of how well the classification model is performing.</p>
<p>To create a confusion matrix we will use another convenience function from sklearn called <code class="docutils literal notranslate"><span class="pre">confusion_matrix</span></code>.
This function takes as a first parameter the true labels of the test set.
We can get these by using the <code class="docutils literal notranslate"><span class="pre">idxmax</span></code> method on the y_test dataframe.
The second parameter is the predicted labels which we did above.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="n">confusion_matrix</span>

<span class="n">true_species</span> <span class="o">=</span> <span class="n">y_test</span><span class="o">.</span><span class="n">idxmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="s2">&quot;columns&quot;</span><span class="p">)</span>

<span class="n">matrix</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">true_species</span><span class="p">,</span> <span class="n">predicted_species</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">matrix</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-output notranslate"><div class="highlight"><pre><span></span><span class="go">[[22  0  8]</span>
<span class="go"> [ 5  0  9]</span>
<span class="go"> [ 6  0 19]]</span>
</pre></div>
</div>
<p>Unfortunately, this matrix is not immediately understandable. Its not clear which column and which row corresponds to which species.
So let’s convert it to a Pandas Dataframe with its index and columns set to the species as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Convert to a pandas dataframe</span>
<span class="n">confusion_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">matrix</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">y_test</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">y_test</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>

<span class="c1"># Set the names of the x and y axis, this helps with the readability of the heatmap.</span>
<span class="n">confusion_df</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;True Label&#39;</span>
<span class="n">confusion_df</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;Predicted Label&#39;</span>
<span class="n">confusion_df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
<p>We can then use the <code class="docutils literal notranslate"><span class="pre">heatmap</span></code> function from seaborn to create a nice visualization of
the confusion matrix.
The <code class="docutils literal notranslate"><span class="pre">annot=True</span></code> parameter here will put the numbers from the confusion matrix in
the heatmap.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">confusion_df</span><span class="p">,</span> <span class="n">annot</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<figure class="align-default" id="id10">
<img alt="Confusion matrix of the test set with high accuracy for Adelie and Gentoo classification and no correctly predicted Chinstrap" src="_images/confusion_matrix.png" />
<figcaption>
<p><span class="caption-text">“Confusion Matrix”</span><a class="headerlink" href="#id10" title="Link to this image"></a></p>
</figcaption>
</figure>
<p>Here are more explanations of this confusion matrix and the classification model.</p>
<ul class="simple">
<li><p>The first row: There are 30 Adelie penguins in the test data, with 22 identified as Adelie (valid), 8 being identified as Gentoo (invalid), and no Adelie is identified as Chinstrap.</p></li>
<li><p>The second row: There are 14 Chinstrap pengunis in the test data, with 5 identified as Adelie (invalid), none are correctly recognized as Chinstrap, and 9 Chinstraps are identified as Gentoo (invalid).</p></li>
<li><p>The third row: There are 25 Gentoo penguins in the test data, with 6 identified as Adelie (invalid), none being recognized as Chinstrap (invalid), and 19 Gentoos are identified as Gentoo (valid).</p></li>
</ul>
<div class="admonition-confusion-matrix exercise important admonition" id="exercise-4">
<p class="admonition-title">Confusion Matrix</p>
<p>Measure the performance of the neural network you trained and
visualize a confusion matrix.</p>
<ul class="simple">
<li><p>Did the neural network perform well on the test set?</p></li>
<li><p>Did you expect this from the training loss you saw?</p></li>
<li><p>What could we do to improve the performance?</p></li>
</ul>
</div>
<div class="admonition-solution solution important dropdown admonition" id="solution-4">
<p class="admonition-title">Solution</p>
<p>The confusion matrix shows that the predictions for Adelie and Gentoo are decent, but could be improved. However, Chinstrap is not predicted ever.</p>
<p>If we go back to the <a class="reference internal" href="#pair-plot"><span class="xref myst"><strong>Pair Plot</strong></span></a> in the Visualization section above, we can figure out that the biggest challenge is distinguishing the Chinstrap penguins from the marginal distributions of the four features (bill length, bill depth, flipper length, and body mass). That means that there is no single variable that separates Chinstrap penguins from all other species. Only the combination of bill length and bill depth gives a good separation of Chinstrap from Adelie and Gentoo penguins.</p>
<p>The training loss was very low, so the low accuracy on the test set may be surprising. But this illustrates very well why a test set is important to give a realistic evaluation when training neural networks (or other machine learning classifiers).</p>
<p>We can try many things to improve the performance from here. One of the first things we can try is to balance the dataset better.</p>
<p>Furthermore, the constructed neural network has a limited number of parameters.
A practical workaround is to increase the number of dense layers and also the number of neurons in each dense layers.</p>
<p>In addition, adjusting the learning rate can also help achieving a high score for the prediction. You will get more info in the <a class="reference internal" href="#./4-advanced-layer-types.html"><span class="xref myst"><strong>Advanced layer types</strong></span></a> episode.</p>
<p>Note that the outcome you have might be slightly different from what is shown in this tutorial.</p>
</div>
</section>
</section>
<section id="refine-the-model">
<h4>9. Refine the model<a class="headerlink" href="#refine-the-model" title="Link to this heading"></a></h4>
<p>As we discussed before the design and training of a neural network comes with
many hyperparameter and model architecture choices.
We will go into more depth of these choices in later episodes.
For now it is important to realize that the parameters we chose were
somewhat arbitrary and more careful consideration needs to be taken to
pick hyperparameter values.</p>
</section>
<section id="share-model">
<h4>10. Share model<a class="headerlink" href="#share-model" title="Link to this heading"></a></h4>
<p>It is very useful to be able to use the trained neural network at a later
stage without having to retrain it.
This can be done by using the <code class="docutils literal notranslate"><span class="pre">save</span></code> method of the model.
It takes a string as a parameter which is the path of a directory where the model is stored.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s1">&#39;my_first_model.keras&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>This saved model can be loaded again by using the <code class="docutils literal notranslate"><span class="pre">load_model</span></code> method as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">pretrained_model</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="s1">&#39;my_first_model.keras&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>This loaded model can be used as before to predict.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># use the pretrained model here</span>
<span class="n">y_pretrained_pred</span> <span class="o">=</span> <span class="n">pretrained_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">pretrained_prediction</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">y_pretrained_pred</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">target</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>

<span class="c1"># idxmax will select the column for each row with the highest value</span>
<span class="n">pretrained_predicted_species</span> <span class="o">=</span> <span class="n">pretrained_prediction</span><span class="o">.</span><span class="n">idxmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="s2">&quot;columns&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">pretrained_predicted_species</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-output notranslate"><div class="highlight"><pre><span></span><span class="go">0     Adelie</span>
<span class="go">1     Gentoo</span>
<span class="go">2     Adelie</span>
<span class="go">3     Gentoo</span>
<span class="go">4     Gentoo</span>
<span class="go">      ...</span>
<span class="go">64    Gentoo</span>
<span class="go">65    Gentoo</span>
<span class="go">66    Adelie</span>
<span class="go">67    Adelie</span>
<span class="go">68    Gentoo</span>
<span class="go">Length: 69, dtype: object</span>
</pre></div>
</div>
<div class="admonition-keypoints keypoints admonition" id="keypoints-0">
<p class="admonition-title">Keypoints</p>
<ul class="simple">
<li><p>The deep learning workflow is a useful tool to structure your approach, it helps to make sure you do not forget any important steps.</p></li>
<li><p>Exploring the data is an important step to familiarize yourself with the problem and to help you determine the relavent inputs and outputs.</p></li>
<li><p>One-hot encoding is a preprocessing step to prepare labels for classification in Keras.</p></li>
<li><p>A fully connected layer is a layer which has connections to all neurons in the previous and subsequent layers.</p></li>
<li><p>keras.layers.Dense is an implementation of a fully connected layer, you can set the number of neurons in the layer and the activation function used.</p></li>
<li><p>To train a neural network with Keras we need to first define the network using layers and the Model class. Then we can train it using the model.fit function.</p></li>
<li><p>Plotting the loss curve can be used to identify and troubleshoot the training process.</p></li>
<li><p>The loss curve on the training set does not provide any information on how well a network performs in a real setting.</p></li>
<li><p>Creating a confusion matrix with results from a test set gives better insight into the network’s performance.</p></li>
</ul>
</div>
</section>
</section>
<span id="document-3-monitor-the-model"></span><section class="tex2jax_ignore mathjax_ignore" id="monitor-the-training-process">
<h3>3. Monitor the training process<a class="headerlink" href="#monitor-the-training-process" title="Link to this heading"></a></h3>
<div class="admonition-questions questions admonition" id="questions-0">
<p class="admonition-title">Questions</p>
<ul class="simple">
<li><p>How do I create a neural network for a regression task?</p></li>
<li><p>How does optimization work?</p></li>
<li><p>How do I monitor the training process?</p></li>
<li><p>How do I detect (and avoid) overfitting?</p></li>
<li><p>What are common options to improve the model performance?</p></li>
</ul>
</div>
<div class="admonition-objectives objectives admonition" id="objectives-0">
<p class="admonition-title">Objectives</p>
<ul class="simple">
<li><p>Explain the importance of keeping your test set clean, by validating on the validation set instead of the test set</p></li>
<li><p>Use the data splits to plot the training process</p></li>
<li><p>Explain how optimization works</p></li>
<li><p>Design a neural network for a regression task</p></li>
<li><p>Measure the performance of your deep neural network</p></li>
<li><p>Interpret the training plots to recognize overfitting</p></li>
<li><p>Use normalization as preparation step for deep learning</p></li>
<li><p>Implement basic strategies to prevent overfitting</p></li>
</ul>
</div>
<div class="admonition-copy-pasting-code instructor dropdown admonition" id="instructor-0">
<p class="admonition-title">Copy-pasting code</p>
<p>In this episode we first introduce a simple approach to the problem,
then we iterate on that a few times to, step-by-step,
working towards a more complex solution.
Unfortunately, this involves using the same code repeatedly over and over again,
only slightly adapting it.</p>
<p>To avoid too much typing, it can help to copy-paste code from higher up in the notebook.
Be sure to make it clear where you are copying from
and what you are actually changing in the copied code.
It can for example help to add a comment to the lines that you added.</p>
</div>
<p>In this episode we will explore how to monitor the training progress, evaluate our the model predictions and finetune the model to avoid over-fitting. For that we will use a more complicated weather data-set.</p>
<section id="formulate-outline-the-problem-weather-prediction">
<h4>1. Formulate / Outline the problem: weather prediction<a class="headerlink" href="#formulate-outline-the-problem-weather-prediction" title="Link to this heading"></a></h4>
<p>Here we want to work with the <em>weather prediction dataset</em> (the light version) which can be
<a class="reference external" href="https://doi.org/10.5281/zenodo.5071376">downloaded from Zenodo</a>.
It contains daily weather observations from 11 different European cities or places through the
years 2000 to 2010. For all locations the data contains the variables ‘mean temperature’, ‘max temperature’, and ‘min temperature’. In addition, for multiple locations, the following variables are provided: ‘cloud_cover’, ‘wind_speed’, ‘wind_gust’, ‘humidity’, ‘pressure’, ‘global_radiation’, ‘precipitation’, ‘sunshine’, but not all of them are provided for every location. A more extensive description of the dataset including the different physical units is given in accompanying metadata file. The full dataset comprises of 10 years (3654 days) of collected weather data across Europe.</p>
<p><img alt="" class="align-center" src="_images/03_weather_prediction_dataset_map.png" /></p>
<div class="note docutils">
<p>European locations in the weather prediction dataset</p>
</div>
<p>A very common task with weather data is to make a prediction about the weather sometime in the future, say the next day. In this episode, we will try to predict tomorrow’s sunshine hours, a challenging-to-predict feature, using a neural network with the available weather data for one location: BASEL.</p>
</section>
<section id="identify-inputs-and-outputs">
<h4>2. Identify inputs and outputs<a class="headerlink" href="#identify-inputs-and-outputs" title="Link to this heading"></a></h4>
<section id="import-dataset">
<h5>Import Dataset<a class="headerlink" href="#import-dataset" title="Link to this heading"></a></h5>
<p>We will now import and explore the weather data-set:</p>
<div class="admonition-load-the-data callout admonition" id="callout-0">
<p class="admonition-title">Load the data</p>
<p>If you have not downloaded the data yet, you can also load it directly from Zenodo:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;https://zenodo.org/record/5071376/files/weather_prediction_dataset_light.csv?download=1&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>SSL certificate error</strong></p>
<!-- Using H4 here because H3 renders to big compared to the title of the callout -->
<p>If you get the following error message: <code class="docutils literal notranslate"><span class="pre">certificate</span> <span class="pre">verify</span> <span class="pre">failed:</span> <span class="pre">unable</span> <span class="pre">to</span> <span class="pre">get</span> <span class="pre">local</span> <span class="pre">issuer</span> <span class="pre">certificate</span></code>,
you can download <a class="reference external" href="https://zenodo.org/record/5071376/files/weather_prediction_dataset_light.csv?download=1">the data from here manually</a>
into a local folder and load the data using the code below.</p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>

<span class="n">filename_data</span> <span class="o">=</span> <span class="s2">&quot;weather_prediction_dataset_light.csv&quot;</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">filename_data</span><span class="p">)</span>
<span class="n">data</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-right"><p></p></th>
<th class="head text-right"><p>DATE</p></th>
<th class="head text-right"><p>MONTH</p></th>
<th class="head text-right"><p>BASEL_cloud_cover</p></th>
<th class="head text-right"><p>BASEL_humidity</p></th>
<th class="head text-right"><p>BASEL_pressure</p></th>
<th class="head text-right"><p>…</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-right"><p>0</p></td>
<td class="text-right"><p>20000101</p></td>
<td class="text-right"><p>1</p></td>
<td class="text-right"><p>8</p></td>
<td class="text-right"><p>0.89</p></td>
<td class="text-right"><p>1.0286</p></td>
<td class="text-right"><p>…</p></td>
</tr>
<tr class="row-odd"><td class="text-right"><p>1</p></td>
<td class="text-right"><p>20000102</p></td>
<td class="text-right"><p>1</p></td>
<td class="text-right"><p>8</p></td>
<td class="text-right"><p>0.87</p></td>
<td class="text-right"><p>1.0318</p></td>
<td class="text-right"><p>…</p></td>
</tr>
<tr class="row-even"><td class="text-right"><p>2</p></td>
<td class="text-right"><p>20000103</p></td>
<td class="text-right"><p>1</p></td>
<td class="text-right"><p>5</p></td>
<td class="text-right"><p>0.81</p></td>
<td class="text-right"><p>1.0314</p></td>
<td class="text-right"><p>…</p></td>
</tr>
<tr class="row-odd"><td class="text-right"><p>3</p></td>
<td class="text-right"><p>20000104</p></td>
<td class="text-right"><p>1</p></td>
<td class="text-right"><p>7</p></td>
<td class="text-right"><p>0.79</p></td>
<td class="text-right"><p>1.0262</p></td>
<td class="text-right"><p>…</p></td>
</tr>
<tr class="row-even"><td class="text-right"><p>4</p></td>
<td class="text-right"><p>20000105</p></td>
<td class="text-right"><p>1</p></td>
<td class="text-right"><p>5</p></td>
<td class="text-right"><p>0.90</p></td>
<td class="text-right"><p>1.0246</p></td>
<td class="text-right"><p>…</p></td>
</tr>
</tbody>
</table>
</section>
<section id="brief-exploration-of-the-data">
<h5>Brief exploration of the data<a class="headerlink" href="#brief-exploration-of-the-data" title="Link to this heading"></a></h5>
<p>Let us start with a quick look at the type of features that we find in the data.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">data</span><span class="o">.</span><span class="n">columns</span>
</pre></div>
</div>
<div class="highlight-output notranslate"><div class="highlight"><pre><span></span><span class="go">Index([&#39;DATE&#39;, &#39;MONTH&#39;, &#39;BASEL_cloud_cover&#39;, &#39;BASEL_humidity&#39;,</span>
<span class="go">       &#39;BASEL_pressure&#39;, &#39;BASEL_global_radiation&#39;, &#39;BASEL_precipitation&#39;,</span>
<span class="go">       &#39;BASEL_sunshine&#39;, &#39;BASEL_temp_mean&#39;, &#39;BASEL_temp_min&#39;, &#39;BASEL_temp_max&#39;,</span>
<span class="go">        ...</span>
<span class="go">       &#39;SONNBLICK_temp_min&#39;, &#39;SONNBLICK_temp_max&#39;, &#39;TOURS_humidity&#39;,</span>
<span class="go">       &#39;TOURS_pressure&#39;, &#39;TOURS_global_radiation&#39;, &#39;TOURS_precipitation&#39;,</span>
<span class="go">       &#39;TOURS_temp_mean&#39;, &#39;TOURS_temp_min&#39;, &#39;TOURS_temp_max&#39;],</span>
<span class="go">      dtype=&#39;object&#39;)</span>
</pre></div>
</div>
<p>There is a total of 9 different measured variables (global_radiation, humidity, etcetera)</p>
<p>Let’s have a look at the shape of the dataset:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">data</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
<div class="highlight-output notranslate"><div class="highlight"><pre><span></span><span class="go">(3654, 91)</span>
</pre></div>
</div>
<p>This will give both the number of samples (3654) and the number of features (89 + month +
date).</p>
<p>For any row <code class="docutils literal notranslate"><span class="pre">i</span></code>, we will use the values of all fields except <code class="docutils literal notranslate"><span class="pre">MONTH</span></code> and <code class="docutils literal notranslate"><span class="pre">DATE</span></code> as the input features <code class="docutils literal notranslate"><span class="pre">X</span></code>.
We want to use them to forecast the number of sunshine hours of the next day,
hence we use the value of the field <code class="docutils literal notranslate"><span class="pre">BASEL_sunshine</span></code> in the <em>subsequent</em> row (<code class="docutils literal notranslate"><span class="pre">i+1</span></code>) as the label that we want to predict (<code class="docutils literal notranslate"><span class="pre">y</span></code>).</p>
</section>
</section>
<section id="prepare-data">
<h4>3. Prepare data<a class="headerlink" href="#prepare-data" title="Link to this heading"></a></h4>
<section id="select-a-subset-and-split-into-data-x-and-labels-y">
<h5>Select a subset and split into data (X) and labels (y)<a class="headerlink" href="#select-a-subset-and-split-into-data-x-and-labels-y" title="Link to this heading"></a></h5>
<p>The full dataset comprises of 10 years (3654 days) from which we will select only the first 3 years. The present dataset is sorted by “DATE”, so for each row <code class="docutils literal notranslate"><span class="pre">i</span></code> in the table we can pick a corresponding feature and location from row <code class="docutils literal notranslate"><span class="pre">i+1</span></code> that we later want to predict with our model. As outlined in step 1, we would like to predict the sunshine hours for the location: BASEL.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">nr_rows</span> <span class="o">=</span> <span class="mi">365</span><span class="o">*</span><span class="mi">3</span> <span class="c1"># 3 years</span>
<span class="c1"># data</span>
<span class="n">X_data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">loc</span><span class="p">[:</span><span class="n">nr_rows</span><span class="p">]</span> <span class="c1"># Select first 3 years</span>
<span class="n">X_data</span> <span class="o">=</span> <span class="n">X_data</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;DATE&#39;</span><span class="p">,</span> <span class="s1">&#39;MONTH&#39;</span><span class="p">])</span> <span class="c1"># Drop date and month column</span>

<span class="c1"># labels (sunshine hours the next day)</span>
<span class="n">y_data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="mi">1</span><span class="p">:(</span><span class="n">nr_rows</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)][</span><span class="s2">&quot;BASEL_sunshine&quot;</span><span class="p">]</span>
</pre></div>
</div>
<p>In general, it is important to check if the data contains any unexpected values such as <code class="docutils literal notranslate"><span class="pre">9999</span></code> or <code class="docutils literal notranslate"><span class="pre">NaN</span></code> or <code class="docutils literal notranslate"><span class="pre">NoneType</span></code>. You can use the pandas <code class="docutils literal notranslate"><span class="pre">data.describe()</span></code> or <code class="docutils literal notranslate"><span class="pre">data.isnull()</span></code> function for this. If so, such values must be removed or replaced.
In the present case the data is luckily well prepared and shouldn’t contain such values, so that this step can be omitted.</p>
</section>
<section id="split-data-and-labels-into-training-validation-and-test-set">
<h5>Split data and labels into training, validation, and test set<a class="headerlink" href="#split-data-and-labels-into-training-validation-and-test-set" title="Link to this heading"></a></h5>
<p>As with classical machine learning techniques, it is required in deep learning to split off a hold-out <em>test set</em> which remains untouched during model training and tuning. It is later used to evaluate the model performance. On top, we will also split off an additional <em>validation set</em>, the reason of which will hopefully become clearer later in this lesson.</p>
<p>To make our lives a bit easier, we employ a trick to create these 3 datasets, <code class="docutils literal notranslate"><span class="pre">training</span> <span class="pre">set</span></code>, <code class="docutils literal notranslate"><span class="pre">test</span> <span class="pre">set</span></code> and <code class="docutils literal notranslate"><span class="pre">validation</span> <span class="pre">set</span></code>, by calling the <code class="docutils literal notranslate"><span class="pre">train_test_split</span></code> method of <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code> twice.</p>
<p>First we create the training set and leave the remainder of 30 % of the data to the two hold-out sets.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">train_test_split</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_holdout</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_holdout</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X_data</span><span class="p">,</span> <span class="n">y_data</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
<p>Now we split the 30 % of the data in two equal sized parts.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">X_val</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_val</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X_holdout</span><span class="p">,</span> <span class="n">y_holdout</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
<p>Setting the <code class="docutils literal notranslate"><span class="pre">random_state</span></code> to <code class="docutils literal notranslate"><span class="pre">0</span></code> is a short-hand at this point. Note however, that changing this seed of the pseudo-random number generator will also change the composition of your data sets. For the sake of reproducibility, this is one example of a parameters that should not change at all.</p>
<div class="admonition-break instructor dropdown admonition" id="instructor-1">
<p class="admonition-title">BREAK</p>
<p>This is a good time for switching instructor and/or a break.</p>
</div>
</section>
</section>
<section id="choose-a-pretrained-model-or-start-building-architecture-from-scratch">
<h4>4. Choose a pretrained model or start building architecture from scratch<a class="headerlink" href="#choose-a-pretrained-model-or-start-building-architecture-from-scratch" title="Link to this heading"></a></h4>
<section id="regression-and-classification">
<h5>Regression and classification<a class="headerlink" href="#regression-and-classification" title="Link to this heading"></a></h5>
<p>In episode 2 we trained a dense neural network on a <em>classification task</em>. For this one hot encoding was used together with a <code class="docutils literal notranslate"><span class="pre">Categorical</span> <span class="pre">Crossentropy</span></code> loss function.
This measured how close the distribution of the neural network outputs corresponds to the distribution of the three values in the one hot encoding.
Now we want to work on a <em>regression task</em>, thus not predicting a class label (or integer number) for a datapoint. In regression, we predict one (and sometimes many) values of a feature. This is typically a floating point number.</p>
<div class="admonition-exercise-architecture-of-the-network exercise important admonition" id="exercise-0">
<p class="admonition-title">Exercise: Architecture of the network</p>
<p>As we want to design a neural network architecture for a regression task,
see if you can first come up with the answers to the following questions:</p>
<ol class="arabic simple">
<li><p>What must be the dimension of our input layer?</p></li>
<li><p>We want to output the prediction of a single number. The output layer of the NN hence cannot be the same as for the classification task earlier. This is because the <code class="docutils literal notranslate"><span class="pre">softmax</span></code> activation being used had a concrete meaning with respect to the class labels which is not needed here. What output layer design would you choose for regression?
Hint: A layer with <code class="docutils literal notranslate"><span class="pre">relu</span></code> activation, with <code class="docutils literal notranslate"><span class="pre">sigmoid</span></code> activation or no activation at all?</p></li>
<li><p>(Optional) How would we change the model if we would like to output a prediction of the precipitation in Basel in <em>addition</em> to the sunshine hours?</p></li>
</ol>
<div class="admonition-solution solution important dropdown admonition" id="solution-0">
<p class="admonition-title">Solution</p>
<ol class="arabic simple">
<li><p>The shape of the input layer has to correspond to the number of features in our data: 89</p></li>
<li><p>The output is a single value per prediction, so the output layer can consist of a dense layer with only one node. The <em>softmax</em> activiation function works well for a classification task, but here we do not want to restrict the possible outcomes to the range of zero and one. In fact, we can omit the activation in the output layer.</p></li>
<li><p>The output layer should have 2 neurons, one for each number that we try to predict. Our y_train (and val and test) then becomes a (n_samples, 2) matrix.</p></li>
</ol>
</div>
</div>
<div class="docutils">
<p>In our example we want to predict the sunshine hours in Basel (or any other place in the dataset) for tomorrow based on the weather data of all 18 locations today. <code class="docutils literal notranslate"><span class="pre">BASEL_sunshine</span></code> is a floating point value (i.e. <code class="docutils literal notranslate"><span class="pre">float64</span></code>). The network should hence output a single float value which is why the last layer of our network will only consist of a single node.</p>
<p>We compose a network of two hidden layers to start off with something. We go by a scheme with 100 neurons in the first hidden layer and 50 neurons in the second layer. As activation function we settle on the <code class="docutils literal notranslate"><span class="pre">relu</span></code> function as a it is very robust and widely used. To make our live easier later, we wrap the definition of the network in a function called <code class="docutils literal notranslate"><span class="pre">create_nn()</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">tensorflow</span><span class="w"> </span><span class="kn">import</span> <span class="n">keras</span>

<span class="k">def</span><span class="w"> </span><span class="nf">create_nn</span><span class="p">(</span><span class="n">input_shape</span><span class="p">):</span>
    <span class="c1"># Input layer</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="n">input_shape</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;input&#39;</span><span class="p">)</span>

    <span class="c1"># Dense layers</span>
    <span class="n">layers_dense</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="s1">&#39;relu&#39;</span><span class="p">)(</span><span class="n">inputs</span><span class="p">)</span>
    <span class="n">layers_dense</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="s1">&#39;relu&#39;</span><span class="p">)(</span><span class="n">layers_dense</span><span class="p">)</span>

    <span class="c1"># Output layer</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">)(</span><span class="n">layers_dense</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;weather_prediction_model&quot;</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">create_nn</span><span class="p">(</span><span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="n">X_data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],))</span>
</pre></div>
</div>
<p>The shape of the input layer has to correspond to the number of features in our data: <code class="docutils literal notranslate"><span class="pre">89</span></code>. We use <code class="docutils literal notranslate"><span class="pre">X_data.shape[1]</span></code> to obtain this value dynamically</p>
<p>The output layer here is a dense layer with only 1 node. And we here have chosen to use <em>no activation function</em>.
While we might use <em>softmax</em> for a classification task, here we do not want to restrict the possible outcomes for a start.</p>
<p>In addition, we have here chosen to write the network creation as a function so that we can use it later again to initiate new models.</p>
<p>Let us check how our model looks like by calling the <code class="docutils literal notranslate"><span class="pre">summary</span></code> method.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-output notranslate"><div class="highlight"><pre><span></span><span class="go">Model: &quot;weather_prediction_model&quot;</span>

<span class="go">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓</span>
<span class="go">┃ Layer (type)                ┃ Output Shape        ┃       Param # ┃</span>
<span class="go">┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩</span>
<span class="go">│ input (InputLayer)          │ (None, 89)          │             0 │</span>
<span class="go">├─────────────────────────────┼─────────────────────┼───────────────┤</span>
<span class="go">│ dense (Dense)               │ (None, 100)         │         9,000 │</span>
<span class="go">├─────────────────────────────┼─────────────────────┼───────────────┤</span>
<span class="go">│ dense_1 (Dense)             │ (None, 50)          │         5,050 │</span>
<span class="go">├─────────────────────────────┼─────────────────────┼───────────────┤</span>
<span class="go">│ dense_2 (Dense)             │ (None, 1)           │            51 │</span>
<span class="go">└─────────────────────────────┴─────────────────────┴───────────────┘</span>

<span class="go"> Total params: 14,101 (55.08 KB)</span>

<span class="go"> Trainable params: 14,101 (55.08 KB)</span>

<span class="go"> Non-trainable params: 0 (0.00 B)</span>
</pre></div>
</div>
<p>When compiling the model we can define a few very important aspects. We will discuss them now in more detail.</p>
<p class="rubric" id="intermezzo-how-do-neural-networks-learn">Intermezzo: How do neural networks learn?</p>
<p>In the introduction we learned about the loss function: it quantifies the total error of the predictions made by the model.
During model training we aim to find the model parameters that minimize the loss.
This is called optimization, but how does optimization actually work?</p>
<p class="rubric" id="gradient-descent">Gradient descent</p>
<p>Gradient descent is a widely used optimization algorithm, most other optimization algorithms are based on it.
It works as follows: Imagine a neural network with only one neuron.
Take a look at the figure below. The plot shows the loss as a function of the weight of the neuron.
As you can see there is a global loss minimum, we would like to find the weight at this point in the parabola.
To do this, we initialize the model weight with some random value. Then we compute the gradient of the loss function with respect
to the weight. This tells us how much the loss function will change if we change the weight by a small amount.
Then, we update the weight by taking a small step in the direction of the negative gradient, so down the slope.
This will slightly decrease the loss. This process is repeated until the loss function reaches a minimum.
The size of the step that is taken in each iteration is called the ‘learning rate’.</p>
<p><img alt="" class="align-center" src="_images/03_gradient_descent.png" /></p>
<p class="rubric" id="batch-gradient-descent">Batch gradient descent</p>
<p>You could use the entire training dataset to perform one learning step in gradient descent,
which would mean that one epoch equals one learning step.
In practice, in each learning step we only use a subset of the training data to compute the loss and the gradients.
This subset is called a ‘batch’, the number of samples in one batch is called the ‘batch size’.</p>
<div class="admonition-exercise-gradient-descent exercise important admonition" id="exercise-1">
<p class="admonition-title">Exercise: Gradient descent</p>
<p>Answer the following questions:</p>
<p><strong>1. What is the goal of optimization?</strong></p>
<ul class="simple">
<li><p>A. To find the weights that maximize the loss function</p></li>
<li><p>B. To find the weights that minimize the loss function</p></li>
</ul>
<p><strong>2. What happens in one gradient descent step?</strong></p>
<ul class="simple">
<li><p>A. The weights are adjusted so that we move in the direction of the gradient, so up the slope of the loss function</p></li>
<li><p>B. The weights are adjusted so that we move in the direction of the gradient, so down the slope of the loss function</p></li>
<li><p>C. The weights are adjusted so that we move in the direction of the negative gradient, so up the slope of the loss function</p></li>
<li><p>D. The weights are adjusted so that we move in the direction of the negative gradient, so down the slope of the loss function</p></li>
</ul>
<p><strong>3. When the batch size is increased:</strong>
(multiple answers might apply)</p>
<ul class="simple">
<li><p>A. The number of samples in an epoch also increases</p></li>
<li><p>B. The number of batches in an epoch goes down</p></li>
<li><p>C. The training progress is more jumpy, because more samples are consulted in each update step (one batch).</p></li>
<li><p>D. The memory load (memory as in computer hardware) of the training process is increased</p></li>
</ul>
</div>
</div>
<div class="admonition-solution solution important dropdown admonition" id="solution-1">
<p class="admonition-title">Solution</p>
<ol class="arabic simple">
<li><p>Correct answer: B. To find the weights that minimize the loss function.
The loss function quantifies the total error of the network, we want to have the smallest error as possible, hence we minimize the loss.</p></li>
<li><p>Correct answer: D The weights are adjusted so that we move in the direction of the negative gradient, so down the slope of the loss function.
We want to move towards the global minimum, so in the opposite direction of the gradient.</p></li>
<li><p>Correct answer: B &amp; D</p>
<ul class="simple">
<li><p>A. The number of samples in an epoch also increases (<strong>incorrect</strong>, an epoch is always defined as passing through the training data for one cycle)</p></li>
<li><p>B. The number of batches in an epoch goes down (<strong>correct</strong>, the number of batches is the samples in an epoch divided by the batch size)</p></li>
<li><p>C. The training progress is more jumpy, because more samples are consulted in each update step (one batch). (<strong>incorrect</strong>, more samples are consulted in each update step, but this makes the progress less jumpy since you get a more accurate estimate of the loss in the entire dataset)</p></li>
<li><p>D. The memory load (memory as in computer hardware) of the training process is increased (<strong>correct</strong>, the data is begin loaded one batch at a time, so more samples means more memory usage)</p></li>
</ul>
</li>
</ol>
</div>
</section>
</section>
<section id="choose-a-loss-function-and-optimizer">
<h4>5. Choose a loss function and optimizer<a class="headerlink" href="#choose-a-loss-function-and-optimizer" title="Link to this heading"></a></h4>
<section id="loss-function">
<h5>Loss function<a class="headerlink" href="#loss-function" title="Link to this heading"></a></h5>
<p>The loss is what the neural network will be optimized on during training, so choosing a suitable loss function is crucial for training neural networks.
In the given case we want to stimulate that the predicted values are as close as possible to the true values. This is commonly done by using the <em>mean squared error</em> (mse) or the <em>mean absolute error</em> (mae), both of which should work OK in this case. Often, mse is preferred over mae because it “punishes” large prediction errors more severely.
In Keras this is implemented in the <code class="docutils literal notranslate"><span class="pre">keras.losses.MeanSquaredError</span></code> class (see Keras documentation: https://keras.io/api/losses/). This can be provided into the <code class="docutils literal notranslate"><span class="pre">model.compile</span></code> method with the <code class="docutils literal notranslate"><span class="pre">loss</span></code> parameter and setting it to <code class="docutils literal notranslate"><span class="pre">mse</span></code>, e.g.</p>
<!--cce:skip-->
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s1">&#39;mse&#39;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="optimizer">
<h5>Optimizer<a class="headerlink" href="#optimizer" title="Link to this heading"></a></h5>
<p>Somewhat coupled to the loss function is the <em>optimizer</em> that we want to use.
The <em>optimizer</em> here refers to the algorithm with which the model learns to optimize on the provided loss function. A basic example for such an optimizer would be <em>stochastic gradient descent</em>. For now, we can largely skip this step and pick one of the most common optimizers that works well for most tasks: the <em>Adam optimizer</em>. Similar to activation functions, the choice of optimizer depends on the problem you are trying to solve, your model architecture and your data. <em>Adam</em> is a good starting point though, which is why we chose it.</p>
<!--cce:skip-->
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;adam&#39;</span><span class="p">,</span>
              <span class="n">loss</span><span class="o">=</span><span class="s1">&#39;mse&#39;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="metrics">
<h5>Metrics<a class="headerlink" href="#metrics" title="Link to this heading"></a></h5>
<p>In our first example (episode 2) we plotted the progression of the loss during training.
That is indeed a good first indicator if things are working alright, i.e. if the loss is indeed decreasing as it should with the number of epochs.
However, when models become more complicated then also the loss functions often become less intuitive.
That is why it is good practice to monitor the training process with additional, more intuitive metrics.
They are not used to optimize the model, but are simply recorded during training.</p>
<p>With Keras, such additional metrics can be added via <code class="docutils literal notranslate"><span class="pre">metrics=[...]</span></code> parameter and can contain one or multiple metrics of interest.
Here we could for instance chose <code class="docutils literal notranslate"><span class="pre">mae</span></code> (<a class="reference external" href="https://glosario.carpentries.org/en/#mean_absolute_error">mean absolute error</a>), or the the <a class="reference external" href="https://glosario.carpentries.org/en/#root_mean_squared_error"><em>root mean squared error</em> (RMSE)</a> which unlike the <em>mse</em> has the same units as the predicted values. For the sake of units, we choose the latter.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;adam&#39;</span><span class="p">,</span>
              <span class="n">loss</span><span class="o">=</span><span class="s1">&#39;mse&#39;</span><span class="p">,</span>
              <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="n">keras</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">RootMeanSquaredError</span><span class="p">()])</span>
</pre></div>
</div>
<p>Let’s create a <code class="docutils literal notranslate"><span class="pre">compile_model</span></code> function to easily compile the model throughout this lesson:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">compile_model</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
    <span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;adam&#39;</span><span class="p">,</span>
                  <span class="n">loss</span><span class="o">=</span><span class="s1">&#39;mse&#39;</span><span class="p">,</span>
                  <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="n">keras</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">RootMeanSquaredError</span><span class="p">()])</span>
<span class="n">compile_model</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
<p>With this, we complete the compilation of our network and are ready to start training.</p>
</section>
</section>
<section id="train-the-model">
<h4>6. Train the model<a class="headerlink" href="#train-the-model" title="Link to this heading"></a></h4>
<p>Now that we created and compiled our dense neural network, we can start training it.
We add the <code class="docutils literal notranslate"><span class="pre">batch_size</span></code> parameter that defines – as discussed above – how many samples from the training data will be used to estimate the error gradient before the model weights are updated.
Larger batches will produce better, more accurate gradient estimates but also less frequent updates of the weights.
Here we are going to use a batch size of 32 which is a common starting point.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span>
                    <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
                    <span class="n">epochs</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span>
                    <span class="n">verbose</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
<p>We can plot the training process using the <code class="docutils literal notranslate"><span class="pre">history</span></code> object returned from the model training.
We will create a function for it, because we will make use of this more often in this lesson!</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">seaborn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">sns</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="k">def</span><span class="w"> </span><span class="nf">plot_history</span><span class="p">(</span><span class="n">history</span><span class="p">,</span> <span class="n">metrics</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Plot the training history</span>

<span class="sd">    Args:</span>
<span class="sd">        history (keras History object that is returned by model.fit())</span>
<span class="sd">        metrics (str, list): Metric or a list of metrics to plot</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">history_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="o">.</span><span class="n">from_dict</span><span class="p">(</span><span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">)</span>
    <span class="n">sns</span><span class="o">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">history_df</span><span class="p">[</span><span class="n">metrics</span><span class="p">])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;epochs&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;metric&quot;</span><span class="p">)</span>

<span class="n">plot_history</span><span class="p">(</span><span class="n">history</span><span class="p">,</span> <span class="s1">&#39;root_mean_squared_error&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p><img alt="" class="align-center" src="_images/03_training_history_1_rmse.png" /></p>
<p>This looks very promising! Our metric (“RMSE”) is dropping nicely and while it maybe keeps fluctuating a bit it does end up at fairly low <em>RMSE</em> values.
But the <em>RMSE</em> is just the root <em>mean</em> squared error, so we might want to look a bit more in detail how well our just trained model does in predicting the sunshine hours.</p>
</section>
<section id="perform-a-prediction-classification">
<h4>7. Perform a Prediction/Classification<a class="headerlink" href="#perform-a-prediction-classification" title="Link to this heading"></a></h4>
<p>Now that we have our model trained, we can make a prediction with the model before measuring the performance of our neural network.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">y_train_predicted</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">y_test_predicted</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition-break instructor dropdown admonition" id="instructor-2">
<p class="admonition-title">BREAK</p>
<p>This is a good time for switching instructor and/or a break.</p>
</div>
</section>
<section id="measure-performance">
<h4>8. Measure performance<a class="headerlink" href="#measure-performance" title="Link to this heading"></a></h4>
<p>There is not a single way to evaluate how a model performs. But there are at least two very common approaches. For a <em>classification task</em> that is to compute a <em>confusion matrix</em> for the test set which shows how often particular classes were predicted correctly or incorrectly.</p>
<p>For the present <em>regression task</em>, it makes more sense to compare true and predicted values in a scatter plot.</p>
<p>So, let’s look at how the predicted sunshine hour have developed with reference to their ground truth values.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># We define a function that we will reuse in this lesson</span>
<span class="k">def</span><span class="w"> </span><span class="nf">plot_predictions</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y_true</span><span class="p">,</span> <span class="n">title</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;ggplot&#39;</span><span class="p">)</span>  <span class="c1"># optional, that&#39;s only to define a visual style</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y_true</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;predicted sunshine hours&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;true sunshine hours&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>

<span class="n">plot_predictions</span><span class="p">(</span><span class="n">y_train_predicted</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s1">&#39;Predictions on the training set&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p><img alt="" class="align-center" src="_images/03_regression_predictions_trainset.png" /></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">plot_predictions</span><span class="p">(</span><span class="n">y_test_predicted</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s1">&#39;Predictions on the test set&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p><img alt="" class="align-center" src="_images/03_regression_predictions_testset.png" /></p>
<div class="admonition-exercise-reflecting-on-our-results exercise important admonition" id="exercise-2">
<p class="admonition-title">Exercise: Reflecting on our results</p>
<ul class="simple">
<li><p>Is the performance of the model as you expected (or better/worse)?</p></li>
<li><p>Is there a noteable difference between training set and test set? And if so, any idea why?</p></li>
<li><p>(Optional) When developing a model, you will often vary different aspects of your model like
which features you use, model parameters and architecture. It is important to settle on a
single-number evaluation metric to compare your models.</p>
<ul>
<li><p>What single-number evaluation metric would you choose here and why?</p></li>
</ul>
</li>
</ul>
</div>
<div class="admonition-solution solution important dropdown admonition" id="solution-2">
<p class="admonition-title">Solution</p>
<p>While the performance on the train set seems reasonable, the performance on the test set is much worse.
This is a common problem called <strong>overfitting</strong>, which we will discuss in more detail later.</p>
<p><strong>Optional exercise:</strong></p>
<p>The metric that we are using: RMSE would be a good one. You could also consider Mean Squared Error, that punishes large errors more (because large errors create even larger squared errors).
It is important that if the model improves in performance on the basis of this metric then that should also lead you a step closer to reaching your goal: to predict tomorrow’s sunshine hours.
If you feel that improving the metric does not lead you closer to your goal, then it would be better to choose a different metric</p>
</div>
<p>The accuracy on the training set seems fairly good.
In fact, considering that the task of predicting the daily sunshine hours is really not easy it might even be surprising how well the model predicts that
(at least on the training set). Maybe a little too good?
We also see the noticeable difference between train and test set when calculating the exact value of the RMSE:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">train_metrics</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">return_dict</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">test_metrics</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">return_dict</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Train RMSE: </span><span class="si">{:.2f}</span><span class="s1">, Test RMSE: </span><span class="si">{:.2f}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">train_metrics</span><span class="p">[</span><span class="s1">&#39;root_mean_squared_error&#39;</span><span class="p">],</span> <span class="n">test_metrics</span><span class="p">[</span><span class="s1">&#39;root_mean_squared_error&#39;</span><span class="p">]))</span>
</pre></div>
</div>
<div class="highlight-output notranslate"><div class="highlight"><pre><span></span><span class="go">24/24 [==============================] - 0s 442us/step - loss: 0.7092 - root_mean_squared_error: 0.8421</span>
<span class="go">6/6 [==============================] - 0s 647us/step - loss: 16.4413 - root_mean_squared_error: 4.0548</span>
<span class="go">Train RMSE: 0.84, Test RMSE: 4.05</span>
</pre></div>
</div>
<p>For those experienced with (classical) machine learning this might look familiar.
The plots above expose the signs of <strong>overfitting</strong> which means that the model has to some extent memorized aspects of the training data.
As a result, it makes much more accurate predictions on the training data than on unseen test data.</p>
<p>Overfitting also happens in classical machine learning, but there it is usually interpreted as the model having more parameters than the training data would justify (say, a decision tree with too many branches for the number of training instances). As a consequence one would reduce the number of parameters to avoid overfitting.
In deep learning the situation is slightly different. It can - as for classical machine learning - also be a sign of having a <em>too big</em> model, meaning a model with too many parameters (layers and/or nodes). However, in deep learning higher number of model parameters are often still considered acceptable and models often perform best (in terms of prediction accuracy) when they are at the verge of overfitting. So, in a way, training deep learning models is always a bit like playing with fire…</p>
<section id="set-expectations-how-difficult-is-the-defined-problem">
<h5>Set expectations: How difficult is the defined problem?<a class="headerlink" href="#set-expectations-how-difficult-is-the-defined-problem" title="Link to this heading"></a></h5>
<p>Before we dive deeper into handling overfitting and (trying to) improving the model performance, let us ask the question: How well must a model perform before we consider it a good model?</p>
<p>Now that we defined a problem (predict tomorrow’s sunshine hours), it makes sense to develop an intuition for how difficult the posed problem is. Frequently, models will be evaluated against a so called <strong>baseline</strong>. A baseline can be the current standard in the field or if such a thing does not exist it could also be an intuitive first guess or toy model. The latter is exactly what we would use for our case.</p>
<p>Maybe the simplest sunshine hour prediction we can easily do is: Tomorrow we will have the same number of sunshine hours as today.
(sounds very naive, but for many observables such as temperature this is already a fairly good predictor)</p>
<p>We can take the <code class="docutils literal notranslate"><span class="pre">BASEL_sunshine</span></code> column of our data, because this contains the sunshine hours from one day before what we have as a label.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">y_baseline_prediction</span> <span class="o">=</span> <span class="n">X_test</span><span class="p">[</span><span class="s1">&#39;BASEL_sunshine&#39;</span><span class="p">]</span>
<span class="n">plot_predictions</span><span class="p">(</span><span class="n">y_baseline_prediction</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s1">&#39;Baseline predictions on the test set&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p><img alt="" class="align-center" src="_images/03_regression_test_5_naive_baseline.png" /></p>
<p>It is difficult to interpret from this plot whether our model is doing better than the baseline.
We can also have a look at the RMSE:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="n">root_mean_squared_error</span>
<span class="n">rmse_baseline</span> <span class="o">=</span> <span class="n">root_mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_baseline_prediction</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Baseline:&#39;</span><span class="p">,</span> <span class="n">rmse_baseline</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Neural network: &#39;</span><span class="p">,</span> <span class="n">test_metrics</span><span class="p">[</span><span class="s1">&#39;root_mean_squared_error&#39;</span><span class="p">])</span>
</pre></div>
</div>
<div class="highlight-output notranslate"><div class="highlight"><pre><span></span><span class="go">Baseline: 3.877323350410224</span>
<span class="go">Neural network:  4.077792167663574</span>
</pre></div>
</div>
<p>Judging from the numbers alone, our neural network prediction would be performing worse than the baseline.</p>
<div class="admonition-exercise-baseline exercise important admonition" id="exercise-3">
<p class="admonition-title">Exercise: Baseline</p>
<ol class="arabic simple">
<li><p>Looking at this baseline: Would you consider this a simple or a hard problem to solve?</p></li>
<li><p>(Optional) Can you think of other baselines?</p></li>
</ol>
</div>
<div class="admonition-solution solution important dropdown admonition" id="solution-3">
<p class="admonition-title">Solution</p>
<ol class="arabic simple">
<li><p>This really depends on your definition of hard! The baseline gives a more accurate prediction than just
randomly predicting a number, so the problem is not impossible to solve with machine learning. However, given the structure of the data and our expectations with respect to quality of prediction, it may remain hard to find a good algorithm which exceeds our baseline by orders of magnitude.</p></li>
<li><p>There are a lot of possible answers. A slighly more complicated baseline would be to take the average over the last couple of days.</p></li>
</ol>
</div>
</section>
</section>
<section id="refine-the-model">
<h4>9. Refine the model<a class="headerlink" href="#refine-the-model" title="Link to this heading"></a></h4>
<section id="watch-your-model-training-closely">
<h5>Watch your model training closely<a class="headerlink" href="#watch-your-model-training-closely" title="Link to this heading"></a></h5>
<p>As we saw when comparing the predictions for the training and the test set, deep learning models are prone to overfitting. Instead of iterating through countless cycles of model trainings and subsequent evaluations with a reserved test set, it is common practice to work with a second split off dataset to monitor the model during training.
This is the <em>validation set</em> which can be regarded as a second test set. As with the test set, the datapoints of the <em>validation set</em> are not used for the actual model training itself. Instead, we evaluate the model with the <em>validation set</em> after every epoch during training, for instance to stop if we see signs of clear overfitting.</p>
<p>Since we are adapting our model (tuning our hyperparameters) based on this validation set, it is <em>very</em> important that it is kept separate from the test set. If we used the same set, we would not know whether our model truly generalizes or is only overfitting.</p>
<div class="admonition-test-vs-validation-set callout admonition" id="callout-1">
<p class="admonition-title">Test vs. validation set</p>
<p>Not everybody agrees on the terminology of test set versus validation set. You might find
examples in literature where these terms are used the other way around.
We are sticking to the definition that is consistent with the Keras API. In there, the validation
set can be used during training, and the test set is reserved for afterwards.</p>
</div>
<p>Let’s give this a try!</p>
<p>We need to initiate a new model – otherwise Keras will simply assume that we want to continue training the model we already trained above.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">create_nn</span><span class="p">(</span><span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="n">X_data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],))</span>
<span class="n">compile_model</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
<p>But now we train it with the small addition of also passing it our validation set:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span>
                    <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
                    <span class="n">epochs</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span>
                    <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">X_val</span><span class="p">,</span> <span class="n">y_val</span><span class="p">))</span>
</pre></div>
</div>
<p>With this we can plot both the performance on the training data and on the validation data!</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">plot_history</span><span class="p">(</span><span class="n">history</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;root_mean_squared_error&#39;</span><span class="p">,</span> <span class="s1">&#39;val_root_mean_squared_error&#39;</span><span class="p">])</span>
</pre></div>
</div>
<p><img alt="" class="align-center" src="_images/03_training_history_2_rmse.png" /></p>
<div class="admonition-exercise-plot-the-training-progress exercise important admonition" id="exercise-4">
<p class="admonition-title">Exercise: plot the training progress.</p>
<ol class="arabic simple">
<li><p>Is there a difference between the training curves of training versus validation data? And if so, what would this imply?</p></li>
<li><p>(Optional) Take a pen and paper, draw the perfect training and validation curves.
(This may seem trivial, but it will trigger you to think about what you actually would like to see)</p></li>
</ol>
<div class="admonition-solution solution important dropdown admonition" id="solution-4">
<p class="admonition-title">Solution</p>
<p>The difference in the two curves shows that something is not completely right here.
The error for the model predictions on the validation set quickly seem to reach a plateau while the error on the training set keeps decreasing.
That is a common signature of <em>overfitting</em>.</p>
<p>Optional:</p>
<p>Ideally you would like the training and validation curves to be identical and slope down steeply
to 0. After that the curves will just consistently stay at 0.</p>
</div>
</div>
<div class="docutils">
<p class="rubric" id="counteract-model-overfitting">Counteract model overfitting</p>
<p>Overfitting is a very common issue and there are many strategies to handle it.
Most similar to classical machine learning might to <strong>reduce the number of parameters</strong>.</p>
<div class="admonition-exercise-try-to-reduce-the-degree-of-overfitting-by-lowering-the-number-of-parameters exercise important admonition" id="exercise-5">
<p class="admonition-title">Exercise: Try to reduce the degree of overfitting by lowering the number of parameters</p>
<p>We can keep the network architecture unchanged (2 dense layers + a one-node output layer) and only play with the number of nodes per layer.
Try to lower the number of nodes in one or both of the two dense layers and observe the changes to the training and validation losses.
If time is short: Suggestion is to run one network with only 10 and 5 nodes in the first and second layer.</p>
<ol class="arabic simple">
<li><p>Is it possible to get rid of overfitting this way?</p></li>
<li><p>Does the overall performance suffer or does it mostly stay the same?</p></li>
<li><p>(optional) How low can you go with the number of parameters without notable effect on the performance on the validation set?</p></li>
</ol>
<div class="admonition-solution solution important dropdown admonition" id="solution-5">
<p class="admonition-title">Solution</p>
<p>Let’s first adapt our <code class="docutils literal notranslate"><span class="pre">create_nn()</span></code> function so that we can tweak the number of nodes in the 2 layers
by passing arguments to the function:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">create_nn</span><span class="p">(</span><span class="n">input_shape</span><span class="p">,</span> <span class="n">nodes1</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">nodes2</span><span class="o">=</span><span class="mi">50</span><span class="p">):</span>
   <span class="c1"># Input layer</span>
   <span class="n">inputs</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="n">input_shape</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;input&#39;</span><span class="p">)</span>
   <span class="c1"># Dense layers</span>
   <span class="n">layers_dense</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">nodes1</span><span class="p">,</span> <span class="s1">&#39;relu&#39;</span><span class="p">)(</span><span class="n">inputs</span><span class="p">)</span>
   <span class="n">layers_dense</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">nodes2</span><span class="p">,</span> <span class="s1">&#39;relu&#39;</span><span class="p">)(</span><span class="n">layers_dense</span><span class="p">)</span>
   <span class="c1"># Output layer</span>
   <span class="n">outputs</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">)(</span><span class="n">layers_dense</span><span class="p">)</span>
   <span class="k">return</span> <span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;model_small&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>Let’s see if it works by creating a much smaller network with 10 nodes in the first layer,
and 5 nodes in the second layer:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">create_nn</span><span class="p">(</span><span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="n">X_data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],),</span> <span class="n">nodes1</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">nodes2</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>Model: &quot;model_small&quot;

┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓
┃ Layer (type)                ┃ Output Shape        ┃       Param # ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩
│ input (InputLayer)          │ (None, 89)          │             0 │
├─────────────────────────────┼─────────────────────┼───────────────┤
│ dense_9 (Dense)             │ (None, 10)          │           900 │
├─────────────────────────────┼─────────────────────┼───────────────┤
│ dense_10 (Dense)            │ (None, 5)           │            55 │
├─────────────────────────────┼─────────────────────┼───────────────┤
│ dense_11 (Dense)            │ (None, 1)           │             6 │
└─────────────────────────────┴─────────────────────┴───────────────┘

 Total params: 961 (3.75 KB)

 Trainable params: 961 (3.75 KB)

 Non-trainable params: 0 (0.00 B)


</pre></div>
</div>
<p>Let’s compile and train this network:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">compile_model</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span>
                   <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span>
                   <span class="n">epochs</span> <span class="o">=</span> <span class="mi">200</span><span class="p">,</span>
                   <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">X_val</span><span class="p">,</span> <span class="n">y_val</span><span class="p">))</span>
<span class="n">plot_history</span><span class="p">(</span><span class="n">history</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;root_mean_squared_error&#39;</span><span class="p">,</span> <span class="s1">&#39;val_root_mean_squared_error&#39;</span><span class="p">])</span>
</pre></div>
</div>
<p><img alt="" class="align-center" src="_images/03_training_history_3_rmse_smaller_model.png" /></p>
<ol class="arabic simple">
<li><p>With this smaller model we have reduced overfitting a bit, since the training and validation loss are now closer to each other, and the validation loss does now reach a plateau and does not further increase.
We have not completely avoided overfitting though.</p></li>
<li><p>In the case of this small example model, the validation RMSE seems to end up around 3.2, which is much better than the 4.08 we had before. Note that you can double check the actual score by calling <code class="docutils literal notranslate"><span class="pre">model.evaluate()</span></code> on the test set.</p></li>
<li><p>In general, it quickly becomes a complicated search for the right “sweet spot”, i.e. the settings for which overfitting will be (nearly) avoided but the model still performs equally well. A model with 3 neurons in both layers seems to be around this spot, reaching an RMSE of 3.1 on the validation set.
Reducing the number of nodes further increases the validation RMSE again.</p></li>
</ol>
</div>
</div>
</div>
<div class="docutils">
<p>We saw that reducing the number of parameters can be a strategy to avoid overfitting.
In practice, however, this is usually not the (main) way to go when it comes to deep learning.
One reason is, that finding the sweet spot can be really hard and time consuming. And it has to be repeated every time the model is adapted, e.g. when more training data becomes available.</p>
<p class="rubric" id="early-stopping-stop-when-things-are-looking-best">Early stopping: stop when things are looking best</p>
<p>Arguable <strong>the</strong> most common technique to avoid (severe) overfitting in deep learning is called <strong>early stopping</strong>.
As the name suggests, this technique just means that you stop the model training if things do not seem to improve anymore.
More specifically, this usually means that the training is stopped if the validation loss does not (notably) improve anymore.
Early stopping is both intuitive and effective to use, so it has become a standard addition for model training.</p>
<p>To better study the effect, we can now safely go back to models with many (too many?) parameters:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">create_nn</span><span class="p">(</span><span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="n">X_data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],))</span>
<span class="n">compile_model</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
<p>To apply early stopping during training it is easiest to use Keras <code class="docutils literal notranslate"><span class="pre">EarlyStopping</span></code> class.
This allows to define the condition of when to stop training. In our case we will say when the validation loss is lowest.
However, since we have seen some fluctuation of the losses during training above we will also set <code class="docutils literal notranslate"><span class="pre">patience=10</span></code> which means that the model will stop training if the validation loss has not gone down for 10 epochs.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">tensorflow.keras.callbacks</span><span class="w"> </span><span class="kn">import</span> <span class="n">EarlyStopping</span>

<span class="n">earlystopper</span> <span class="o">=</span> <span class="n">EarlyStopping</span><span class="p">(</span>
    <span class="n">monitor</span><span class="o">=</span><span class="s1">&#39;val_loss&#39;</span><span class="p">,</span>
    <span class="n">patience</span><span class="o">=</span><span class="mi">10</span>
    <span class="p">)</span>

<span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span>
                    <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span>
                    <span class="n">epochs</span> <span class="o">=</span> <span class="mi">200</span><span class="p">,</span>
                    <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">X_val</span><span class="p">,</span> <span class="n">y_val</span><span class="p">),</span>
                    <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">earlystopper</span><span class="p">])</span>
</pre></div>
</div>
<p>As before, we can plot the losses during training:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">plot_history</span><span class="p">(</span><span class="n">history</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;root_mean_squared_error&#39;</span><span class="p">,</span> <span class="s1">&#39;val_root_mean_squared_error&#39;</span><span class="p">])</span>
</pre></div>
</div>
<p><img alt="" class="align-center" src="_images/03_training_history_3_rmse_early_stopping.png" /></p>
<p>This still seems to reveal the onset of overfitting, but the training stops before the discrepancy between training and validation loss can grow further.
In addition to avoiding severe cases of overfitting, early stopping has the advantage that the number of training epochs will be regulated automatically.</p>
<p>What might be a bit unintuitive is that the training runs might now end very rapidly.
This raises the question: have we really reached an optimum yet?
And often the answer to this is “no”, which is why early stopping frequently is combined with other approaches to avoid overfitting.
Overfitting means that a model (seemingly) performs better on seen data compared to unseen data. One then often also says that it does not “generalize” well.
Techniques to avoid overfitting, or to improve model generalization, are termed <strong>regularization techniques</strong> and we will come back to this in <strong>episode 4</strong>.</p>
<p class="rubric" id="batchnorm-the-standard-scaler-for-deep-learning">BatchNorm: the “standard scaler” for deep learning</p>
<p>A very common step in classical machine learning pipelines is to scale the features, for instance by using sckit-learn’s <code class="docutils literal notranslate"><span class="pre">StandardScaler</span></code>.
This can in principle also be done for deep learning.
An alternative, more common approach, is to add <strong>BatchNormalization</strong> layers (<a class="reference external" href="https://keras.io/api/layers/normalization_layers/batch_normalization/">documentation of the batch normalization layer</a>) which will learn how to scale the input values.
Similar to dropout, batch normalization is available as a network layer in Keras and can be added to the network in a similar way.
It does not require any additional parameter setting.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">BatchNormalization</span></code> can be inserted as yet another layer into the architecture.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">create_nn</span><span class="p">(</span><span class="n">input_shape</span><span class="p">):</span>
    <span class="c1"># Input layer</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="n">input_shape</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;input&#39;</span><span class="p">)</span>

    <span class="c1"># Dense layers</span>
    <span class="n">layers_dense</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">BatchNormalization</span><span class="p">()(</span><span class="n">inputs</span><span class="p">)</span> <span class="c1"># This is new!</span>
    <span class="n">layers_dense</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="s1">&#39;relu&#39;</span><span class="p">)(</span><span class="n">layers_dense</span><span class="p">)</span>
    <span class="n">layers_dense</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="s1">&#39;relu&#39;</span><span class="p">)(</span><span class="n">layers_dense</span><span class="p">)</span>

    <span class="c1"># Output layer</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">)(</span><span class="n">layers_dense</span><span class="p">)</span>

    <span class="c1"># Defining the model and compiling it</span>
    <span class="k">return</span> <span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;model_batchnorm&quot;</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">create_nn</span><span class="p">(</span><span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="n">X_data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],))</span>
<span class="n">compile_model</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
<p>This new layer appears in the model summary as well.</p>
<div class="highlight-output notranslate"><div class="highlight"><pre><span></span><span class="go">Model: &quot;model_batchnorm&quot;</span>

<span class="go">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓</span>
<span class="go">┃ Layer (type)                ┃ Output Shape        ┃       Param # ┃</span>
<span class="go">┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩</span>
<span class="go">│ input (InputLayer)          │ (None, 89)          │             0 │</span>
<span class="go">├─────────────────────────────┼─────────────────────┼───────────────┤</span>
<span class="go">│ batch_normalization_1       │ (None, 89)          │           356 │</span>
<span class="go">│ (BatchNormalization)        │                     │               │</span>
<span class="go">├─────────────────────────────┼─────────────────────┼───────────────┤</span>
<span class="go">│ dense_6 (Dense)             │ (None, 100)         │         9,000 │</span>
<span class="go">├─────────────────────────────┼─────────────────────┼───────────────┤</span>
<span class="go">│ dense_7 (Dense)             │ (None, 50)          │         5,050 │</span>
<span class="go">├─────────────────────────────┼─────────────────────┼───────────────┤</span>
<span class="go">│ dense_8 (Dense)             │ (None, 1)           │            51 │</span>
<span class="go">└─────────────────────────────┴─────────────────────┴───────────────┘</span>

<span class="go"> Total params: 14,457 (56.47 KB)</span>

<span class="go"> Trainable params: 14,279 (55.78 KB)</span>

<span class="go"> Non-trainable params: 178 (712.00 B)</span>
</pre></div>
</div>
<p>We can train the model again as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span>
                    <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span>
                    <span class="n">epochs</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
                    <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">X_val</span><span class="p">,</span> <span class="n">y_val</span><span class="p">),</span>
                    <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">earlystopper</span><span class="p">])</span>

<span class="n">plot_history</span><span class="p">(</span><span class="n">history</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;root_mean_squared_error&#39;</span><span class="p">,</span> <span class="s1">&#39;val_root_mean_squared_error&#39;</span><span class="p">])</span>
</pre></div>
</div>
<p><img alt="" class="align-center" src="_images/03_training_history_5_rmse_batchnorm.png" /></p>
<div class="admonition-batchnorm-parameters callout admonition" id="callout-2">
<p class="admonition-title">Batchnorm parameters</p>
<p>You may have noticed that the number of parameters of the Batchnorm layers corresponds to
4 parameters per input node.
These are the moving mean, moving standard deviation, additional scaling factor (gamma) and offset factor (beta).
There is a difference in behavior for Batchnorm between training and prediction time.
During training time, the data is scaled with the mean and standard deviation of the batch.
During prediction time, the moving mean and moving standard deviation of the training set is used instead.
The additional parameters gamma and beta are introduced to allow for more flexibility in output values, and are used in both training and prediction.</p>
</div>
</div>
</section>
<section id="run-on-test-set-and-compare-to-naive-baseline">
<h5>Run on test set and compare to naive baseline<a class="headerlink" href="#run-on-test-set-and-compare-to-naive-baseline" title="Link to this heading"></a></h5>
<p>It seems that no matter what we add, the overall loss does not decrease much further (we at least avoided overfitting though!).
Let us again plot the results on the test set:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">y_test_predicted</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">plot_predictions</span><span class="p">(</span><span class="n">y_test_predicted</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s1">&#39;Predictions on the test set&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p><img alt="" class="align-center" src="_images/03_regression_test_5_dropout_batchnorm.png" /></p>
<p>Well, the above is certainly not perfect. But how good or bad is this? Maybe not good enough to plan your picnic for tomorrow.
But let’s better compare it to the naive baseline we created in the beginning. What would you say, did we improve on that?</p>
<div class="admonition-exercise-simplify-the-model-and-add-data exercise important admonition" id="exercise-6">
<p class="admonition-title">Exercise: Simplify the model and add data</p>
<p>You may have been wondering why we are including weather observations from
multiple cities to predict sunshine hours only in Basel. The weather is
a complex phenomenon with correlations over large distances and time scales,
but what happens if we limit ourselves to only one city?</p>
<ol class="arabic simple">
<li><p>Since we will be reducing the number of features quite significantly,
we could afford to include more data. Instead of using only 3 years, use
8 or 9 years!</p></li>
<li><p>Only use the features in the dataset that are for Basel, remove the data for other cities.
You can use something like:</p></li>
</ol>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">cols</span> <span class="o">=</span> <span class="p">[</span><span class="n">c</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">X_data</span><span class="o">.</span><span class="n">columns</span> <span class="k">if</span> <span class="n">c</span><span class="p">[:</span><span class="mi">5</span><span class="p">]</span> <span class="o">==</span> <span class="s1">&#39;BASEL&#39;</span><span class="p">]</span>
<span class="n">X_data</span> <span class="o">=</span> <span class="n">X_data</span><span class="p">[</span><span class="n">cols</span><span class="p">]</span>
</pre></div>
</div>
<ol class="arabic simple" start="3">
<li><p>Now rerun the last model we defined which included the BatchNorm layer.
Recreate the scatter plot comparing your predictions with the true values,
and evaluate the model by computing the RMSE on the test score.
Note that even though we will use many more observations than previously,
the network should still train quickly because we reduce the number of
features (columns).
Is the prediction better compared to what we had before?</p></li>
<li><p>(Optional) Try to train a model on all years that are available,
and all features from all cities. How does it perform?</p></li>
</ol>
<div class="admonition-solution solution important dropdown admonition" id="solution-6">
<p class="admonition-title">Solution</p>
<p class="rubric" id="use-9-years-out-of-the-dataset">1. Use 9 years out of the dataset</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">nr_rows</span> <span class="o">=</span> <span class="mi">365</span><span class="o">*</span><span class="mi">9</span>
<span class="c1"># data</span>
<span class="n">X_data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">loc</span><span class="p">[:</span><span class="n">nr_rows</span><span class="p">]</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;DATE&#39;</span><span class="p">,</span> <span class="s1">&#39;MONTH&#39;</span><span class="p">])</span>

<span class="c1"># labels (sunshine hours the next day)</span>
<span class="n">y_data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="mi">1</span><span class="p">:(</span><span class="n">nr_rows</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)][</span><span class="s2">&quot;BASEL_sunshine&quot;</span><span class="p">]</span>
</pre></div>
</div>
<p class="rubric" id="only-use-features-for-basel">2. Only use features for Basel</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># only use columns with &#39;BASEL&#39;</span>
<span class="n">cols</span> <span class="o">=</span> <span class="p">[</span><span class="n">c</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">X_data</span><span class="o">.</span><span class="n">columns</span> <span class="k">if</span> <span class="n">c</span><span class="p">[:</span><span class="mi">5</span><span class="p">]</span> <span class="o">==</span> <span class="s1">&#39;BASEL&#39;</span><span class="p">]</span>
<span class="n">X_data</span> <span class="o">=</span> <span class="n">X_data</span><span class="p">[</span><span class="n">cols</span><span class="p">]</span>
</pre></div>
</div>
<p class="rubric" id="rerun-the-model-and-evaluate-it">3. Rerun the model and evaluate it</p>
<p>Do the train-test-validation split:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_holdout</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_holdout</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X_data</span><span class="p">,</span> <span class="n">y_data</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">X_val</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_val</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X_holdout</span><span class="p">,</span> <span class="n">y_holdout</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
<p>Create the network. We can re-use the <code class="docutils literal notranslate"><span class="pre">create_nn()</span></code> function that we already have. Because we have reduced the number of input features
the number of parameters in the network goes down from 14457 to 6137.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># create the network and view its summary</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">create_nn</span><span class="p">(</span><span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="n">X_data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],))</span>
<span class="n">compile_model</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
<p>Fit with early stopping and output showing performance on validation set:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span>
                   <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span>
                   <span class="n">epochs</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
                   <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">X_val</span><span class="p">,</span> <span class="n">y_val</span><span class="p">),</span>
                   <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">earlystopper</span><span class="p">],</span>
                   <span class="n">verbose</span> <span class="o">=</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">plot_history</span><span class="p">(</span><span class="n">history</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;root_mean_squared_error&#39;</span><span class="p">,</span> <span class="s1">&#39;val_root_mean_squared_error&#39;</span><span class="p">])</span>
</pre></div>
</div>
<p>Create a scatter plot to compare with true observations:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">y_test_predicted</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">plot_predictions</span><span class="p">(</span><span class="n">y_test_predicted</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s1">&#39;Predictions on the test set&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p><img alt="" class="align-center" src="_images/03_scatter_plot_basel_model.png" /></p>
<p>Compute the RMSE on the test set:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">test_metrics</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">return_dict</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Test RMSE: </span><span class="si">{</span><span class="n">test_metrics</span><span class="p">[</span><span class="s2">&quot;root_mean_squared_error&quot;</span><span class="p">]</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-output notranslate"><div class="highlight"><pre><span></span><span class="go">Test RMSE: 3.3761725425720215</span>
</pre></div>
</div>
<p>This RMSE is already a lot better compared to what we had before and certainly better than the baseline.
Additionally, it could be further improved with hyperparameter tuning.</p>
<p>Note that because we ran <code class="docutils literal notranslate"><span class="pre">train_test_split()</span></code> again, we are evaluating on a different test set than before.
In the real world it is important to always compare results on the exact same test set.</p>
<p class="rubric" id="optional-train-a-model-on-all-years-and-all-features-available">4. (optional) Train a model on all years and all features available.</p>
<p>You can tweak the above code to use all years and all features:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># We cannot take all rows, because we need to be able to take the sunshine hours of the next day</span>
<span class="n">nr_rows</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span> <span class="o">-</span> <span class="mi">2</span>

<span class="c1"># data</span>
<span class="n">X_data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">loc</span><span class="p">[:</span><span class="n">nr_rows</span><span class="p">]</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;DATE&#39;</span><span class="p">,</span> <span class="s1">&#39;MONTH&#39;</span><span class="p">])</span>

<span class="c1"># labels (sunshine hours the next day)</span>
<span class="n">y_data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="mi">1</span><span class="p">:(</span><span class="n">nr_rows</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)][</span><span class="s2">&quot;BASEL_sunshine&quot;</span><span class="p">]</span>
</pre></div>
</div>
<p>For the rest you can use the same code as above to train and evaluate the model</p>
<p>This results in an RMSE on the test set of 3.23 (your result can be different, but should be in the same range).
From this we can conclude that adding more training data results in even better performance!</p>
</div>
</div>
<div class="docutils">
<div class="admonition-tensorboard callout admonition" id="callout-3">
<p class="admonition-title">Tensorboard</p>
<p>If we run many different experiments with different architectures,
it can be difficult to keep track of these different models or compare the achieved performance.
We can use <em>tensorboard</em>, a framework that keeps track of our experiments and shows graphs like we plotted above.
Tensorboard is included in our tensorflow installation by default.
To use it, we first need to add a <em>callback</em> to our (compiled) model that saves the progress of training performance in a logs rectory:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">tensorflow.keras.callbacks</span><span class="w"> </span><span class="kn">import</span> <span class="n">TensorBoard</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">datetime</span>
<span class="n">log_dir</span> <span class="o">=</span> <span class="s2">&quot;logs/fit/&quot;</span> <span class="o">+</span> <span class="n">datetime</span><span class="o">.</span><span class="n">datetime</span><span class="o">.</span><span class="n">now</span><span class="p">()</span><span class="o">.</span><span class="n">strftime</span><span class="p">(</span><span class="s2">&quot;%Y%m</span><span class="si">%d</span><span class="s2">-%H%M%S&quot;</span><span class="p">)</span> <span class="c1"># You can adjust this to add a more meaningful model name</span>
<span class="n">tensorboard_callback</span> <span class="o">=</span> <span class="n">TensorBoard</span><span class="p">(</span><span class="n">log_dir</span><span class="o">=</span><span class="n">log_dir</span><span class="p">,</span> <span class="n">histogram_freq</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span>
                   <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span>
                   <span class="n">epochs</span> <span class="o">=</span> <span class="mi">200</span><span class="p">,</span>
                   <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">X_val</span><span class="p">,</span> <span class="n">y_val</span><span class="p">),</span>
                   <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">tensorboard_callback</span><span class="p">],</span>
                   <span class="n">verbose</span> <span class="o">=</span> <span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
<p>You can launch the tensorboard interface from a Jupyter notebook, showing all trained models:</p>
<!--cce:skip-->
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="n">load_ext</span> <span class="n">tensorboard</span>
<span class="o">%</span><span class="n">tensorboard</span> <span class="o">--</span><span class="n">logdir</span> <span class="n">logs</span><span class="o">/</span><span class="n">fit</span>
</pre></div>
</div>
<p>Which will show an interface that looks something like this:
<img alt="" class="align-center" src="_images/03_tensorboard.png" /></p>
</div>
</div>
</section>
</section>
<section id="save-model">
<h4>10. Save model<a class="headerlink" href="#save-model" title="Link to this heading"></a></h4>
<p>Now that we have a somewhat acceptable model, let us not forget to save it for future users to benefit from our explorative efforts!</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s1">&#39;my_tuned_weather_model.keras&#39;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="outlook">
<h4>Outlook<a class="headerlink" href="#outlook" title="Link to this heading"></a></h4>
<p>Correctly predicting tomorrow’s sunshine hours is apparently not that simple.
Our models get the general trends right, but still predictions vary quite a bit and can even be far off.</p>
<div class="admonition-open-question-what-could-be-next-steps-to-further-improve-the-model exercise important admonition" id="exercise-7">
<p class="admonition-title">Open question: What could be next steps to further improve the model?</p>
<p>With unlimited options to modify the model architecture or to play with the training parameters, deep learning can trigger very extensive hunting for better and better results.
Usually models are “well behaving” in the sense that small changes to the architectures also only result in small changes of the performance (if any).
It is often tempting to hunt for some magical settings that will lead to much better results. But do those settings exist?
Applying common sense is often a good first step to make a guess of how much better results <em>could</em> be.
In the present case we might certainly not expect to be able to reliably predict sunshine hours for the next day with 5-10 minute precision.
But how much better our model could be exactly, often remains difficult to answer.</p>
<ul class="simple">
<li><p>What changes to the model architecture might make sense to explore?</p></li>
<li><p>Ignoring changes to the model architecture, what might notably improve the prediction quality?</p></li>
</ul>
<div class="admonition-solution solution important dropdown admonition" id="solution-7">
<p class="admonition-title">Solution</p>
<p>This is an open question. And we don’t actually know how far one could push this sunshine hour prediction (try it out yourself if you like! We’re curious!).
But there are a few things that might be worth exploring.
Regarding the model architecture:</p>
<ul class="simple">
<li><p>In the present case we do not see a magical silver bullet to suddenly boost the performance. But it might be worth testing if <em>deeper</em> networks do better (more layers).</p></li>
</ul>
<p>Other changes that might impact the quality notably:</p>
<ul class="simple">
<li><p>The most obvious answer here would be: more data! Even this will not always work (e.g. if data is very noisy and uncorrelated, more data might not add much).</p></li>
<li><p>Related to more data: use data augmentation. By creating realistic variations of the available data, the model might improve as well.</p></li>
<li><p>More data can mean more data points (you can test it yourself by taking more than the 3 years we used here!)</p></li>
<li><p>More data can also mean more features! What about adding the month?</p></li>
<li><p>The labels we used here (sunshine hours) are highly biased, many days with no or nearly no sunshine but a few with &gt;10 hours. Techniques such as oversampling or undersampling might handle such biased labels better.</p></li>
</ul>
<p>Another alternative would be to not only look at data from one day, but use the data of a longer period such as a full week.
This will turn the data into time series data which in turn might also make it worth to apply different model architectures…</p>
</div>
</div>
<div class="docutils">
<div class="admonition-keypoints keypoints admonition" id="keypoints-0">
<p class="admonition-title">Keypoints</p>
<ul class="simple">
<li><p>Separate training, validation, and test sets allows monitoring and evaluating your model.</p></li>
<li><p>Batchnormalization scales the data as part of the model.</p></li>
</ul>
</div>
</div>
</section>
</section>
<span id="document-4-advanced-layer-types"></span><section class="tex2jax_ignore mathjax_ignore" id="advanced-layer-types">
<h3>4. Advanced layer types<a class="headerlink" href="#advanced-layer-types" title="Link to this heading"></a></h3>
<div class="admonition-questions questions admonition" id="questions-0">
<p class="admonition-title">Questions</p>
<ul class="simple">
<li><p>Why do we need different types of layers?</p></li>
<li><p>What are good network designs for image data?</p></li>
<li><p>What is a convolutional layer?</p></li>
<li><p>How can we use different types of layers to prevent overfitting?</p></li>
<li><p>What is hyperparameter tuning?</p></li>
</ul>
</div>
<div class="admonition-objectives objectives admonition" id="objectives-0">
<p class="admonition-title">Objectives</p>
<ul class="simple">
<li><p>Understand why convolutional and pooling layers are useful for image data</p></li>
<li><p>Implement a convolutional neural network on an image dataset</p></li>
<li><p>Use a dropout layer to prevent overfitting</p></li>
<li><p>Be able to tune the hyperparameters of a Keras model</p></li>
</ul>
</div>
<section id="different-types-of-layers">
<h4>Different types of layers<a class="headerlink" href="#different-types-of-layers" title="Link to this heading"></a></h4>
<p>Networks are like onions: a typical neural network consists of many layers. In fact, the word <em>deep</em> in <em>deep learning</em>
refers to the many layers that make the network deep.</p>
<p>So far, we have seen one type of layer, namely the <strong>fully connected</strong>, or <strong>dense</strong> layer. This layer is called fully connected, because all input neurons are taken into account by each output neuron. The number of parameters that need to be learned by the network, is thus in the order of magnitude of the number of input neurons times the number of hidden neurons.</p>
<p>However, there are many different types of layers that perform different calculations and take different inputs. In this episode we will take a look at <strong>convolutional layers</strong> and <strong>dropout layers</strong>, which are useful in the context of image data, but also in many other types of (structured) data.</p>
</section>
<section id="formulate-outline-the-problem-image-classification">
<h4>1. Formulate / Outline the problem: Image classification<a class="headerlink" href="#formulate-outline-the-problem-image-classification" title="Link to this heading"></a></h4>
<p>The <a class="reference external" href="https://www.kaggle.com/datasets/mlcommons/the-dollar-street-dataset">MLCommons Dollar Street Dataset</a> is a collection of images of everyday household items from homes around the world that visually captures socioeconomic diversity of traditionally underrepresented populations. We use <a class="reference external" href="https://zenodo.org/records/10970014">a subset of the original dataset</a> that can be used for multiclass classification with 10 categories. Let’s load the data:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">pathlib</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="n">DATA_FOLDER</span> <span class="o">=</span> <span class="n">pathlib</span><span class="o">.</span><span class="n">Path</span><span class="p">(</span><span class="s1">&#39;data/dataset_dollarstreet/&#39;</span><span class="p">)</span> <span class="c1"># change to location where you stored the data</span>
<span class="n">train_images</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">DATA_FOLDER</span> <span class="o">/</span> <span class="s1">&#39;train_images.npy&#39;</span><span class="p">)</span>
<span class="n">val_images</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">DATA_FOLDER</span> <span class="o">/</span> <span class="s1">&#39;test_images.npy&#39;</span><span class="p">)</span>
<span class="n">train_labels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">DATA_FOLDER</span> <span class="o">/</span> <span class="s1">&#39;train_labels.npy&#39;</span><span class="p">)</span>
<span class="n">val_labels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">DATA_FOLDER</span> <span class="o">/</span> <span class="s1">&#39;test_labels.npy&#39;</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition-a-note-about-data-provenance callout admonition" id="callout-0">
<p class="admonition-title">A note about data provenance</p>
<p>In an earlier version, this part of the lesson used a different example dataset.
During <a class="reference external" href="https://github.com/carpentries-lab/reviews/issues/25#issuecomment-1953271802">peer review</a>, the decision was made to replace that dataset due to the way it had been compiled using images “scraped” from the internet without permission from or credit to the original creators of those images. Unfortunately, uncredited use of images is a common problem among datasets used to benchmark models for image classification.</p>
<p>The Dollar Street dataset was chosen for use in the lesson as it contains only images <a class="reference external" href="https://www.gapminder.org/dollar-street/about?">created by the Gapminder project</a> for the purposes of using them in the dataset.
The original Dollar Street dataset is very large – more than 100 GB – with the potential to grow even bigger, so we created a subset for use in this lesson.</p>
</div>
<section id="dollar-street-10-dataset">
<h5>Dollar Street 10 dataset<a class="headerlink" href="#dollar-street-10-dataset" title="Link to this heading"></a></h5>
<p>The Dollar Street 10 dataset consists of images of 10 different classes, this is the mapping of the categories:</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Category</p></th>
<th class="head"><p>label</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>day bed</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-odd"><td><p>dishrag</p></td>
<td><p>1</p></td>
</tr>
<tr class="row-even"><td><p>plate</p></td>
<td><p>2</p></td>
</tr>
<tr class="row-odd"><td><p>running shoe</p></td>
<td><p>3</p></td>
</tr>
<tr class="row-even"><td><p>soap dispenser</p></td>
<td><p>4</p></td>
</tr>
<tr class="row-odd"><td><p>street sign</p></td>
<td><p>5</p></td>
</tr>
<tr class="row-even"><td><p>table lamp</p></td>
<td><p>6</p></td>
</tr>
<tr class="row-odd"><td><p>tile roof</p></td>
<td><p>7</p></td>
</tr>
<tr class="row-even"><td><p>toilet seat</p></td>
<td><p>8</p></td>
</tr>
<tr class="row-odd"><td><p>washing machine</p></td>
<td><p>9</p></td>
</tr>
</tbody>
</table>
<div class="admonition-framing-the-classification-task instructor dropdown admonition" id="instructor-0">
<p class="admonition-title">Framing the classification task</p>
<p>The sample images from the dataset, shown below, provide a good opportunity to lead a discussion with learners about the nature of the images and the classification task we will be training a model to perform.
For example, although the images can all be assumed to include the object they are labelled with, not all images are <em>of</em> those objects i.e. the object is one of several present in the image.
This makes the task of the classifier more difficult, as does the more culturally diverse set of objects present in the image, but both of these properties make the trained model more robust.
After training, we can consider ourselves to be asking the model “which of these ten objects is present in this image?”, as opposed to e.g. “which of these ten objects is this an image of?”</p>
</div>
<p><img alt="" class="align-center" src="_images/04_dollar_street_10.png" /></p>
<div class="note docutils">
<p>Sample images from the Dollar Street 10 dataset. Each image is labelled with a category, for example: ‘street sign’ or ‘soap dispenser’</p>
</div>
</section>
</section>
<section id="identify-inputs-and-outputs">
<h4>2. Identify inputs and outputs<a class="headerlink" href="#identify-inputs-and-outputs" title="Link to this heading"></a></h4>
<section id="explore-the-data">
<h5>Explore the data<a class="headerlink" href="#explore-the-data" title="Link to this heading"></a></h5>
<p>Let’s do a quick exploration of the dimensions of the data:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">train_images</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
<div class="highlight-output notranslate"><div class="highlight"><pre><span></span><span class="go">(878, 64, 64, 3)</span>
</pre></div>
</div>
<p>The first value, <code class="docutils literal notranslate"><span class="pre">878</span></code>, is the number of training images in the dataset.
The remainder of the shape, namely <code class="docutils literal notranslate"><span class="pre">(64,</span> <span class="pre">64,</span> <span class="pre">3)</span></code>, denotes
the dimension of one image. The last value 3 is typical for color images,
and stands for the three color channels <strong>R</strong>ed, <strong>G</strong>reen, <strong>B</strong>lue.</p>
<div class="admonition-number-of-features-in-dollar-street-10 exercise important admonition" id="exercise-0">
<p class="admonition-title">Number of features in Dollar Street 10</p>
<p>How many features does one image in the Dollar Street 10 dataset have?</p>
<ul class="simple">
<li><p>A. 64</p></li>
<li><p>B. 4096</p></li>
<li><p>C. 12288</p></li>
<li><p>D. 878</p></li>
</ul>
</div>
<div class="admonition-solution solution important dropdown admonition" id="solution-0">
<p class="admonition-title">Solution</p>
<p>The correct solution is C: 12288</p>
<p>There are 4096 pixels in one image (64 * 64), each pixel has 3 channels (RGB). So 4096 * 3 = 12288.</p>
</div>
<p>We can find out the range of values of our input data as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">train_images</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">train_images</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-output notranslate"><div class="highlight"><pre><span></span><span class="go">(0, 255)</span>
</pre></div>
</div>
<p>So the values of the three channels range between <code class="docutils literal notranslate"><span class="pre">0</span></code> and <code class="docutils literal notranslate"><span class="pre">255</span></code>.
Lastly, we inspect the dimension of the labels:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">train_labels</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
<div class="highlight-output notranslate"><div class="highlight"><pre><span></span><span class="go">(878,)</span>
</pre></div>
</div>
<p>So we have, for each image, a single value denoting the label.
To find out what the possible values of these labels are:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">train_labels</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">train_labels</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-output notranslate"><div class="highlight"><pre><span></span><span class="go">(0, 9)</span>
</pre></div>
</div>
<p>The values of the labels range between <code class="docutils literal notranslate"><span class="pre">0</span></code> and <code class="docutils literal notranslate"><span class="pre">9</span></code>, denoting 10 different classes.</p>
</section>
</section>
<section id="prepare-data">
<h4>3. Prepare data<a class="headerlink" href="#prepare-data" title="Link to this heading"></a></h4>
<p>The training set consists of 878 images of <code class="docutils literal notranslate"><span class="pre">64x64</span></code> pixels and 3 channels (RGB values). The RGB values are between <code class="docutils literal notranslate"><span class="pre">0</span></code> and <code class="docutils literal notranslate"><span class="pre">255</span></code>. For input of neural networks, it is better to have small input values. So we normalize our data between <code class="docutils literal notranslate"><span class="pre">0</span></code> and <code class="docutils literal notranslate"><span class="pre">1</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">train_images</span> <span class="o">=</span> <span class="n">train_images</span> <span class="o">/</span> <span class="mf">255.0</span>
<span class="n">val_images</span> <span class="o">=</span> <span class="n">val_images</span> <span class="o">/</span> <span class="mf">255.0</span>
</pre></div>
</div>
</section>
<section id="choose-a-pretrained-model-or-start-building-architecture-from-scratch">
<h4>4. Choose a pretrained model or start building architecture from scratch<a class="headerlink" href="#choose-a-pretrained-model-or-start-building-architecture-from-scratch" title="Link to this heading"></a></h4>
<section id="convolutional-layers">
<h5>Convolutional layers<a class="headerlink" href="#convolutional-layers" title="Link to this heading"></a></h5>
<p>In the previous episodes, we used ‘fully connected layers’ , that connected all input values of a layer to all outputs of a layer.
This results in many connections, and thus many weights to be learned, in the network.
Note that our input dimension is now quite high (even with small pictures of <code class="docutils literal notranslate"><span class="pre">64x64</span></code> pixels): we have 12288 features.</p>
<div class="admonition-number-of-parameters-parameters-exercise-1 exercise important admonition" id="exercise-1">
<p class="admonition-title">Number of parameters{#parameters-exercise-1}</p>
<p>Suppose we create a single Dense (fully connected) layer with 100 hidden units that connect to the input pixels, how many parameters does this layer have?</p>
<ul class="simple">
<li><p>A. 1228800</p></li>
<li><p>B. 1228900</p></li>
<li><p>C. 100</p></li>
<li><p>D. 12288</p></li>
</ul>
<div class="admonition-solution solution important dropdown admonition" id="solution-1">
<p class="admonition-title">Solution</p>
<p>The correct answer is B: Each entry of the input dimensions, i.e. the <code class="docutils literal notranslate"><span class="pre">shape</span></code> of one single data point, is connected with 100 neurons of our hidden layer, and each of these neurons has a bias term associated to it. So we have <code class="docutils literal notranslate"><span class="pre">1228900</span></code> parameters to learn.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">width</span><span class="p">,</span> <span class="n">height</span> <span class="o">=</span> <span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span>
<span class="n">n_hidden_neurons</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">n_bias</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">n_input_items</span> <span class="o">=</span> <span class="n">width</span> <span class="o">*</span> <span class="n">height</span> <span class="o">*</span> <span class="mi">3</span>
<span class="n">n_parameters</span> <span class="o">=</span> <span class="p">(</span><span class="n">n_input_items</span> <span class="o">*</span> <span class="n">n_hidden_neurons</span><span class="p">)</span> <span class="o">+</span> <span class="n">n_bias</span>
<span class="n">n_parameters</span>
</pre></div>
</div>
<div class="highlight-output notranslate"><div class="highlight"><pre><span></span><span class="go">1228900</span>
</pre></div>
</div>
<p>We can also check this by building the layer in Keras:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">inputs</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">n_input_items</span><span class="p">,))</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">)(</span><span class="n">inputs</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-output notranslate"><div class="highlight"><pre><span></span><span class="go">Model: &quot;functional&quot;</span>

<span class="go">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┓</span>
<span class="go">┃ Layer (type)                 ┃ Output Shape         ┃     Param # ┃</span>
<span class="go">┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━┩</span>
<span class="go">│ input_layer (InputLayer)     │ (None, 12288)        │           0 │</span>
<span class="go">├──────────────────────────────┼──────────────────────┼─────────────┤</span>
<span class="go">│ dense (Dense)                │ (None, 100)          │   1,228,900 │</span>
<span class="go">└──────────────────────────────┴──────────────────────┴─────────────┘</span>

<span class="go"> Total params: 1,228,900 (4.69 MB)</span>

<span class="go"> Trainable params: 1,228,900 (4.69 MB)</span>

<span class="go"> Non-trainable params: 0 (0.00 B)</span>
</pre></div>
</div>
</div>
</div>
<div class="docutils">
<p>We can decrease the number of units in our hidden layer, but this also decreases the number of patterns our network can remember. Moreover, if we increase the image size, the number of weights will ‘explode’, even though the task of recognizing large images is not necessarily more difficult than the task of recognizing small images.</p>
<p>The solution is that we make the network learn in a ‘smart’ way. The features that we learn should be similar both for small and large images, and similar features (e.g. edges, corners) can appear anywhere in the image (in mathematical terms: <em>translation invariant</em>). We do this by making use of a concept from image processing that predates deep learning.</p>
<p>A <strong>convolution matrix</strong>, or <strong>kernel</strong>, is a matrix transformation that we ‘slide’ over the image to calculate features at each position of the image. For each pixel, we calculate the matrix product between the kernel and the pixel with its surroundings. A kernel is typically small, between 3x3 and 7x7 pixels. We can for example think of the 3x3 kernel:</p>
<div class="highlight-output notranslate"><div class="highlight"><pre><span></span><span class="go">[[-1, -1, -1],</span>
<span class="go"> [0, 0, 0]</span>
<span class="go"> [1, 1, 1]]</span>
</pre></div>
</div>
<p>This kernel will give a high value to a pixel if it is on a horizontal border between dark and light areas.
Note that for RGB images, the kernel should also have a depth of 3.</p>
<p>In the following image, we see the effect of such a kernel on the values of a single-channel image. The red cell in the output matrix is the result of multiplying and summing the values of the red square in the input, and the kernel. Applying this kernel to a real image shows that it indeed detects horizontal edges.</p>
<p><img alt="" class="align-center" src="_images/04_conv_matrix.png" /></p>
<p><img alt="" class="align-center" src="_images/04_conv_image.png" /></p>
<p>In our <strong>convolutional layer</strong> our hidden units are a number of convolutional matrices (or kernels), where the values of the matrices are the weights that we learn in the training process. The output of a convolutional layer is an ‘image’ for each of the kernels, that gives the output of the kernel applied to each pixel.</p>
<div class="admonition-playing-with-convolutions callout admonition" id="callout-1">
<p class="admonition-title">Playing with convolutions</p>
<p>Convolutions applied to images can be hard to grasp at first. Fortunately there are resources out
there that enable users to interactively play around with images and convolutions:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://setosa.io/ev/image-kernels/">Image kernels explained</a> shows how different convolutions can achieve certain effects on an image, like sharpening and blurring.</p></li>
<li><p><a class="reference external" href="https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-convolutional-neural-networks#">The convolutional neural network cheat sheet</a>
shows animated examples of the different components of convolutional neural nets</p></li>
</ul>
</div>
</div>
<div class="admonition-border-pixels exercise important admonition" id="exercise-2">
<p class="admonition-title">Border pixels</p>
<p>What, do you think, happens to the border pixels when applying a convolution?</p>
<div class="admonition-solution solution important dropdown admonition" id="solution-2">
<p class="admonition-title">Solution</p>
<p>There are different ways of dealing with border pixels.
You can ignore them, which means that your output image is slightly smaller then your input.
It is also possible to ‘pad’ the borders, e.g. with the same value or with zeros, so that the convolution can also be applied to the border pixels.
In that case, the output image will have the same size as the input image.</p>
<p><a class="reference external" href="https://datacarpentry.org/image-processing/06-blurring.html#callout4">This callout in the Data Carpentry: Image Processing with Python curriculum</a>
provides more detail about convolution at the boundaries of an image,
in the context of applying a <em>Gaussian blur</em>.</p>
</div>
</div>
<div class="docutils">
<div class="admonition-number-of-model-parameters exercise important admonition" id="exercise-3">
<p class="admonition-title">Number of model parameters</p>
<p>Suppose we apply a convolutional layer with 100 kernels of size 3 * 3 * 3 (the last dimension applies to the rgb channels) to our images of 32 * 32 * 3 pixels. How many parameters do we have? Assume, for simplicity, that the kernels do not use bias terms. Compare this to the answer of the earlier exercise, <a class="reference internal" href="#parameters-exercise-1"><span class="xref myst">“Number of Parameters”</span></a>.</p>
<div class="admonition-solution solution important dropdown admonition" id="solution-3">
<p class="admonition-title">Solution</p>
<p>We have 100 matrices with 3 * 3 * 3 = 27 values each so that gives 27 * 100 = 2700 weights. This is a magnitude of 2000 less than the fully connected layer with 100 units! Nevertheless, as we will see, convolutional networks work very well for image data. This illustrates the expressiveness of convolutional layers.</p>
</div>
</div>
</div>
<div class="docutils">
<p>So let us look at a network with a few convolutional layers. We need to finish with a Dense layer to connect the output cells of the convolutional layer to the outputs for our classes.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">inputs</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="n">train_images</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">)(</span><span class="n">inputs</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Flatten</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;dollar_street_model_small&quot;</span><span class="p">)</span>

<span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-output notranslate"><div class="highlight"><pre><span></span><span class="go">Model: &quot;dollar_street_model_small&quot;</span>

<span class="go">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┓</span>
<span class="go">┃ Layer (type)                 ┃ Output Shape         ┃     Param # ┃</span>
<span class="go">┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━┩</span>
<span class="go">│ input_layer_1 (InputLayer)   │ (None, 64, 64, 3)    │           0 │</span>
<span class="go">├──────────────────────────────┼──────────────────────┼─────────────┤</span>
<span class="go">│ conv2d (Conv2D)              │ (None, 62, 62, 50)   │       1,400 │</span>
<span class="go">├──────────────────────────────┼──────────────────────┼─────────────┤</span>
<span class="go">│ conv2d_1 (Conv2D)            │ (None, 60, 60, 50)   │      22,550 │</span>
<span class="go">├──────────────────────────────┼──────────────────────┼─────────────┤</span>
<span class="go">│ flatten (Flatten)            │ (None, 180000)       │           0 │</span>
<span class="go">├──────────────────────────────┼──────────────────────┼─────────────┤</span>
<span class="go">│ dense_1 (Dense)              │ (None, 10)           │   1,800,010 │</span>
<span class="go">└──────────────────────────────┴──────────────────────┴─────────────┘</span>

<span class="go"> Total params: 1,823,960 (6.96 MB)</span>

<span class="go"> Trainable params: 1,823,960 (6.96 MB)</span>

<span class="go"> Non-trainable params: 0 (0.00 B)</span>
</pre></div>
</div>
<div class="admonition-convolutional-neural-network exercise important admonition" id="exercise-4">
<p class="admonition-title">Convolutional Neural Network</p>
<p>Inspect the network above:</p>
<ul class="simple">
<li><p>What do you think is the function of the <code class="docutils literal notranslate"><span class="pre">Flatten</span></code> layer?</p></li>
<li><p>Which layer has the most parameters? Do you find this intuitive?</p></li>
<li><p>(optional) This dataset is similar to the often used CIFAR-10 dataset.
We can get inspiration for neural network architectures that could work on our dataset here: https://paperswithcode.com/sota/image-classification-on-cifar-10 . Pick a model and try to understand how it works.</p></li>
</ul>
<div class="admonition-solution solution important dropdown admonition" id="solution-4">
<p class="admonition-title">Solution</p>
<ul class="simple">
<li><p>The Flatten layer converts the 60x60x50 output of the convolutional layer into a single one-dimensional vector, that can be used as input for a dense layer.</p></li>
<li><p>The last dense layer has the most parameters. This layer connects every single output ‘pixel’ from the convolutional layer to the 10 output classes.
That results in a large number of connections, so a large number of parameters. This undermines a bit the expressiveness of the convolutional layers, that have much fewer parameters.</p></li>
</ul>
</div>
</div>
</div>
<div class="docutils">
<div class="admonition-search-for-existing-architectures-or-pretrained-models callout admonition" id="callout-2">
<p class="admonition-title">Search for existing architectures or pretrained models</p>
<p>So far in this course we have built neural networks from scratch, because we want you to fully understand the basics of Keras.
In the real world however, you would first search for existing solutions to your problem.</p>
<p>You could for example search for ‘large CNN image classification Keras implementation’, and see if you can find any Keras implementations
of more advanced architectures that you could reuse.
A lot of the best-performing architectures for image classification are convolutional neural networks or at least have some elements in common.
Therefore, we will introduce convolutional neural networks here, and the best way to teach you is by
developing a neural network from scratch!</p>
</div>
</div>
<div class="admonition-demonstrate-searching-for-existing-architectures instructor dropdown admonition" id="instructor-1">
<p class="admonition-title">Demonstrate searching for existing architectures</p>
<p>At this point it can be nice to apply above callout box and demonstrate searching for state-of-the-art implementations.
If you google for ‘large CNN image classification Keras implementation’ one of the top search results links to <a class="reference external" href="https://keras.io/examples/vision/image_classification_from_scratch/">an example from the Keras documentation for a small version of the Xception model</a>.</p>
<p>It can be a nice learning opportunity to go through the notebook and show that the learners should
already be familiar with a lot of the syntax (for example Conv2D, Dense, BatchNorm layers, adam optimizer, the deep learning workflow).
You can show that even though the model is much deeper, the input and output layer are still the same.
The aim is to demonstrate that what we are learning is really the basis for more complex models,
and you do not need to reinvent the wheel.</p>
</div>
</section>
<section id="pooling-layers">
<h5>Pooling layers<a class="headerlink" href="#pooling-layers" title="Link to this heading"></a></h5>
<p>Often in convolutional neural networks, the convolutional layers are intertwined with <strong>Pooling layers</strong>. As opposed to the convolutional layer, the pooling layer actually alters the dimensions of the image and reduces it by a scaling factor. It is basically decreasing the resolution of your picture. The rationale behind this is that higher layers of the network should focus on higher-level features of the image. By introducing a pooling layer, the subsequent convolutional layer has a broader ‘view’ on the original image.</p>
<p>Let’s put it into practice. We compose a Convolutional network with two convolutional layers and two pooling layers.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">create_nn</span><span class="p">():</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="n">train_images</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">)(</span><span class="n">inputs</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">MaxPooling2D</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))(</span><span class="n">x</span><span class="p">)</span> <span class="c1"># a new maxpooling layer</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">MaxPooling2D</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))(</span><span class="n">x</span><span class="p">)</span> <span class="c1"># a new maxpooling layer (same as maxpool)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Flatten</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span> <span class="c1"># a new Dense layer</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>

    <span class="n">model</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;dollar_street_model&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">model</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">create_nn</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-output notranslate"><div class="highlight"><pre><span></span><span class="go">Model: &quot;dollar_street_model&quot;</span>

<span class="go">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┓</span>
<span class="go">┃ Layer (type)                 ┃ Output Shape         ┃     Param # ┃</span>
<span class="go">┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━┩</span>
<span class="go">│ input_layer_2 (InputLayer)   │ (None, 64, 64, 3)    │           0 │</span>
<span class="go">├──────────────────────────────┼──────────────────────┼─────────────┤</span>
<span class="go">│ conv2d_2 (Conv2D)            │ (None, 62, 62, 50)   │       1,400 │</span>
<span class="go">├──────────────────────────────┼──────────────────────┼─────────────┤</span>
<span class="go">│ max_pooling2d (MaxPooling2D) │ (None, 31, 31, 50)   │           0 │</span>
<span class="go">├──────────────────────────────┼──────────────────────┼─────────────┤</span>
<span class="go">│ conv2d_3 (Conv2D)            │ (None, 29, 29, 50)   │      22,550 │</span>
<span class="go">├──────────────────────────────┼──────────────────────┼─────────────┤</span>
<span class="go">│ max_pooling2d_1              │ (None, 14, 14, 50)   │           0 │</span>
<span class="go">│ (MaxPooling2D)               │                      │             │</span>
<span class="go">├──────────────────────────────┼──────────────────────┼─────────────┤</span>
<span class="go">│ flatten_1 (Flatten)          │ (None, 9800)         │           0 │</span>
<span class="go">├──────────────────────────────┼──────────────────────┼─────────────┤</span>
<span class="go">│ dense_2 (Dense)              │ (None, 50)           │     490,050 │</span>
<span class="go">├──────────────────────────────┼──────────────────────┼─────────────┤</span>
<span class="go">│ dense_3 (Dense)              │ (None, 10)           │         510 │</span>
<span class="go">└──────────────────────────────┴──────────────────────┴─────────────┘</span>

<span class="go"> Total params: 514,510 (1.96 MB)</span>

<span class="go"> Trainable params: 514,510 (1.96 MB)</span>

<span class="go"> Non-trainable params: 0 (0.00 B)</span>
</pre></div>
</div>
</section>
</section>
<section id="choose-a-loss-function-and-optimizer">
<h4>5. Choose a loss function and optimizer<a class="headerlink" href="#choose-a-loss-function-and-optimizer" title="Link to this heading"></a></h4>
<p>We compile the model using the adam optimizer (other optimizers could also be used here!).
Similar to the penguin classification task, we will use the crossentropy function to calculate the model’s loss.
This loss function is appropriate to use when the data has two or more label classes.</p>
<p>Remember that our target class is represented by a single integer, whereas the output of our network has 10 nodes, one for each class.
So, we should have actually one-hot encoded the targets and used a softmax activation for the neurons in our output layer!
Luckily, there is a quick fix to calculate crossentropy loss for data that
has its classes represented by integers, the <code class="docutils literal notranslate"><span class="pre">SparseCategoricalCrossentropy()</span></code> function.
Adding the argument <code class="docutils literal notranslate"><span class="pre">from_logits=True</span></code> accounts for the fact that the output has a linear activation instead of softmax.
This is what is often done in practice, because it spares you from having to worry about one-hot encoding.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">compile_model</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
    <span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;adam&#39;</span><span class="p">,</span>
                  <span class="n">loss</span><span class="o">=</span><span class="n">keras</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">SparseCategoricalCrossentropy</span><span class="p">(</span><span class="n">from_logits</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
                  <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">])</span>
<span class="n">compile_model</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition-break instructor dropdown admonition" id="instructor-2">
<p class="admonition-title">BREAK</p>
<p>This is a good time for switching instructor and/or a break.</p>
</div>
</section>
<section id="train-the-model">
<h4>6. Train the model<a class="headerlink" href="#train-the-model" title="Link to this heading"></a></h4>
<p>We then train the model for 10 epochs:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_images</span><span class="p">,</span> <span class="n">train_labels</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                    <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">val_images</span><span class="p">,</span> <span class="n">val_labels</span><span class="p">))</span>
</pre></div>
</div>
</section>
<section id="perform-a-prediction-classification">
<h4>7. Perform a Prediction/Classification<a class="headerlink" href="#perform-a-prediction-classification" title="Link to this heading"></a></h4>
<p>Here we skip performing a prediction, and continue to measuring the performance.
In practice, you will only do this step once in a while when you actually need to have the individual predictions,
often you know enough based on the evaluation metric scores.
Of course, behind the scenes whenever you measure performance you have to make predictions and compare them to the ground truth.</p>
</section>
<section id="measure-performance">
<h4>8. Measure performance<a class="headerlink" href="#measure-performance" title="Link to this heading"></a></h4>
<p>We can plot the training process using the history:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">seaborn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">sns</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>

<span class="k">def</span><span class="w"> </span><span class="nf">plot_history</span><span class="p">(</span><span class="n">history</span><span class="p">,</span> <span class="n">metrics</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Plot the training history</span>

<span class="sd">    Args:</span>
<span class="sd">        history (keras History object that is returned by model.fit())</span>
<span class="sd">        metrics(str, list): Metric or a list of metrics to plot</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">history_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="o">.</span><span class="n">from_dict</span><span class="p">(</span><span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">)</span>
    <span class="n">sns</span><span class="o">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">history_df</span><span class="p">[</span><span class="n">metrics</span><span class="p">])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;epochs&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;metric&quot;</span><span class="p">)</span>
<span class="n">plot_history</span><span class="p">(</span><span class="n">history</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">,</span> <span class="s1">&#39;val_accuracy&#39;</span><span class="p">])</span>
</pre></div>
</div>
<p><img alt="" class="align-center" src="_images/04_training_history_1.png" /></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">plot_history</span><span class="p">(</span><span class="n">history</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;loss&#39;</span><span class="p">,</span> <span class="s1">&#39;val_loss&#39;</span><span class="p">])</span>
</pre></div>
</div>
<p><img alt="" class="align-center" src="_images/04_training_history_loss_1.png" /></p>
<p>It seems that the model is overfitting a lot, because the training accuracy increases, while the validation accuracy stagnates. Meanwhile, the training loss keeps decreasing while the validation loss actually starts increasing after a few epochs.</p>
<div class="admonition-comparison-with-a-network-with-only-dense-layers instructor dropdown admonition" id="instructor-3">
<p class="admonition-title">Comparison with a network with only dense layers</p>
<p>The callout box below compares the CNN approach with a network with only dense layers.
Depending on time, the following discussion can be extended in depth up to your liking. You have several options:</p>
<ol class="arabic simple">
<li><p>It can be used as a good recap exercise. The exercise question is then:
‘How does this simple CNN compare to a neural network with only dense layers?
Implement a dense neural network and compare its performance to that of the CNN’.
This will take 30-45 minutes and might deviate the focus away from CNNs.</p></li>
<li><p>You can demonstrate (no typing along), just to show how the network would look like and make the comparison.</p></li>
<li><p>You can just mention that a simple network with only dense layers reaches 18% accuracy, considerably worse than our simple CNN.</p></li>
</ol>
</div>
<div class="admonition-comparison-with-a-network-with-only-dense-layers callout admonition" id="callout-3">
<p class="admonition-title">Comparison with a network with only dense layers</p>
<p>How does this simple CNN compare to a neural network with only dense layers?</p>
<p>We can define a neural network with only dense layers:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">create_dense_model</span><span class="p">():</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="n">train_images</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Flatten</span><span class="p">()(</span><span class="n">inputs</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">keras</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">,</span>
                              <span class="n">name</span><span class="o">=</span><span class="s1">&#39;dense_model&#39;</span><span class="p">)</span>

<span class="n">dense_model</span> <span class="o">=</span> <span class="n">create_dense_model</span><span class="p">()</span>
<span class="n">dense_model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-output notranslate"><div class="highlight"><pre><span></span><span class="go">Model: &quot;dense_model&quot;</span>

<span class="go">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┓</span>
<span class="go">┃ Layer (type)                 ┃ Output Shape         ┃     Param # ┃</span>
<span class="go">┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━┩</span>
<span class="go">│ input_layer_3 (InputLayer)   │ (None, 64, 64, 3)    │           0 │</span>
<span class="go">├──────────────────────────────┼──────────────────────┼─────────────┤</span>
<span class="go">│ flatten_2 (Flatten)          │ (None, 12288)        │           0 │</span>
<span class="go">├──────────────────────────────┼──────────────────────┼─────────────┤</span>
<span class="go">│ dense_4 (Dense)              │ (None, 50)           │     614,450 │</span>
<span class="go">├──────────────────────────────┼──────────────────────┼─────────────┤</span>
<span class="go">│ dense_5 (Dense)              │ (None, 50)           │       2,550 │</span>
<span class="go">├──────────────────────────────┼──────────────────────┼─────────────┤</span>
<span class="go">│ dense_6 (Dense)              │ (None, 10)           │         510 │</span>
<span class="go">└──────────────────────────────┴──────────────────────┴─────────────┘</span>

<span class="go"> Total params: 617,510 (2.36 MB)</span>

<span class="go"> Trainable params: 617,510 (2.36 MB)</span>

<span class="go"> Non-trainable params: 0 (0.00 B)</span>
</pre></div>
</div>
<p>As you can see this model has more parameters than our simple CNN, let’s train and evaluate it!</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">compile_model</span><span class="p">(</span><span class="n">dense_model</span><span class="p">)</span>
<span class="n">history</span> <span class="o">=</span> <span class="n">dense_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_images</span><span class="p">,</span> <span class="n">train_labels</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
                    <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">val_images</span><span class="p">,</span> <span class="n">val_labels</span><span class="p">))</span>
<span class="n">plot_history</span><span class="p">(</span><span class="n">history</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">,</span> <span class="s1">&#39;val_accuracy&#39;</span><span class="p">])</span>
</pre></div>
</div>
<p><img alt="" class="align-center" src="_images/04_dense_model_training_history.png" /></p>
<p>As you can see the validation accuracy only reaches about 18%, whereas the CNN reached about 28% accuracy.</p>
<p>This demonstrates that convolutional layers are a big improvement over dense layers for these kind of datasets.</p>
</div>
</section>
<section id="refine-the-model">
<h4>9. Refine the model<a class="headerlink" href="#refine-the-model" title="Link to this heading"></a></h4>
<div class="admonition-network-depth exercise important admonition" id="exercise-5">
<p class="admonition-title">Network depth</p>
<p>What, do you think, will be the effect of adding a convolutional layer to your model? Will this model have more or fewer parameters?
Try it out. Create a <code class="docutils literal notranslate"><span class="pre">model</span></code> that has an additional <code class="docutils literal notranslate"><span class="pre">Conv2d</span></code> layer with 50 filters and another MaxPooling2D layer after the last MaxPooling2D layer. Train it for 10 epochs and plot the results.</p>
<p><strong>HINT</strong>:
The model definition that we used previously needs to be adjusted as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">inputs</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="n">train_images</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">)(</span><span class="n">inputs</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">MaxPooling2D</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">MaxPooling2D</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))(</span><span class="n">x</span><span class="p">)</span>
<span class="c1"># Add your extra layers here</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Flatten</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition-solution solution important dropdown admonition" id="solution-5">
<p class="admonition-title">Solution</p>
<p>We add an extra Conv2D layer after the second pooling layer:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">create_nn_extra_layer</span><span class="p">():</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="n">train_images</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">)(</span><span class="n">inputs</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">MaxPooling2D</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">MaxPooling2D</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))(</span><span class="n">x</span><span class="p">)</span> <span class="c1">#</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span> <span class="c1"># extra layer</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">MaxPooling2D</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))(</span><span class="n">x</span><span class="p">)</span> <span class="c1"># extra layer</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Flatten</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>

    <span class="n">model</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;dollar_street_model&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">model</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">create_nn_extra_layer</span><span class="p">()</span>
</pre></div>
</div>
<p>With the model defined above, we can inspect the number of parameters:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-output notranslate"><div class="highlight"><pre><span></span><span class="go">Model: &quot;dollar_street_model&quot;</span>

<span class="go">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┓</span>
<span class="go">┃ Layer (type)                 ┃ Output Shape         ┃     Param # ┃</span>
<span class="go">┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━┩</span>
<span class="go">│ input_layer_4 (InputLayer)   │ (None, 64, 64, 3)    │           0 │</span>
<span class="go">├──────────────────────────────┼──────────────────────┼─────────────┤</span>
<span class="go">│ conv2d_4 (Conv2D)            │ (None, 62, 62, 50)   │       1,400 │</span>
<span class="go">├──────────────────────────────┼──────────────────────┼─────────────┤</span>
<span class="go">│ max_pooling2d_2              │ (None, 31, 31, 50)   │           0 │</span>
<span class="go">│ (MaxPooling2D)               │                      │             │</span>
<span class="go">├──────────────────────────────┼──────────────────────┼─────────────┤</span>
<span class="go">│ conv2d_5 (Conv2D)            │ (None, 29, 29, 50)   │      22,550 │</span>
<span class="go">├──────────────────────────────┼──────────────────────┼─────────────┤</span>
<span class="go">│ max_pooling2d_3              │ (None, 14, 14, 50)   │           0 │</span>
<span class="go">│ (MaxPooling2D)               │                      │             │</span>
<span class="go">├──────────────────────────────┼──────────────────────┼─────────────┤</span>
<span class="go">│ conv2d_6 (Conv2D)            │ (None, 12, 12, 50)   │      22,550 │</span>
<span class="go">├──────────────────────────────┼──────────────────────┼─────────────┤</span>
<span class="go">│ max_pooling2d_4              │ (None, 6, 6, 50)     │           0 │</span>
<span class="go">│ (MaxPooling2D)               │                      │             │</span>
<span class="go">├──────────────────────────────┼──────────────────────┼─────────────┤</span>
<span class="go">│ flatten_3 (Flatten)          │ (None, 1800)         │           0 │</span>
<span class="go">├──────────────────────────────┼──────────────────────┼─────────────┤</span>
<span class="go">│ dense_7 (Dense)              │ (None, 50)           │      90,050 │</span>
<span class="go">├──────────────────────────────┼──────────────────────┼─────────────┤</span>
<span class="go">│ dense_8 (Dense)              │ (None, 10)           │         510 │</span>
<span class="go">└──────────────────────────────┴──────────────────────┴─────────────┘</span>

<span class="go"> Total params: 137,060 (535.39 KB)</span>

<span class="go"> Trainable params: 137,060 (535.39 KB)</span>

<span class="go"> Non-trainable params: 0 (0.00 B)</span>
</pre></div>
</div>
<p>The number of parameters has decreased by adding this layer.
We can see that the extra layers decrease the resolution from 14x14 to 6x6,
as a result, the input of the Dense layer is smaller than in the previous network.
To train the network and plot the results:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">compile_model</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_images</span><span class="p">,</span> <span class="n">train_labels</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                   <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">val_images</span><span class="p">,</span> <span class="n">val_labels</span><span class="p">))</span>
<span class="n">plot_history</span><span class="p">(</span><span class="n">history</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">,</span> <span class="s1">&#39;val_accuracy&#39;</span><span class="p">])</span>
</pre></div>
</div>
<p><img alt="" class="align-center" src="_images/04_training_history_2.png" /></p>
</div>
</div>
<div class="docutils">
<div class="admonition-other-types-of-data callout admonition" id="callout-4">
<p class="admonition-title">Other types of data</p>
<p>Convolutional and Pooling layers are also applicable to different types of
data than image data. Whenever the data is ordered in a (spatial) dimension,
and <em>translation invariant</em> features are expected to be useful, convolutions
can be used. Think for example of time series data from an accelerometer,
audio data for speech recognition, or 3d structures of chemical compounds.</p>
</div>
</div>
<div class="admonition-why-and-when-to-use-convolutional-neural-networks exercise important admonition" id="exercise-6">
<p class="admonition-title">Why and when to use convolutional neural networks</p>
<ol class="arabic simple">
<li><p>Would it make sense to train a convolutional neural network (CNN) on the penguins dataset and why?</p></li>
<li><p>Would it make sense to train a CNN on the weather dataset and why?</p></li>
<li><p>(Optional) Can you think of a different machine learning task that would benefit from a
CNN architecture?</p></li>
</ol>
<div class="admonition-solution solution important dropdown admonition" id="solution-6">
<p class="admonition-title">Solution</p>
<ol class="arabic simple">
<li><p>No that would not make sense. Convolutions only work when the features of the data can be ordered
in a meaningful way. Pixels for example are ordered in a spatial dimension.
This kind of order cannot be applied to the features of the penguin dataset.
If we would have pictures or audio recordings of the penguins as input data
it would make sense to use a CNN architecture.</p></li>
<li><p>It would make sense, but only if we approach the problem from a different angle then we did before.
Namely, 1D convolutions work quite well on sequential data such as timeseries. If we have as our input a matrix
of the different weather conditions over time in the past x days, a CNN would be suited to quickly grasp
the temporal relationship over days.</p></li>
<li><p>Some example domains in which CNNs are applied:</p></li>
</ol>
<ul class="simple">
<li><p>Text data</p></li>
<li><p>Timeseries, specifically audio</p></li>
<li><p>Molecular structures</p></li>
</ul>
</div>
</div>
<div class="docutils">
<p class="rubric" id="dropout">Dropout</p>
<p>Note that the training loss continues to decrease, while the validation loss stagnates, and even starts to increase over the course of the epochs. Similarly, the accuracy for the validation set does not improve anymore after some epochs. This means we are overfitting on our training data set.</p>
<p>Techniques to avoid overfitting, or to improve model generalization, are termed <strong>regularization techniques</strong>.
One of the most versatile regularization technique is <strong>dropout</strong> (<a class="reference external" href="https://jmlr.org/papers/v15/srivastava14a.html">Srivastava et al., 2014</a>).
Dropout means that during each training cycle (one forward pass of the data through the model) a random fraction of neurons in a dense layer are turned off.
This is described with the dropout rate between 0 and 1 which determines the fraction of nodes to silence at a time.</p>
<p><img alt="" class="align-center" src="_images/neural_network_sketch_dropout.png" /></p>
<p>The intuition behind dropout is that it enforces redundancies in the network by constantly removing different elements of a network. The model can no longer rely on individual nodes and instead must create multiple “paths”. In addition, the model has to make predictions with much fewer nodes and weights (connections between the nodes).
As a result, it becomes much harder for a network to memorize particular features. At first this might appear a quite drastic approach which affects the network architecture strongly.
In practice, however, dropout is computationally a very elegant solution which does not affect training speed. And it frequently works very well.</p>
<p><strong>Important to note:</strong> Dropout layers will only randomly silence nodes during training! During a prediction step, all nodes remain active (dropout is off). During training, the sample of nodes that are silenced are different for each training instance, to give all nodes a chance to observe enough training data to learn its weights.</p>
<p>Let us add a dropout layer after each pooling layer towards the end of the network that randomly drops 80% of the nodes.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">create_nn_with_dropout</span><span class="p">():</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="n">train_images</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">)(</span><span class="n">inputs</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">MaxPooling2D</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.8</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span> <span class="c1"># This is new!</span>

    <span class="n">x</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">MaxPooling2D</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.8</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span> <span class="c1"># This is new!</span>

    <span class="n">x</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">MaxPooling2D</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.8</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span> <span class="c1"># This is new!</span>

    <span class="n">x</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Flatten</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;dropout_model&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">model</span>

<span class="n">model_dropout</span> <span class="o">=</span> <span class="n">create_nn_with_dropout</span><span class="p">()</span>
<span class="n">model_dropout</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-output notranslate"><div class="highlight"><pre><span></span><span class="go">Model: &quot;dropout_model&quot;</span>

<span class="go">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┓</span>
<span class="go">┃ Layer (type)                 ┃ Output Shape         ┃     Param # ┃</span>
<span class="go">┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━┩</span>
<span class="go">│ input_layer_5 (InputLayer)   │ (None, 64, 64, 3)    │           0 │</span>
<span class="go">├──────────────────────────────┼──────────────────────┼─────────────┤</span>
<span class="go">│ conv2d_7 (Conv2D)            │ (None, 62, 62, 50)   │       1,400 │</span>
<span class="go">├──────────────────────────────┼──────────────────────┼─────────────┤</span>
<span class="go">│ max_pooling2d_5              │ (None, 31, 31, 50)   │           0 │</span>
<span class="go">│ (MaxPooling2D)               │                      │             │</span>
<span class="go">├──────────────────────────────┼──────────────────────┼─────────────┤</span>
<span class="go">│ dropout (Dropout)            │ (None, 31, 31, 50)   │           0 │</span>
<span class="go">├──────────────────────────────┼──────────────────────┼─────────────┤</span>
<span class="go">│ conv2d_8 (Conv2D)            │ (None, 29, 29, 50)   │      22,550 │</span>
<span class="go">├──────────────────────────────┼──────────────────────┼─────────────┤</span>
<span class="go">│ max_pooling2d_6              │ (None, 14, 14, 50)   │           0 │</span>
<span class="go">│ (MaxPooling2D)               │                      │             │</span>
<span class="go">├──────────────────────────────┼──────────────────────┼─────────────┤</span>
<span class="go">│ dropout_1 (Dropout)          │ (None, 14, 14, 50)   │           0 │</span>
<span class="go">├──────────────────────────────┼──────────────────────┼─────────────┤</span>
<span class="go">│ conv2d_9 (Conv2D)            │ (None, 12, 12, 50)   │      22,550 │</span>
<span class="go">├──────────────────────────────┼──────────────────────┼─────────────┤</span>
<span class="go">│ max_pooling2d_7              │ (None, 6, 6, 50)     │           0 │</span>
<span class="go">│ (MaxPooling2D)               │                      │             │</span>
<span class="go">├──────────────────────────────┼──────────────────────┼─────────────┤</span>
<span class="go">│ dropout_2 (Dropout)          │ (None, 6, 6, 50)     │           0 │</span>
<span class="go">├──────────────────────────────┼──────────────────────┼─────────────┤</span>
<span class="go">│ flatten_4 (Flatten)          │ (None, 1800)         │           0 │</span>
<span class="go">├──────────────────────────────┼──────────────────────┼─────────────┤</span>
<span class="go">│ dense_9 (Dense)              │ (None, 50)           │      90,050 │</span>
<span class="go">├──────────────────────────────┼──────────────────────┼─────────────┤</span>
<span class="go">│ dense_10 (Dense)             │ (None, 10)           │         510 │</span>
<span class="go">└──────────────────────────────┴──────────────────────┴─────────────┘</span>

<span class="go"> Total params: 137,060 (535.39 KB)</span>

<span class="go"> Trainable params: 137,060 (535.39 KB)</span>

<span class="go"> Non-trainable params: 0 (0.00 B)</span>
</pre></div>
</div>
<p>We can see that the dropout does not alter the dimensions of the image, and has zero parameters.</p>
<p>We again compile and train the model.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">compile_model</span><span class="p">(</span><span class="n">model_dropout</span><span class="p">)</span>

<span class="n">history</span> <span class="o">=</span> <span class="n">model_dropout</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_images</span><span class="p">,</span> <span class="n">train_labels</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
                    <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">val_images</span><span class="p">,</span> <span class="n">val_labels</span><span class="p">))</span>
</pre></div>
</div>
<p>And inspect the training results:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">plot_history</span><span class="p">(</span><span class="n">history</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">,</span> <span class="s1">&#39;val_accuracy&#39;</span><span class="p">])</span>
</pre></div>
</div>
<p><img alt="" class="align-center" src="_images/04_training_history_3.png" /></p>
<p>Now we see that the gap between the training accuracy and validation accuracy is much smaller, and that the final accuracy on the validation set is higher than without dropout.</p>
<div class="admonition-vary-dropout-rate exercise important admonition" id="exercise-7">
<p class="admonition-title">Vary dropout rate</p>
<ol class="arabic simple">
<li><p>What do you think would happen if you lower the dropout rate? Try it out, and
see how it affects the model training.</p></li>
<li><p>You are varying the dropout rate and checking its effect on the model performance,
what is the term associated to this procedure?</p></li>
</ol>
<div class="admonition-solution solution important dropdown admonition" id="solution-7">
<p class="admonition-title">Solution</p>
<p class="rubric" id="varying-the-dropout-rate">1. Varying the dropout rate</p>
<p>The code below instantiates and trains a model with varying dropout rates.
You can see from the resulting plot that the ideal dropout rate in this case is around 0.9.
This is where the val loss is lowest.</p>
<p>Note that it can take a while to train these 6 networks.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">create_nn_with_dropout</span><span class="p">(</span><span class="n">dropout_rate</span><span class="p">):</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="n">train_images</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">)(</span><span class="n">inputs</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">MaxPooling2D</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout_rate</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>

    <span class="n">x</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">MaxPooling2D</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout_rate</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>

    <span class="n">x</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout_rate</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Flatten</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;dropout_model&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">model</span>

<span class="n">early_stopper</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">callbacks</span><span class="o">.</span><span class="n">EarlyStopping</span><span class="p">(</span><span class="n">monitor</span><span class="o">=</span><span class="s1">&#39;val_loss&#39;</span><span class="p">,</span> <span class="n">patience</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>

<span class="n">dropout_rates</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.95</span><span class="p">]</span>
<span class="n">val_losses</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">dropout_rate</span> <span class="ow">in</span> <span class="n">dropout_rates</span><span class="p">:</span>
    <span class="n">model_dropout</span> <span class="o">=</span> <span class="n">create_nn_with_dropout</span><span class="p">(</span><span class="n">dropout_rate</span><span class="p">)</span>
    <span class="n">compile_model</span><span class="p">(</span><span class="n">model_dropout</span><span class="p">)</span>
    <span class="n">model_dropout</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_images</span><span class="p">,</span> <span class="n">train_labels</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span>
                      <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">val_images</span><span class="p">,</span> <span class="n">val_labels</span><span class="p">),</span>
                      <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">early_stopper</span><span class="p">]</span>
                      <span class="p">)</span>

    <span class="n">val_loss</span><span class="p">,</span> <span class="n">val_acc</span> <span class="o">=</span> <span class="n">model_dropout</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">val_images</span><span class="p">,</span>  <span class="n">val_labels</span><span class="p">)</span>
    <span class="n">val_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">val_loss</span><span class="p">)</span>

<span class="n">loss_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;dropout_rate&#39;</span><span class="p">:</span> <span class="n">dropout_rates</span><span class="p">,</span> <span class="s1">&#39;val_loss&#39;</span><span class="p">:</span> <span class="n">val_losses</span><span class="p">})</span>


<span class="n">sns</span><span class="o">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">loss_df</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s1">&#39;dropout_rate&#39;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s1">&#39;val_loss&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p><img alt="" class="align-center" src="_images/04_vary_dropout_rate.png" /></p>
<p class="rubric" id="term-associated-to-this-procedure">2. Term associated to this procedure</p>
<p>This is called hyperparameter tuning.</p>
</div>
</div>
</div>
<div class="docutils">
<p class="rubric" id="hyperparameter-tuning">Hyperparameter tuning</p>
<div class="admonition-do-a-live-demo-instead-of-live-coding instructor dropdown admonition" id="instructor-4">
<p class="admonition-title">Do a live demo instead of live coding</p>
<p>You might want to demonstrate this section on hyperparameter tuning instead of doing live coding.
The goal is to show that hyperparameter tuning can be done easily with <code class="docutils literal notranslate"><span class="pre">keras_tuner</span></code>, not to memorize all the exact syntax of how to do it. This will probably save you half an hour of participants typing over code that they already know from before. In addition, on really slow machines running the grid search could possibly take more than 10 minutes.</p>
</div>
</div>
<p>Recall that hyperparameters are model configuration settings that are chosen before the training process and affect the model’s learning behavior and performance, for example the dropout rate. In general, if you are varying hyperparameters to find the combination of hyperparameters with the best model performance this is called hyperparameter tuning. A naive way to do this is to write a for-loop and train a slightly different model in every cycle.
However, it is better to use the <code class="docutils literal notranslate"><span class="pre">keras_tuner</span></code> package for this.</p>
<p>Let’s first define a function that creates a neuronal network given 2 hyperparameters, namely the dropout rate and the number of layers:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">create_nn_with_hp</span><span class="p">(</span><span class="n">dropout_rate</span><span class="p">,</span> <span class="n">n_layers</span><span class="p">):</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="n">train_images</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">inputs</span>
    <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_layers</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">MaxPooling2D</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout_rate</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Flatten</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;cifar_model&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">model</span>
</pre></div>
</div>
<p>Now, let’s find the best combination of hyperparameters using grid search.
Grid search is the simplest hyperparameter tuning strategy,
you test all the combinations of predefined values for the hyperparameters that you want to vary.</p>
<p>For this we will make use of the package <code class="docutils literal notranslate"><span class="pre">keras_tuner</span></code>, we can install it by typing in the command line:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>keras_tuner
</pre></div>
</div>
<p>Note that this can take some time to train (around 5 minutes or longer).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">keras_tuner</span>

<span class="n">hp</span> <span class="o">=</span> <span class="n">keras_tuner</span><span class="o">.</span><span class="n">HyperParameters</span><span class="p">()</span>

<span class="k">def</span><span class="w"> </span><span class="nf">build_model</span><span class="p">(</span><span class="n">hp</span><span class="p">):</span>
    <span class="c1"># Define values for hyperparameters to try out:</span>
    <span class="n">n_layers</span> <span class="o">=</span> <span class="n">hp</span><span class="o">.</span><span class="n">Int</span><span class="p">(</span><span class="s2">&quot;n_layers&quot;</span><span class="p">,</span> <span class="n">min_value</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">max_value</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">dropout_rate</span> <span class="o">=</span> <span class="n">hp</span><span class="o">.</span><span class="n">Float</span><span class="p">(</span><span class="s2">&quot;dropout_rate&quot;</span><span class="p">,</span> <span class="n">min_value</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">max_value</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

    <span class="n">model</span> <span class="o">=</span> <span class="n">create_nn_with_hp</span><span class="p">(</span><span class="n">dropout_rate</span><span class="p">,</span> <span class="n">n_layers</span><span class="p">)</span>
    <span class="n">compile_model</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">model</span>

<span class="n">tuner</span> <span class="o">=</span> <span class="n">keras_tuner</span><span class="o">.</span><span class="n">GridSearch</span><span class="p">(</span><span class="n">build_model</span><span class="p">,</span> <span class="n">objective</span><span class="o">=</span><span class="s1">&#39;val_loss&#39;</span><span class="p">)</span>

<span class="n">tuner</span><span class="o">.</span><span class="n">search</span><span class="p">(</span><span class="n">train_images</span><span class="p">,</span> <span class="n">train_labels</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
             <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">val_images</span><span class="p">,</span> <span class="n">val_labels</span><span class="p">))</span>
</pre></div>
</div>
<div class="highlight-output notranslate"><div class="highlight"><pre><span></span><span class="go">Trial 6 Complete [00h 00m 19s]</span>
<span class="go">val_loss: 2.086069345474243</span>

<span class="go">Best val_loss So Far: 2.086069345474243</span>
<span class="go">Total elapsed time: 00h 01m 28s</span>
</pre></div>
</div>
<p>Let’s have a look at the results:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">tuner</span><span class="o">.</span><span class="n">results_summary</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-output notranslate"><div class="highlight"><pre><span></span><span class="go">Results summary</span>
<span class="go">Results in ./untitled_project</span>
<span class="go">Showing 10 best trials</span>
<span class="go">Objective(name=&quot;val_loss&quot;, direction=&quot;min&quot;)</span>

<span class="go">Trial 0005 summary</span>
<span class="go">Hyperparameters:</span>
<span class="go">n_layers: 2</span>
<span class="go">dropout_rate: 0.8</span>
<span class="go">Score: 2.086069345474243</span>

<span class="go">Trial 0000 summary</span>
<span class="go">Hyperparameters:</span>
<span class="go">n_layers: 1</span>
<span class="go">dropout_rate: 0.2</span>
<span class="go">Score: 2.101102352142334</span>

<span class="go">Trial 0001 summary</span>
<span class="go">Hyperparameters:</span>
<span class="go">n_layers: 1</span>
<span class="go">dropout_rate: 0.5</span>
<span class="go">Score: 2.1184325218200684</span>

<span class="go">Trial 0003 summary</span>
<span class="go">Hyperparameters:</span>
<span class="go">n_layers: 2</span>
<span class="go">dropout_rate: 0.2</span>
<span class="go">Score: 2.1233835220336914</span>

<span class="go">Trial 0002 summary</span>
<span class="go">Hyperparameters:</span>
<span class="go">n_layers: 1</span>
<span class="go">dropout_rate: 0.8</span>
<span class="go">Score: 2.1370232105255127</span>

<span class="go">Trial 0004 summary</span>
<span class="go">Hyperparameters:</span>
<span class="go">n_layers: 2</span>
<span class="go">dropout_rate: 0.5</span>
<span class="go">Score: 2.143627882003784</span>
</pre></div>
</div>
<div class="admonition-hyperparameter-tuning exercise important admonition" id="exercise-8">
<p class="admonition-title">Hyperparameter tuning</p>
<p>1: Looking at the grid search results, select all correct statements:</p>
<ul class="simple">
<li><p>A. 6 different models were trained in this grid search run, because there are 6 possible combinations for the defined hyperparameter values</p></li>
<li><p>B. 2 different models were trained, 1 for each hyperparameter that we want to change</p></li>
<li><p>C. 1 model is trained with 6 different hyperparameter combinations</p></li>
<li><p>D. The model with 2 layer and a dropout rate of 0.5 is the best model with a validation loss of 2.144</p></li>
<li><p>E. The model with 2 layers and a dropout rate of 0.8 is the best model with a validation loss of 2.086</p></li>
<li><p>F. We found the model with the best possible combination of dropout rate and number of layers</p></li>
</ul>
<p>2 (Optional): Perform a grid search finding the best combination of the following hyperparameters: 2 different activation functions: ‘relu’, and ‘tanh’, and 2 different values for the kernel size: 3 and 4. Which combination works best?</p>
<p><strong>Hint</strong>: Instead of <code class="docutils literal notranslate"><span class="pre">hp.Int</span></code> you should now use <code class="docutils literal notranslate"><span class="pre">hp.Choice(&quot;name&quot;,</span> <span class="pre">[&quot;value1&quot;,</span> <span class="pre">&quot;value2&quot;])</span></code> to use hyperparameters from a predefined set of possible values.</p>
<div class="admonition-solution solution important dropdown admonition" id="solution-8">
<p class="admonition-title">Solution</p>
<p>1:</p>
<ul class="simple">
<li><p>A: Correct, 2 values for number of layers (1 and 2) are combined with 3 values for the dropout rate (0.2, 0.5, 0.8). 2 * 3 = 6 combinations</p></li>
<li><p>B: Incorrect, a model is trained for each combination of defined hyperparameter values</p></li>
<li><p>C: Incorrect, it is important to note that you actually train and test different models for each run of the grid search</p></li>
<li><p>D: Incorrect, this is the worst model since the validation loss is highest</p></li>
<li><p>E: Correct, this is the best model with the lowest loss</p></li>
<li><p>F: Incorrect, it could be that a different number of layers in combination with a dropout rate that we did not test (for example 3 layers and a dropout rate of 0.6) could be the best model.</p></li>
</ul>
<p>2 (Optional):</p>
<p>You need to adapt the code as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">create_nn_with_hp</span><span class="p">(</span><span class="n">activation_function</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">):</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="n">train_images</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">inputs</span>
    <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="p">(</span><span class="n">kernel_size</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="n">activation_function</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">MaxPooling2D</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.2</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Flatten</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">activation_function</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;cifar_model&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">model</span>

<span class="n">hp</span> <span class="o">=</span> <span class="n">keras_tuner</span><span class="o">.</span><span class="n">HyperParameters</span><span class="p">()</span>

<span class="k">def</span><span class="w"> </span><span class="nf">build_model</span><span class="p">(</span><span class="n">hp</span><span class="p">):</span>
    <span class="n">kernel_size</span> <span class="o">=</span> <span class="n">hp</span><span class="o">.</span><span class="n">Int</span><span class="p">(</span><span class="s2">&quot;kernel_size&quot;</span><span class="p">,</span> <span class="n">min_value</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">max_value</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">activation</span> <span class="o">=</span> <span class="n">hp</span><span class="o">.</span><span class="n">Choice</span><span class="p">(</span><span class="s2">&quot;activation&quot;</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;relu&quot;</span><span class="p">,</span> <span class="s2">&quot;tanh&quot;</span><span class="p">])</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">create_nn_with_hp</span><span class="p">(</span><span class="n">activation</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">)</span>
    <span class="n">compile_model</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">model</span>

<span class="n">tuner</span> <span class="o">=</span> <span class="n">keras_tuner</span><span class="o">.</span><span class="n">GridSearch</span><span class="p">(</span><span class="n">build_model</span><span class="p">,</span> <span class="n">objective</span><span class="o">=</span><span class="s1">&#39;val_loss&#39;</span><span class="p">,</span> <span class="n">project_name</span><span class="o">=</span><span class="s1">&#39;new_project&#39;</span><span class="p">)</span>
<span class="n">tuner</span><span class="o">.</span><span class="n">search</span><span class="p">(</span><span class="n">train_images</span><span class="p">,</span> <span class="n">train_labels</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
             <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">val_images</span><span class="p">,</span> <span class="n">val_labels</span><span class="p">))</span>
</pre></div>
</div>
<div class="highlight-output notranslate"><div class="highlight"><pre><span></span><span class="go">Trial 4 Complete [00h 00m 25s]</span>
<span class="go">val_loss: 2.0591845512390137</span>

<span class="go">Best val_loss So Far: 2.0277602672576904</span>
<span class="go">Total elapsed time: 00h 01m 30s</span>
</pre></div>
</div>
<p>Let’s look at the results:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">tuner</span><span class="o">.</span><span class="n">results_summary</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-output notranslate"><div class="highlight"><pre><span></span><span class="go">Results summary</span>
<span class="go">Results in ./new_project</span>
<span class="go">Showing 10 best trials</span>
<span class="go">Objective(name=&quot;val_loss&quot;, direction=&quot;min&quot;)</span>

<span class="go">Trial 0001 summary</span>
<span class="go">Hyperparameters:</span>
<span class="go">kernel_size: 3</span>
<span class="go">activation: tanh</span>
<span class="go">Score: 2.0277602672576904</span>

<span class="go">Trial 0003 summary</span>
<span class="go">Hyperparameters:</span>
<span class="go">kernel_size: 4</span>
<span class="go">activation: tanh</span>
<span class="go">Score: 2.0591845512390137</span>

<span class="go">Trial 0000 summary</span>
<span class="go">Hyperparameters:</span>
<span class="go">kernel_size: 3</span>
<span class="go">activation: relu</span>
<span class="go">Score: 2.123767614364624</span>

<span class="go">Trial 0002 summary</span>
<span class="go">Hyperparameters:</span>
<span class="go">kernel_size: 4</span>
<span class="go">activation: relu</span>
<span class="go">Score: 2.150160551071167</span>
</pre></div>
</div>
<p>A kernel size of 3 and <code class="docutils literal notranslate"><span class="pre">tanh</span></code> as activation function is the best tested combination.</p>
</div>
</div>
<div class="docutils">
<p>Grid search can quickly result in a combinatorial explosion because all combinations of hyperparameters are trained and tested.
Instead, <code class="docutils literal notranslate"><span class="pre">random</span> <span class="pre">search</span></code> randomly samples combinations of hyperparemeters, allowing for a much larger look through a large number of possible hyperparameter combinations.</p>
<p>Next to grid search and random search there are many different hyperparameter tuning strategies, including <a class="reference external" href="https://en.wikipedia.org/wiki/Neural_architecture_search">neural architecture search</a> where a separate neural network is trained to find the best architecture for a model!</p>
<p class="rubric" id="share-model">10. Share model</p>
<p>Let’s save our model</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s1">&#39;cnn_model.keras&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p class="rubric" id="conclusion-and-next-steps">Conclusion and next steps</p>
<p>How successful were we with creating a model here?
With ten image classes, and assuming that we would not ask the model to classify an image that contains none of the given classes of object, a model working on complete guesswork would be correct 10% of the time.
Against this baseline accuracy of 10%, and considering the diversity and relatively low resolution of the example images, perhaps our last model’s validation accuracy of ~30% is not too bad.
What could be done to improve on this performance?
We might try adjusting the number of layers and their parameters, such as the number of units in a layer, or providing more training data (we were using only a subset of the original Dollar Street dataset here).
Or we could explore some other deep learning techniques, such as transfer learning, to create more sophisticated models.</p>
<div class="admonition-keypoints keypoints admonition" id="keypoints-0">
<p class="admonition-title">Keypoints</p>
<ul class="simple">
<li><p>Convolutional layers make efficient reuse of model parameters.</p></li>
<li><p>Pooling layers decrease the resolution of your input</p></li>
<li><p>Dropout is a way to prevent overfitting</p></li>
</ul>
</div>
</div>
</section>
</section>
<span id="document-5-transfer-learning"></span><section class="tex2jax_ignore mathjax_ignore" id="transfer-learning">
<h3>5. Transfer learning<a class="headerlink" href="#transfer-learning" title="Link to this heading"></a></h3>
<div class="admonition-questions questions admonition" id="questions-0">
<p class="admonition-title">Questions</p>
<ul class="simple">
<li><p>How do I apply a pre-trained model to my data?</p></li>
</ul>
</div>
<div class="admonition-objectives objectives admonition" id="objectives-0">
<p class="admonition-title">Objectives</p>
<ul class="simple">
<li><p>Adapt a state-of-the-art pre-trained network to your own dataset</p></li>
</ul>
</div>
<section id="what-is-transfer-learning">
<h4>What is transfer learning?<a class="headerlink" href="#what-is-transfer-learning" title="Link to this heading"></a></h4>
<p>Instead of training a model from scratch, with transfer learning you make use of models that are trained on another machine learning task. The pre-trained network captures generic knowledge during pre-training and will only be ‘fine-tuned’ to the specifics of your dataset.</p>
<p>An example: Let’s say that you want to train a model to classify images of different dog breeds. You could make use of a pre-trained network that learned how to classify images of dogs and cats. The pre-trained network will not know anything about different dog breeds, but it will have captured some general knowledge of, on a high-level, what dogs look like, and on a low-level all the different features (eyes, ears, paws, fur) that make up an image of a dog. Further training this model on your dog breed dataset is a much easier task than training from scratch, because the model can use the general knowledge captured in the pre-trained network.</p>
<p><img alt="" class="align-center" src="_images/05-transfer_learning.png" /></p>
<!-- 
Edit this plot using the Mermaid live editor:
1. Open this link that includes the source code of the chart to open the live editor web interface:
https://mermaid.live/edit#pako:eNpVkE1vgzAMhv9K5MPUSrQKAWUlh0kr9NZetp02drAgUCRIqhC0dZT_vizso_PJb_zYr-MRCl1KEFC1-q04orFk_5Ar4uL-ZZHpuic3JEXbkwwtLl_JanVHLk8GG0UOrrO9kO3CJ-QKXs4T0tGBqq-kIXuJRjWqnubK1s9JZ5F5I7I1Upb_fL7rqRe7a8g7LiGATpoOm9J9YPyCc7BH2ckchEtLWeHQ2hxyNTkUB6sfz6oAYc0gAzB6qI8gKmx7p4ZTiVZmDdYGu9_XE6pnrf-0LBurzWE-mb-cZ0CM8A5iRdfUBeObmEZJzKOEJRHnUQBnECwK15zRMGJxzNkmoXwK4MMPD30bpSHjt5SHSfyzzs7bzQtPn9Xpf_E
2. Make changes to the chart as desired in the live editor
3. Download the newly created diagram from the live editor (Actions / PNG) and replace the existing image in the episode folder (episodes/fig/05-transfer_learning.png)
4. (optional) crop the image to remove the white space around the plot in a separate image editor
5. Update the URL in step 1 of this comment to the new URL of the live editor
-->
<p>In this episode we will learn how use Keras to adapt a state-of-the-art pre-trained model to the <a class="reference external" href="https://zenodo.org/records/10970014">Dollar Street Dataset</a>.</p>
</section>
<section id="formulate-outline-the-problem">
<h4>1. Formulate / Outline the problem<a class="headerlink" href="#formulate-outline-the-problem" title="Link to this heading"></a></h4>
<p>Just like in the previous episode, we use the Dollar Street 10 dataset.</p>
<p>We load the data in the same way as the previous episode:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">pathlib</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="n">DATA_FOLDER</span> <span class="o">=</span> <span class="n">pathlib</span><span class="o">.</span><span class="n">Path</span><span class="p">(</span><span class="s1">&#39;data/dataset_dollarstreet/&#39;</span><span class="p">)</span> <span class="c1"># change to location where you stored the data</span>
<span class="n">train_images</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">DATA_FOLDER</span> <span class="o">/</span> <span class="s1">&#39;train_images.npy&#39;</span><span class="p">)</span>
<span class="n">val_images</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">DATA_FOLDER</span> <span class="o">/</span> <span class="s1">&#39;test_images.npy&#39;</span><span class="p">)</span>
<span class="n">train_labels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">DATA_FOLDER</span> <span class="o">/</span> <span class="s1">&#39;train_labels.npy&#39;</span><span class="p">)</span>
<span class="n">val_labels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">DATA_FOLDER</span> <span class="o">/</span> <span class="s1">&#39;test_labels.npy&#39;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="identify-inputs-and-outputs">
<h4>2. Identify inputs and outputs<a class="headerlink" href="#identify-inputs-and-outputs" title="Link to this heading"></a></h4>
<p>As discussed in the previous episode, the input are images of dimension 64 x 64 pixels with 3 colour channels each.
The goal is to predict one out of 10 classes to which the image belongs.</p>
</section>
<section id="prepare-the-data">
<h4>3. Prepare the data<a class="headerlink" href="#prepare-the-data" title="Link to this heading"></a></h4>
<p>We prepare the data as before, scaling the values between 0 and 1.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">train_images</span> <span class="o">=</span> <span class="n">train_images</span> <span class="o">/</span> <span class="mf">255.0</span>
<span class="n">val_images</span> <span class="o">=</span> <span class="n">val_images</span> <span class="o">/</span> <span class="mf">255.0</span>
</pre></div>
</div>
</section>
<section id="choose-a-pre-trained-model-or-start-building-architecture-from-scratch">
<h4>4. Choose a pre-trained model or start building architecture from scratch<a class="headerlink" href="#choose-a-pre-trained-model-or-start-building-architecture-from-scratch" title="Link to this heading"></a></h4>
<p>Let’s define our model input layer using the shape of our training images:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># input tensor</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tensorflow</span><span class="w"> </span><span class="kn">import</span> <span class="n">keras</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">train_images</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>
</pre></div>
</div>
<p>Our images are 64 x 64 pixels, whereas the pre-trained model that we will use was
trained on images of 160 x 160 pixels.
To adapt our data accordingly, we add an upscale layer that resizes the images to 160 x 160 pixels during training and prediction.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># upscale layer</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">tensorflow</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">tf</span>
<span class="n">method</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">image</span><span class="o">.</span><span class="n">ResizeMethod</span><span class="o">.</span><span class="n">BILINEAR</span>
<span class="n">upscale</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Lambda</span><span class="p">(</span>
  <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">image</span><span class="o">.</span><span class="n">resize_with_pad</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">160</span><span class="p">,</span> <span class="mi">160</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="n">method</span><span class="p">))(</span><span class="n">inputs</span><span class="p">)</span>
</pre></div>
</div>
<p>From the <code class="docutils literal notranslate"><span class="pre">keras.applications</span></code> module we use the <code class="docutils literal notranslate"><span class="pre">DenseNet121</span></code> architecture.
This architecture was proposed by the paper: <a class="reference external" href="https://arxiv.org/abs/1608.06993">Densely Connected Convolutional Networks (CVPR 2017)</a>. It is trained on the <a class="reference external" href="https://www.image-net.org/">Imagenet</a> dataset, which contains 14,197,122 annotated images according to the WordNet hierarchy with over 20,000 classes.</p>
<p>We will have a look at the architecture later, for now it is enough to know
that it is a convolutional neural network with 121 layers that was designed
to work well on image classification tasks.</p>
<p>Let’s configure the DenseNet121:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">base_model</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">applications</span><span class="o">.</span><span class="n">DenseNet121</span><span class="p">(</span><span class="n">include_top</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                            <span class="n">pooling</span><span class="o">=</span><span class="s1">&#39;max&#39;</span><span class="p">,</span>
                                            <span class="n">weights</span><span class="o">=</span><span class="s1">&#39;imagenet&#39;</span><span class="p">,</span>
                                            <span class="n">input_tensor</span><span class="o">=</span><span class="n">upscale</span><span class="p">,</span>
                                            <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">160</span><span class="p">,</span><span class="mi">160</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span>
                                            <span class="p">)</span>
</pre></div>
</div>
<div class="admonition-ssl-certificate-verify-failed-error callout admonition" id="callout-0">
<p class="admonition-title">SSL: certificate verify failed error</p>
<p>If you get the following error message: <code class="docutils literal notranslate"><span class="pre">certificate</span> <span class="pre">verify</span> <span class="pre">failed:</span> <span class="pre">unable</span> <span class="pre">to</span> <span class="pre">get</span> <span class="pre">local</span> <span class="pre">issuer</span> <span class="pre">certificate</span></code>,
you can download <a class="reference external" href="https://storage.googleapis.com/tensorflow/keras-applications/densenet/densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5">the weights of the model manually</a>
and then load in the weights from the downloaded file:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">base_model</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">applications</span><span class="o">.</span><span class="n">DenseNet121</span><span class="p">(</span>
    <span class="n">include_top</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">pooling</span><span class="o">=</span><span class="s1">&#39;max&#39;</span><span class="p">,</span>
    <span class="n">weights</span><span class="o">=</span><span class="s1">&#39;densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5&#39;</span><span class="p">,</span> <span class="c1"># this should refer to the weights file you downloaded</span>
    <span class="n">input_tensor</span><span class="o">=</span><span class="n">upscale</span><span class="p">,</span>
    <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">160</span><span class="p">,</span><span class="mi">160</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<p>By setting <code class="docutils literal notranslate"><span class="pre">include_top</span></code> to <code class="docutils literal notranslate"><span class="pre">False</span></code> we exclude the fully connected layer at the
top of the network, hence the final output layer. This layer was used to predict the Imagenet classes,
but will be of no use for our Dollar Street dataset.
Note that the ‘top layer’ appears at the bottom in the output of <code class="docutils literal notranslate"><span class="pre">model.summary()</span></code>.</p>
<p>We add <code class="docutils literal notranslate"><span class="pre">pooling='max'</span></code> so that max pooling is applied to the output of the DenseNet121 network.</p>
<p>By setting <code class="docutils literal notranslate"><span class="pre">weights='imagenet'</span></code> we use the weights that resulted from training
this network on the Imagenet data.</p>
<p>We connect the network to the <code class="docutils literal notranslate"><span class="pre">upscale</span></code> layer that we defined before.</p>
<section id="only-train-a-head-network">
<h5>Only train a ‘head’ network<a class="headerlink" href="#only-train-a-head-network" title="Link to this heading"></a></h5>
<p>Instead of fine-tuning all the weights of the DenseNet121 network using our dataset,
we choose to freeze all these weights and only train a so-called ‘head network’
that sits on top of the pre-trained network. You can see the DenseNet121 network
as extracting a meaningful feature representation from our image. The head network
will then be trained to decide on which of the 10 Dollar Street dataset classes the image belongs.</p>
<p>We will turn of the <code class="docutils literal notranslate"><span class="pre">trainable</span></code> property of the base model:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">base_model</span><span class="o">.</span><span class="n">trainable</span> <span class="o">=</span> <span class="kc">False</span>
</pre></div>
</div>
<p>Let’s define our ‘head’ network:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">out</span> <span class="o">=</span> <span class="n">base_model</span><span class="o">.</span><span class="n">output</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Flatten</span><span class="p">()(</span><span class="n">out</span><span class="p">)</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">BatchNormalization</span><span class="p">()(</span><span class="n">out</span><span class="p">)</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">)(</span><span class="n">out</span><span class="p">)</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)(</span><span class="n">out</span><span class="p">)</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">)(</span><span class="n">out</span><span class="p">)</span>
</pre></div>
</div>
<p>Finally we define our model:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">out</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition-inspect-the-densenet121-network exercise important admonition" id="exercise-0">
<p class="admonition-title">Inspect the DenseNet121 network</p>
<p>Have a look at the network architecture with <code class="docutils literal notranslate"><span class="pre">model.summary()</span></code>.
It is indeed a deep network, so expect a long summary!</p>
<p class="rubric" id="trainable-parameters">1.Trainable parameters</p>
<p>How many parameters are there? How many of them are trainable?</p>
<p>Why is this and how does it effect the time it takes to train the model?</p>
<p class="rubric" id="head-and-base">2. Head and base</p>
<p>Can you see in the model summary which part is the base network and which part is the head network?</p>
<p class="rubric" id="max-pooling">3. Max pooling</p>
<p>Which layer is added because we provided <code class="docutils literal notranslate"><span class="pre">pooling='max'</span></code> as argument for <code class="docutils literal notranslate"><span class="pre">DenseNet121()</span></code>?</p>
</div>
<div class="admonition-solutions solution important dropdown admonition" id="solution-0">
<p class="admonition-title">Solutions</p>
<p class="rubric" id="id1">1. Trainable parameters</p>
<p>Total number of parameters: 7093360, out of which only 53808 are trainable.</p>
<p>The 53808 trainable parameters are the weights of the head network. All other parameters are ‘frozen’ because we set <code class="docutils literal notranslate"><span class="pre">base_model.trainable=False</span></code>. Because only a small proportion of the parameters have to be updated at each training step, this will greatly speed up training time.</p>
<p class="rubric" id="id2">2. Head and base</p>
<p>The head network starts at the <code class="docutils literal notranslate"><span class="pre">flatten</span></code> layer, 5 layers before the final layer.</p>
<p class="rubric" id="id3">3. Max pooling</p>
<p>The <code class="docutils literal notranslate"><span class="pre">max_pool</span></code> layer right before the <code class="docutils literal notranslate"><span class="pre">flatten</span></code> layer is added because we provided <code class="docutils literal notranslate"><span class="pre">pooling='max'</span></code>.</p>
</div>
<div class="admonition-training-and-evaluating-the-pre-trained-model exercise important admonition" id="exercise-1">
<p class="admonition-title">Training and evaluating the pre-trained model</p>
<p class="rubric" id="compile-the-model">1. Compile the model</p>
<p>Compile the model:</p>
<ul class="simple">
<li><p>Use the <code class="docutils literal notranslate"><span class="pre">adam</span></code> optimizer</p></li>
<li><p>Use the <code class="docutils literal notranslate"><span class="pre">SparseCategoricalCrossentropy</span></code> loss with <code class="docutils literal notranslate"><span class="pre">from_logits=True</span></code>.</p></li>
<li><p>Use ‘accuracy’ as a metric.</p></li>
</ul>
<p class="rubric" id="train-the-model">2. Train the model</p>
<p>Train the model on the training dataset:</p>
<ul class="simple">
<li><p>Use a batch size of 32</p></li>
<li><p>Train for 30 epochs, but use an earlystopper with a patience of 5</p></li>
<li><p>Pass the validation dataset as validation data so we can monitor performance on the validation data during training</p></li>
<li><p>Store the result of training in a variable called <code class="docutils literal notranslate"><span class="pre">history</span></code></p></li>
<li><p>Training can take a while, it is a much larger model than what we have seen so far.</p></li>
</ul>
<p class="rubric" id="inspect-the-results">3. Inspect the results</p>
<p>Plot the training history and evaluate the trained model. What do you think of the results?</p>
<p class="rubric" id="optional-try-out-other-pre-trained-neural-networks">4. (Optional) Try out other pre-trained neural networks</p>
<p>Train and evaluate another pre-trained model from https://keras.io/api/applications/. How does it compare to DenseNet121?</p>
</div>
<div class="admonition-solution solution important dropdown admonition" id="solution-1">
<p class="admonition-title">Solution</p>
<p class="rubric" id="id4">1. Compile the model</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;adam&#39;</span><span class="p">,</span>
              <span class="n">loss</span><span class="o">=</span><span class="n">keras</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">SparseCategoricalCrossentropy</span><span class="p">(</span><span class="n">from_logits</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
              <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">])</span>
</pre></div>
</div>
<p class="rubric" id="id5">2. Train the model</p>
<p>Define the early stopper:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">early_stopper</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">callbacks</span><span class="o">.</span><span class="n">EarlyStopping</span><span class="p">(</span><span class="n">monitor</span><span class="o">=</span><span class="s1">&#39;val_accuracy&#39;</span><span class="p">,</span>
                              <span class="n">patience</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
</pre></div>
</div>
<p>Train the model:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">train_images</span><span class="p">,</span>
                    <span class="n">y</span><span class="o">=</span><span class="n">train_labels</span><span class="p">,</span>
                    <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
                    <span class="n">epochs</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span>
                    <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">early_stopper</span><span class="p">],</span>
                    <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">val_images</span><span class="p">,</span> <span class="n">val_labels</span><span class="p">))</span>
</pre></div>
</div>
<p class="rubric" id="id6">3. Inspect the results</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">plot_history</span><span class="p">(</span><span class="n">history</span><span class="p">,</span> <span class="n">metrics</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Plot the training history</span>

<span class="sd">    Args:</span>
<span class="sd">        history (keras History object that is returned by model.fit())</span>
<span class="sd">        metrics(str, list): Metric or a list of metrics to plot</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">history_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="o">.</span><span class="n">from_dict</span><span class="p">(</span><span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">)</span>
    <span class="n">sns</span><span class="o">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">history_df</span><span class="p">[</span><span class="n">metrics</span><span class="p">])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;epochs&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;metric&quot;</span><span class="p">)</span>

<span class="n">plot_history</span><span class="p">(</span><span class="n">history</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">,</span> <span class="s1">&#39;val_accuracy&#39;</span><span class="p">])</span>
</pre></div>
</div>
<p><img alt="" class="align-center" src="_images/05_training_history_transfer_learning.png" />
The final validation accuracy reaches 64%, this is a huge improvement over 30% accuracy we reached with the simple convolutional neural network that we build from scratch in the previous episode.</p>
</div>
</section>
</section>
<section id="concluding-the-power-of-transfer-learning">
<h4>Concluding: The power of transfer learning<a class="headerlink" href="#concluding-the-power-of-transfer-learning" title="Link to this heading"></a></h4>
<p>In many domains, large networks are available that have been trained on vast amounts of data, such as in computer vision and natural language processing. Using transfer learning, you can benefit from the knowledge that was captured from another machine learning task. In many fields, transfer learning will outperform models trained from scratch, especially if your dataset is small or of poor quality.</p>
<div class="admonition-keypoints keypoints admonition" id="keypoints-0">
<p class="admonition-title">Keypoints</p>
<ul class="simple">
<li><p>Large pre-trained models capture generic knowledge about a domain</p></li>
<li><p>Use the <code class="docutils literal notranslate"><span class="pre">keras.applications</span></code> module to easily use pre-trained models for your own datasets</p></li>
</ul>
</div>
</section>
</section>
<span id="document-6-outlook"></span><section class="tex2jax_ignore mathjax_ignore" id="outlook">
<h3>6. Outlook<a class="headerlink" href="#outlook" title="Link to this heading"></a></h3>
<div class="admonition-questions questions admonition" id="questions-0">
<p class="admonition-title">Questions</p>
<ul class="simple">
<li><p>How does what I learned in this course translate to real-world problems?</p></li>
<li><p>How do I organise a deep learning project?</p></li>
<li><p>What are next steps to take after this course?</p></li>
</ul>
</div>
<div class="admonition-objectives objectives admonition" id="objectives-0">
<p class="admonition-title">Objectives</p>
<ul class="simple">
<li><p>Understand that what we learned in this course can be applied to real-world problems</p></li>
<li><p>Use best practices for organising a deep learning project</p></li>
<li><p>Identify next steps to take after this course</p></li>
</ul>
</div>
<p>You have come to the end of this course.
In this episode we will look back at what we have learned so far, how to apply that to real-world problems, and identify
next steps to take to start applying deep learning in your own projects.</p>
<section id="real-world-application">
<h4>Real-world application<a class="headerlink" href="#real-world-application" title="Link to this heading"></a></h4>
<p>To introduce the core concepts of deep learning we have used quite simple machine learning problems.
But how does what we learned so far apply to real-world applications?</p>
<p>To illustrate that what we learned is actually the basis of successful applications in research,
we will have a look at an example from the field of <a class="reference external" href="https://en.wikipedia.org/wiki/Cheminformatics">cheminformatics</a>.</p>
<div class="admonition-instructor instructor dropdown admonition" id="instructor-0">
<p class="admonition-title">Instructor</p>
<p>You don’t have to use this project as an example.
It works best to use a suitable deep learning project that you know well and are passionate about.</p>
</div>
<p>We will have a look at <a class="reference external" href="https://github.com/matchms/ms2deepscore/blob/0.4.0/notebooks/MS2DeepScore_tutorial.ipynb">this notebook</a>.
It is part of the codebase for <a class="reference external" href="https://doi.org/10.1186/s13321-021-00558-4">this paper</a>.</p>
<p>In short, the deep learning problem is that of finding out how similar two molecules are in terms of their molecular properties,
based on their mass spectrum.
You can compare this to comparing two pictures of animals, and predicting how similar they are.</p>
<p>A Siamese neural network is used to solve the problem.
In a Siamese neural network you have two input vectors, let’s say two images of animals or two mass spectra.
They pass through a base network. Instead of outputting a class or number with one or a few output neurons, the output layer
of the base network is a whole vector of for example 100 neurons. After passing through the base network, you end up with two of these
vectors representing the two inputs. The goal of the base network is to output a meaningful representation of the input (this is called an embedding).
The next step is to compute the cosine similarity between these two output vectors,
cosine similarity is a measure for how similar two vectors are to each other, ranging from 0 (completely different) to 1 (identical).
This cosine similarity is compared to the actual similarity between the two inputs and this error is used to update the weights in the network.</p>
<p>Don’t worry if you do not fully understand the deep learning problem and the approach that is taken here.
We just want you to appreciate that you already learned enough to be able to do this yourself in your own domain.</p>
<div class="admonition-exercise-a-real-world-deep-learning-application exercise important admonition" id="exercise-0">
<p class="admonition-title">Exercise: A real-world deep learning application</p>
<ol class="arabic simple">
<li><p>Looking at the ‘Model training’ section of the notebook, what do you recognize from what you learned in this course?</p></li>
<li><p>Can you identify the different steps of the deep learning workflow in this notebook?</p></li>
<li><p>(Optional): Try to understand the neural network architecture from the first figure of <a class="reference external" href="https://doi.org/10.1186/s13321-021-00558-4">the paper</a>.
a. Why are there 10.000 neurons in the input layer?
b. What do you think would happen if you would decrease the size of spectral embedding layer drastically, to for example 5 neurons?</p></li>
</ol>
<div class="admonition-solution solution important dropdown admonition" id="solution-0">
<p class="admonition-title">Solution</p>
<ol class="arabic simple">
<li><p>The model summary for the Siamese model is more complex than what we have seen so far,
but it is basically a repetition of Dense, BatchNorm, and Dropout layers.
The syntax for training and evaluating the model is the same as what we learned in this course.
EarlyStopping as well as the Adam optimizer is used.</p></li>
<li><p>The different steps are not as clearly defined as in this course, but you should be able to identify ‘3: Data preparation’,
‘4: Choose a pretrained model or start building architecture from scratch’, ‘5: Choose a loss function and optimizer’, ‘6: Train the model’,
‘7: Make predictions’ (which is called ‘Model inference’ in this notebook), and ‘10: Save model’.</p></li>
<li><p>(optional)
a. Because the shape of the input is 10.000. More specifically, the spectrum is binned into a size 10.000 vector,
apparently this is a good size to represent the mass spectrum.
b. This would force the neural network to have a representation of the mass spectrum in only 5 numbers.
This representation would probably be more generic, but might fail to capture all the characteristics found in the spectrum.
This would likely result in underfitting.</p></li>
</ol>
</div>
</div>
<div class="docutils">
<p>Hopefully you can appreciate that what you learned in this course, can be applied to real-world problems as well.</p>
<div class="admonition-extensive-data-preparation callout admonition" id="callout-0">
<p class="admonition-title">Extensive data preparation</p>
<p>You might have noticed that the data preparation for this example is much more extensive than what we have done so far
in this course. This is quite common for applied deep learning projects. It is said that 90% of the time in a
deep learning problem is spent on data preparation, and only 10% on modeling!</p>
</div>
</div>
<div class="admonition-discussion discussion important admonition" id="discussion-0">
<p class="admonition-title">Discussion</p>
<p class="rubric" id="bias-and-evaluation">Bias and Evaluation</p>
<p>Bias has been discussed in the context of machine learning, deep learning and artificial intelligence frequently and on various levels.
That is because there are many aspects to bias.
One the one hand, bias is very technical: a model can be biased towards certain classes or certain features.
On the other hand, this can have very practical and severe impact on the users of a such a model;
for instance when it comes to misclassification in relation to color of the skin or geographical location.</p>
<p>If such biases are reflected in a dataset that is used for model validation and testing, you might not be able to see them.
In order to get an evaluation that is representative for the diversity found in the real world, it is therefore important to use a test set that reflects this diversity as much as possible.</p>
<p>The need for such a dataset as opposed to existing datasets that mostly presumed Western standards has been one of the motivations for creating the Dollar Street Dataset – and why we have used it in this lesson.
The creators <a class="reference external" href="https://papers.nips.cc/paper_files/paper/2022/hash/5474d9d43c0519aa176276ff2c1ca528-Abstract-Datasets_and_Benchmarks.html">have shown</a> that more diversity in a training dataset can contribute to significant model improvements.
A model trained on a more diverse dataset is more robust against unexpected occurrences.</p>
<p>Therefore, it is important to fully understand the quantitative evaluation of a new model:
it reflects the model’s performance on the test set, but it does not say anything about how well that dataset represents the real world.
Also be aware that such matters can be related to racism and other forms of discrimination.
Depending on the use case, diversity can also refer to imbalance on other, more subtle and less sensitive dimensions.</p>
<p><strong>Discuss the following statement with your neighbors:</strong></p>
<ul class="simple">
<li><p>What forms of bias and data imbalance can you think of?</p></li>
<li><p>How would they affect the performance of a deep learning model?</p></li>
</ul>
</div>
<div class="admonition-discussion discussion important admonition" id="discussion-1">
<p class="admonition-title">Discussion</p>
<p class="rubric" id="discussion-large-language-models-and-prompt-engineering">Discussion: Large Language Models and prompt engineering</p>
<p>Large Language Models (LLMs) are deep learning models that are able to perform general-purpose language generation.
They are trained on large amounts of texts, such all pages of Wikipedia.
In recent years the quality of LLMs language understanding and generation has increased tremendously, and since the launch of generative chatbot ChatGPT in 2022 the power of LLMs is now appreciated by the general public.</p>
<p>It is becoming more and more feasible to unleash this power in scientific research. For example, the authors of <a class="reference external" href="https://doi.org/10.1021/acscentsci.3c01087">Zheng et al. (2023)</a> guided ChatGPT in the automation of extracting chemical information from a large amount of research articles. The authors did not implement a deep learning model themselves, but instead they designed the right input for ChatGPT (called a ‘prompt’) that would produce optimal outputs. This is called prompt engineering. A highly simplified example of such a prompt would be: “Given compounds X and Y and context Z, what are the chemical details of the reaction?”</p>
<p>Developments in LLM research are moving fast, at the end of 2023 the newest ChatGPT version <a class="reference external" href="https://openai.com/blog/chatgpt-can-now-see-hear-and-speak">could take images and sound as input</a>.
In theory, this means that you can solve the Dollar Street image classification problem from the previous episode by prompt engineering, with prompts similar to “Which out of these categories: [LIST OF CATEGORIES] is depicted in the image”.</p>
<p><strong>Discuss the following statement with your neighbors:</strong></p>
<p><em>In a few years most machine learning problems in scientific research can be solved with prompt engineering.</em></p>
</div>
</section>
<section id="organising-deep-learning-projects">
<h4>Organising deep learning projects<a class="headerlink" href="#organising-deep-learning-projects" title="Link to this heading"></a></h4>
<p>As you might have noticed already in this course, deep learning projects can quickly become messy.
Here follow some best practices for keeping your projects organized:</p>
<section id="organise-experiments-in-notebooks">
<h5>1. Organise experiments in notebooks<a class="headerlink" href="#organise-experiments-in-notebooks" title="Link to this heading"></a></h5>
<p>Jupyter notebooks are a useful tool for doing deep learning experiments.
You can very easily modify your code bit by bit, and interactively look at the results.
In addition you can explain why you are doing things in markdown cells.</p>
<ul class="simple">
<li><p>As a rule of thumb do one approach or experiment in one notebook.</p></li>
<li><p>Give consistent and meaningful names to notebooks, such as: <code class="docutils literal notranslate"><span class="pre">01-all-cities-simple-cnn.ipynb</span></code></p></li>
<li><p>Add a rationale on top and a conclusion on the bottom of each notebook</p></li>
</ul>
<p><a class="reference external" href="https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1007007"><em>Ten simple rules for writing and sharing computational analyses in Jupyter Notebooks</em></a> provides further advice on how to maximise the usefulness and reproducibility of experiments captured in a notebook.</p>
</section>
<section id="use-python-modules">
<h5>2. Use Python modules<a class="headerlink" href="#use-python-modules" title="Link to this heading"></a></h5>
<p>Code that is repeatedly used should live in a Python module and not be copied to multiple notebooks.
You can import functions and classes from the module(s) in the notebooks.
This way you can remove a lot of code definition from your notebooks and have a focus on the actual experiment.</p>
</section>
<section id="keep-track-of-your-results-in-a-central-place">
<h5>3. Keep track of your results in a central place<a class="headerlink" href="#keep-track-of-your-results-in-a-central-place" title="Link to this heading"></a></h5>
<p>Always evaluate your experiments in the same way, on the exact same test set.
Document the results of your experiments in a consistent and meaningful way.
You can use a simple spreadsheet such as this:</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>MODEL NAME</p></th>
<th class="head"><p>MODEL DESCRIPTION</p></th>
<th class="head"><p>RMSE</p></th>
<th class="head"><p>TESTSET NAME</p></th>
<th class="head"><p>GITHUB COMMIT</p></th>
<th class="head"><p>COMMENTS</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>weather_prediction_v1.0</p></td>
<td><p>Basel features only, 10 years. nn: 100-50</p></td>
<td><p>3.21</p></td>
<td><p>10_years_v1.0</p></td>
<td><p>ed28d85</p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p>weather_prediction_v1.1</p></td>
<td><p>all features, 10 years. nn: 100-50</p></td>
<td><p>3.35</p></td>
<td><p>10_years_v1.0</p></td>
<td><p>4427b78</p></td>
<td><p></p></td>
</tr>
</tbody>
</table>
<p>You could also use a tool such as <a class="reference external" href="https://wandb.ai/site">Weights and Biases</a> for this.</p>
<div class="admonition-cookiecutter-data-science callout admonition" id="callout-1">
<p class="admonition-title">Cookiecutter data science</p>
<p>If you want to get more pointers for organising deep learning, or data science projects in general,
we recommend <a class="reference external" href="https://drivendata.github.io/cookiecutter-data-science/">Cookiecutter data science</a>.
It is a template for initiating an organized data science project folder structure
that you can adapt to your own needs.</p>
</div>
</section>
</section>
<section id="next-steps">
<h4>Next steps<a class="headerlink" href="#next-steps" title="Link to this heading"></a></h4>
<p>You now understand the basic principles of deep learning and are able to implement your own deep learning pipelines in Python.
But there is still so much to learn and do!</p>
<p>Here are some suggestions for next steps you can take in your endeavor to become a deep learning expert:</p>
<ul class="simple">
<li><p>Learn more by going through a few of <a class="reference internal" href="#learners/reference.md#external-references"><span class="xref myst">the learning resources we have compiled for you</span></a></p></li>
<li><p>Apply what you have learned to your own projects. Use the deep learning workflow to structure your work.
Start as simple as possible, and incrementally increase the complexity of your approach.</p></li>
<li><p>Compete in a <a class="reference external" href="https://www.kaggle.com/competitions">Kaggle competition</a> to practice what you have learned.</p></li>
<li><p>Get access to a GPU. Your deep learning experiments will progress much quicker if you have to wait for your network to train
in a few seconds instead of hours (which is the order of magnitude of speedup you can expect from training on a GPU instead of CPU).
Tensorflow/Keras will automatically detect and use a GPU if it is available on your system without any code changes.
A simple and quick way to get access to a GPU is to use <a class="reference external" href="https://colab.google/">Google Colab</a></p></li>
</ul>
<div class="admonition-keypoints keypoints admonition" id="keypoints-0">
<p class="admonition-title">Keypoints</p>
<ul class="simple">
<li><p>Although the data preparation and model architectures are somewhat more complex,
what we have learned in this course can directly be applied to real-world problems</p></li>
<li><p>Use what you have learned in this course as a basis for your own learning trajectory in the world of deep learning</p></li>
</ul>
</div>
</section>
</section>
</div>
<div class="toctree-wrapper compound">
<span id="document-reference"></span><section class="tex2jax_ignore mathjax_ignore" id="reference-for-learners">
<h3>Reference for learners<a class="headerlink" href="#reference-for-learners" title="Link to this heading"></a></h3>
<section id="glossary">
<h4>Glossary<a class="headerlink" href="#glossary" title="Link to this heading"></a></h4>
<ul class="simple">
<li><p><a class="reference external" href="https://glosario.carpentries.org/en/#artificial_intelligence">artificial intelligence</a></p></li>
<li><p><a class="reference external" href="https://glosario.carpentries.org/en/#machine_learning">machine learning</a></p></li>
<li><p><a class="reference external" href="https://glosario.carpentries.org/en/#deep_learning">deep learning</a></p></li>
<li><p><a class="reference external" href="https://glosario.carpentries.org/en/#neural_network">neural network</a></p></li>
<li><p><a class="reference external" href="https://glosario.carpentries.org/en/#cnn">convolutional neural network (CNN)</a></p></li>
<li><p><a class="reference external" href="https://glosario.carpentries.org/en/#rnn">recurrent neural network (RNN)</a></p></li>
<li><p><a class="reference external" href="https://glosario.carpentries.org/en/#accuracy">accuracy</a></p></li>
<li><p><a class="reference external" href="https://glosario.carpentries.org/en/#epoch_dl">epoch</a></p></li>
<li><p><a class="reference external" href="https://glosario.carpentries.org/en/#learning_rate">learning rate</a></p></li>
<li><p><a class="reference external" href="https://glosario.carpentries.org/en/#confusion_matrix">confusion matrix</a></p></li>
<li><p><a class="reference external" href="https://glosario.carpentries.org/en/#class_imbalance">class imbalance</a></p></li>
<li><p><a class="reference external" href="https://glosario.carpentries.org/en/#overfitting">overfitting</a></p></li>
<li><p><a class="reference external" href="https://glosario.carpentries.org/en/#hidden_layer">hidden layer</a></p></li>
</ul>
</section>
<section id="external-references">
<h4>External references<a class="headerlink" href="#external-references" title="Link to this heading"></a></h4>
<p>Here is a (non exhaustive) list of external resources for further study after this lesson:</p>
<section id="miscellaneous-resources">
<h5>Miscellaneous resources<a class="headerlink" href="#miscellaneous-resources" title="Link to this heading"></a></h5>
<ul class="simple">
<li><p><a class="reference external" href="https://machinelearningmastery.com/difference-test-validation-datasets/">the difference between validation data and test data</a></p></li>
<li><p><a class="reference external" href="https://machinelearningmastery.com/learning-curves-for-diagnosing-machine-learning-model-performance/">underfitting and overfitting</a></p></li>
<li><p><a class="reference external" href="https://towardsdatascience.com/handling-imbalanced-datasets-in-deep-learning-f48407a0e758">Unbalanced data</a></p></li>
<li><p><a class="reference external" href="https://www.tensorflow.org/tutorials/structured_data/imbalanced_data">Unbalanced data in Keras</a></p></li>
<li><p><a class="reference external" href="http://playground.tensorflow.org/">Tensorflow Playground, for visualizing neural networks</a></p></li>
<li><p><a class="reference external" href="https://learn.deeplearning.ai/chatgpt-prompt-eng/lesson/1/lesson_1">ChatGPT prompt engineering course</a></p></li>
</ul>
</section>
<section id="some-ml-challenges-or-benchmarks">
<h5>Some ML challenges or benchmarks<a class="headerlink" href="#some-ml-challenges-or-benchmarks" title="Link to this heading"></a></h5>
<ul class="simple">
<li><p>https://mlcontests.com/</p></li>
<li><p><a class="reference external" href="https://www.kaggle.com/">Kaggle, machine learning competitions</a></p></li>
<li><p><a class="reference external" href="https://predictioncenter.org/">protein structure prediction</a></p></li>
<li><p><a class="reference external" href="https://www.ebi.ac.uk/msd-srv/capri/">prediction of protein-protein interactions</a></p></li>
</ul>
</section>
<section id="some-courses-for-deeper-learning">
<h5>Some courses for deeper learning:<a class="headerlink" href="#some-courses-for-deeper-learning" title="Link to this heading"></a></h5>
<ul class="simple">
<li><p><a class="reference external" href="https://www.fast.ai/">Fast AI course: making neural nets uncool again</a></p></li>
<li><p><a class="reference external" href="https://www.udacity.com/course/deep-learning-pytorch--ud188">Intro to Deep Learning with PyTorch</a>, the course is quite intuitive</p></li>
<li><p>Coursera courses by Andrew Ng:</p>
<ul>
<li><p><a class="reference external" href="https://www.coursera.org/learn/ai-for-everyone">AI for everyone</a>, for beginners who won’t do ML projects but are courious about what AI really is and what AI can do</p></li>
<li><p><a class="reference external" href="https://www.coursera.org/learn/machine-learning">ML course</a> and <a class="reference external" href="https://www.coursera.org/specializations/deep-learning">DL course</a>, quite intensive courses for beginner/intermediate-level researchers who will do ML/DL projects</p></li>
<li><p><a class="reference external" href="https://www.coursera.org/learn/machine-learning-projects">Structuring Machine Learning Projects</a>, how to conduct ML projects with useful ML engineering strategies</p></li>
<li><p><a class="reference external" href="https://databricks.com/p/ebook/big-book-of-machine-learning-use-cases?utm_medium=paid+search&amp;amp;utm_source=google&amp;amp;utm_campaign=15631674924&amp;amp;utm_adgroup=130078635494&amp;amp;utm_content=ebook&amp;amp;utm_offer=big-book-of-machine-learning-use-cases&amp;amp;utm_ad=587637991591&amp;amp;utm_term=machine%20learning&amp;amp;gclid=CjwKCAjw9qiTBhBbEiwAp-GE0WaK3IrtfBeDWjb7L2ZDQg5_YgevbwoD288bq0sGgYNhcTlnjZfLaBoCC_EQAvD_BwE">Book on Machine Learning</a></p></li>
</ul>
</li>
<li><p><a class="reference external" href="https://www.deeplearningbook.org/">Book: Ian Goodfellow and Yoshua Bengio and Aaron Courville -  Deep Learning</a>. A really thorough, detailed (though math-heavy) book on everything (for example Generative Adverserial Networks or Autoencoders) you want to know about deep learning</p></li>
<li><p><a class="reference external" href="https://udlbook.github.io/udlbook/">Book: Simon J.D. Prince - Understanding Deep Learning</a>. A less dense and slightly more modern overview of deep learning with coding examples for each chapter.</p></li>
</ul>
</section>
</section>
</section>
<span id="document-instructor-notes"></span><section class="tex2jax_ignore mathjax_ignore" id="instructor-notes">
<h3>Instructor notes<a class="headerlink" href="#instructor-notes" title="Link to this heading"></a></h3>
<section id="setup-before-the-lesson">
<h4>Setup before the lesson<a class="headerlink" href="#setup-before-the-lesson" title="Link to this heading"></a></h4>
<p>The required python packages for this lesson often result in installation issues,
so it is advisable to organize a pre-workshop setup session where learners can show their installation and get help with problems.</p>
<p>Installations on learners’ devices have the advantage of lowering the threshold to continue with the material beyond the workshop. Note though, that this lesson can also be taught on a cloud environment such as <a class="reference external" href="https://colab.research.google.com/">Google colab</a> or <a class="reference external" href="https://github.com/carpentries/scaffolds/blob/master/instructions/workshop-coordination.md#my-binder">My Binder</a>. This can serve as a backup environment if local installations fail. Some cloud environments offer the possibility to run the code on a GPU, which significantly increases the runtime of deep learning code.</p>
</section>
<section id="deep-learning-workflow">
<h4>Deep learning workflow<a class="headerlink" href="#deep-learning-workflow" title="Link to this heading"></a></h4>
<p>The episodes are quite long, because they cover a full cycle of the deep learning workflow. It really helps to structure your teaching by making it clear where in the 10-step deep learning workflow we are. You can for example use headers in your notebook for each of the steps in the workflow.</p>
</section>
<section id="episode-3-monitor-the-training-process">
<h4>Episode 3: Monitor the training process<a class="headerlink" href="#episode-3-monitor-the-training-process" title="Link to this heading"></a></h4>
<p>When episode 3 is taught on a different day then episode 2, it is very useful to start with a recap of episode 2. The Key Points of episode 2 can be iterated, and you can go through the code of the previous session (without actually running it). This will help learners in the big exercise on creating a neural network.</p>
<p>The following exercises work well to do in groups / break-out rooms:</p>
<ul class="simple">
<li><p>Split data into training, validation, and test set</p></li>
<li><p>Create the neural network. Note that this is a fairly challenging exercise, but learners should be able to do this based on their experiences in episode 2 (see also remark about recap).</p></li>
<li><p>Predict the labels for both training and test set and compare to the true values</p></li>
<li><p>Try to reduce the degree of overfitting by lowering the number of parameters</p></li>
<li><p>Create a similar scatter plot for a reasonable baseline</p></li>
<li><p>Open question: What could be next steps to further improve the model?
All other exercises are small and can be done individually.</p></li>
</ul>
</section>
<section id="presentation-slides">
<h4>Presentation slides<a class="headerlink" href="#presentation-slides" title="Link to this heading"></a></h4>
<p>There are no official presentation slides for this lesson, but this material does include some example
slides from when this course was taught by different institutions. These slides can be found in
the
<a class="reference external" href="https://github.com/carpentries-incubator/deep-learning-intro/tree/main/instructors/slides">slides</a>
folder.</p>
</section>
</section>
<span id="document-learner-profiles"></span><section class="tex2jax_ignore mathjax_ignore" id="learner-profiles">
<h3>Learner profiles<a class="headerlink" href="#learner-profiles" title="Link to this heading"></a></h3>
<section id="ann-from-meteorology">
<h4>Ann from Meteorology<a class="headerlink" href="#ann-from-meteorology" title="Link to this heading"></a></h4>
<p>Ann has collected 2-3 GB of structured image data from several autonomous microscope on balloon expeditions into the atmosphere within her PhD programme. Each image has a timestamp which can be related to the height of the balloon at a given point and associated weather conditions. The images are unstructured and she would like to detect from the images if the balloon traversed a cloud or not. She has tried to do that with standard image processing methods, but the image artifacts to descriminate are somewhat diverse. Ann has used machine learning on tabular data before and would like to use Deep Learning for the images at hand. She saw collaborators in another lab do that and would like to pick up this skill.</p>
</section>
<section id="barbara-from-material-science">
<h4>Barbara from Material Science<a class="headerlink" href="#barbara-from-material-science" title="Link to this heading"></a></h4>
<p>Barbara just started her PostDoc in Material Science. Her new group has a large amount of scanning electron miscroscope images stored which exhibit several metals when exposed to a plasma. The team also made the effort to highlight solid deposits in these images and thus obtained 20,000 images with such annotations. Barbara performed some image analysis before and hence has the feeling that Deep Learning may help her in this task. She saw her labmates use ML algorithms for this and is motivated to finally understand these approaches.</p>
</section>
<section id="dan-from-life-sciences">
<h4>Dan from Life Sciences<a class="headerlink" href="#dan-from-life-sciences" title="Link to this heading"></a></h4>
<p>Dan produced a large population of bacteria that were subject to genetic alterations resulting in 10 different phenotypes. The latter can be identified by different colors, shapes and movement speed under a fluorescence microscope. Dan does not have much of experience with image processing techniques to segment these different objects, but used GUI based tools like <a class="reference external" href="https://fiji.sc">fiji</a> and others. He has recorded 50-60 movies of 30 minutes each. 10 of these movies have been produced with one type of phenotype only. Dan doesn’t consider himself a strong coder, but needs to identify bacteria of the phenotypes in the dataset. He is interested to learn if Deep Learning can help.</p>
</section>
<section id="eric-from-pediatrics-science">
<h4>Eric from Pediatrics Science<a class="headerlink" href="#eric-from-pediatrics-science" title="Link to this heading"></a></h4>
<p>Eric ran a large array of clinical trials in his hospital to improve children pharmaceutics for treating a common (non-lethal) virus. He obtained a table that lists: the progression of the treatment for each patient; the dose of the drug given; whether the patient was in the placebo group or not; and moe. As the table has more than 100 000 rows, Eric is certain that he can use ML to cluster the rows in one column where the data taking was inconsistent. Eric has coded before when necessary, but never saw it as something he needed to learn. His cheatsheet is his core wisdom with code. His supervisor invited him to take a course on Machine Learning as “this is the tech of these days!” his boss said.</p>
</section>
</section>
</div>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="schedule">
<h1>Schedule<a class="headerlink" href="#schedule" title="Link to this heading"></a></h1>
<section id="day-1">
<h2>Day 1<a class="headerlink" href="#day-1" title="Link to this heading"></a></h2>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Time</p></th>
<th class="head"><p>Topic</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>9:00</p></td>
<td><p>Welcome and Icebreaker</p></td>
</tr>
<tr class="row-odd"><td><p>9:10</p></td>
<td><p><a class="reference internal" href="#document-1-introduction"><span class="doc std std-doc">Introduction to Deep Learning</span></a></p></td>
</tr>
<tr class="row-even"><td><p>9:50</p></td>
<td><p>Coffee Break</p></td>
</tr>
<tr class="row-odd"><td><p>10:00</p></td>
<td><p><a class="reference internal" href="#document-2-keras"><span class="doc std std-doc">Classification by a neural network using Keras</span></a></p></td>
</tr>
<tr class="row-even"><td><p>10:50</p></td>
<td><p>Coffee Break</p></td>
</tr>
<tr class="row-odd"><td><p>11:00</p></td>
<td><p><a class="reference internal" href="#document-2-keras"><span class="doc std std-doc">Classification by a neural network using Keras</span></a></p></td>
</tr>
<tr class="row-even"><td><p>11:50</p></td>
<td><p>Wrap-up</p></td>
</tr>
<tr class="row-odd"><td><p>12:00</p></td>
<td><p>END</p></td>
</tr>
</tbody>
</table>
</section>
<section id="day-2">
<h2>Day 2<a class="headerlink" href="#day-2" title="Link to this heading"></a></h2>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Time</p></th>
<th class="head"><p>Topic</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>9:00</p></td>
<td><p>Welcome and recap</p></td>
</tr>
<tr class="row-odd"><td><p>9:10</p></td>
<td><p><a class="reference internal" href="#document-3-monitor-the-model"><span class="doc std std-doc">Monitor the training process</span></a></p></td>
</tr>
<tr class="row-even"><td><p>9:50</p></td>
<td><p>Coffee Break</p></td>
</tr>
<tr class="row-odd"><td><p>10:00</p></td>
<td><p><a class="reference internal" href="#document-3-monitor-the-model"><span class="doc std std-doc">Monitor the training process</span></a></p></td>
</tr>
<tr class="row-even"><td><p>10:20</p></td>
<td><p><a class="reference internal" href="#document-4-advanced-layer-types"><span class="doc std std-doc">Advanced Layer Types</span></a></p></td>
</tr>
<tr class="row-odd"><td><p>10:50</p></td>
<td><p>Coffee Break</p></td>
</tr>
<tr class="row-even"><td><p>11:00</p></td>
<td><p><a class="reference internal" href="#document-4-advanced-layer-types"><span class="doc std std-doc">Advanced Layer Types</span></a></p></td>
</tr>
<tr class="row-odd"><td><p>11:20</p></td>
<td><p><a class="reference internal" href="#document-5-transfer-learning"><span class="doc std std-doc">Transfer learning</span></a></p></td>
</tr>
<tr class="row-even"><td><p>11:40</p></td>
<td><p><a class="reference internal" href="#document-6-outlook"><span class="doc std std-doc">Outlook</span></a></p></td>
</tr>
<tr class="row-odd"><td><p>11:50</p></td>
<td><p>Wrap-up</p></td>
</tr>
<tr class="row-even"><td><p>12:00</p></td>
<td><p>END</p></td>
</tr>
</tbody>
</table>
</section>
<section id="day-3">
<h2>Day 3<a class="headerlink" href="#day-3" title="Link to this heading"></a></h2>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Time</p></th>
<th class="head"><p>Topic</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>9:00</p></td>
<td><p>Welcome and recap</p></td>
</tr>
<tr class="row-odd"><td><p>9:05</p></td>
<td><p>TBD</p></td>
</tr>
<tr class="row-even"><td><p>9:55</p></td>
<td><p>Coffee Break</p></td>
</tr>
<tr class="row-odd"><td><p>10:00</p></td>
<td><p>TBD</p></td>
</tr>
<tr class="row-even"><td><p>10:50</p></td>
<td><p>Coffee Break</p></td>
</tr>
<tr class="row-odd"><td><p>11:00</p></td>
<td><p>TBD</p></td>
</tr>
<tr class="row-even"><td><p>11:50</p></td>
<td><p>Post-workshop survey</p></td>
</tr>
<tr class="row-odd"><td><p>12:00</p></td>
<td><p>END</p></td>
</tr>
</tbody>
</table>
</section>
<section id="other-related-lessons">
<h2>Other related lessons<a class="headerlink" href="#other-related-lessons" title="Link to this heading"></a></h2>
<section id="original-carpentries-lesson">
<h3>Original carpentries lesson<a class="headerlink" href="#original-carpentries-lesson" title="Link to this heading"></a></h3>
<p>This lesson is an adaptation of <a class="reference external" href="https://carpentries-lab.github.io/deep-learning-intro">https://carpentries-lab.github.io/deep-learning-intro</a>, licensed CC-BY-4.0.</p>
<section id="introduction-to-artificial-neural-networks-in-python">
<h4>Introduction to artificial neural networks in Python<a class="headerlink" href="#introduction-to-artificial-neural-networks-in-python" title="Link to this heading"></a></h4>
<p>The <a class="reference external" href="https://carpentries-incubator.github.io/machine-learning-neural-python/">Introduction to artificial neural networks in Python lesson</a>
takes a different angle to introducing deep learning,
focusing on computer vision with the application on medical images.</p>
</section>
<section id="introduction-to-machine-learning-in-python-with-scikit-learn">
<h4>Introduction to machine learning in Python with scikit-learn<a class="headerlink" href="#introduction-to-machine-learning-in-python-with-scikit-learn" title="Link to this heading"></a></h4>
<p>The <a class="reference external" href="https://esciencecenter-digital-skills.github.io/scikit-learn-mooc/">Introduction to machine learning in Python with scikit-learn lesson</a>
introduces practical machine learning using Python. It is a good lesson to follow in preparation for this lesson,
since basic knowledge of machine learning and Python programming skills are required for this lesson.</p>
<div class="checklist docutils">
<p class="rubric" id="prerequisites">Prerequisites</p>
<p>Learners are expected to have the following knowledge:</p>
<ul class="simple">
<li><p>Basic Python programming skills and familiarity with the Pandas package.</p></li>
<li><p>Basic knowledge on machine learning, including the following concepts: Data cleaning, train &amp; test split, type of problems (regression, classification), overfitting &amp; underfitting, metrics (accuracy, recall, etc.).</p></li>
</ul>
</div>
<div class="instructor docutils">
<p class="rubric" id="id2"></p>
</div>
</section>
</section>
</section>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2025, The contributors.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>